Source,Caption
./dataset/81ab3d22-0ae8-4c76-8f91-55757ec4678a.pdf,step-wise select prompt to select next step.
./dataset/90a408aa-dbbf-4ade-a9c9-32abbf57f7e8.pdf,Generated Persona
./dataset/0e6e79e6-6fd4-4911-bc45-dcb272e80beb.pdf,Failure analysis across the four methods on the Advanced task set. The x-axis represents the task IDs that are sorted in ascending order based on the length of the task's action steps. And the y-axis presents two metrics for each task: (1) the count of failures observed over three repetitions (left y-axis); (2) the length of the ground truth action steps (right y-axis).
./dataset/bedcf7d3-a91c-4c3b-b266-9c4a24efa24c.png,TTS case of Scoring Bias.
./dataset/de788cb6-6929-4152-8795-84208728ec9c.jpg,3D trajectory plot plot
./dataset/05c5deb7-4d90-449a-bdfe-8ddae0b273b6.png,"Examples of misinformation generated by LLMs. The latest LLMs, GPT-4, mistakenly provides an irrelevant website link when citing a paper."
./dataset/e709d770-8749-42bf-a5c0-25b7a8586b14.png,Impact of LLM's intuition-based and KG embedding-based rewards.
./dataset/bf1cc378-76ce-4ef4-9d9e-89e8235bfcfa.png,"Our boundary-aware three-stage training framework. In the first stage, visual features are aligned with LLM's semantic space through image-text training. In the second stage, we transform a large-scale multi-event dataset into a QA format based on templates, training VTimeLLM to possess temporal boundary awareness and understand events within the boundaries. In the third stage, we create a high-quality dialogue dataset for instruction tuning, which aligns VTimeLLM with human intent and also enables more precise video temporal understanding."
./dataset/73b3749f-15c4-495a-855c-0cec56bc0f6f.jpg,Safety alignment on Vicuna
./dataset/4776ebd2-ec49-4b8f-b578-50a75046432e.jpg,SWE-bench-NF
./dataset/116a259c-88af-489d-a797-f6f11ad44fec.pdf,"Ratio of three different types of mappings during iterated learning (curves are the average of 15 different runs, shadow region is the variance)."
./dataset/dcec127a-cd1a-4799-9eec-363af9dfd90c.pdf,"footnotesize The process of building the discriminator RelD, which is trained on the constructed dataset RelQA and used to detect hallucination of LLMs' generated answers."
./dataset/9fe6f42c-3aa6-40de-9501-14eb83231fd6.pdf,Result on Iterative Denoising
./dataset/dede1fa3-4ea6-4c35-b97a-782f70a9a633.pdf,Approximations for the inverse square-root with various iteration counts.
./dataset/f411fd52-ac0f-4380-b271-6bbda90f50ba.pdf,LLMs cooperate with compression module for input compression.
./dataset/11528ea8-54a0-4a2b-b8e3-bc1fe73c18a8.pdf,The proposed Mamba layer for lane-aware probability learning.
./dataset/0cd6b5c1-d673-4bc9-a4db-07bf2457a74b.pdf,The proportion of answers judged correctly by LLMs in different question categories under two QA prompting settings on NQ.
./dataset/5f04cb02-8686-481a-8180-c65b446a5770.pdf,Example of Unclear Implementation Details
./dataset/01d77f3d-1374-431c-a45a-2cd65b7921a6.pdf,An illustration of a weakness of logistic regression.
./dataset/a5bfedab-3e24-4445-a796-a28d3db9cd12.pdf,Model performance with respect to different prompt templates.
./dataset/498cf0bf-e7fd-430f-ab06-3194d52616e7.pdf,Non-relevant error observed in MALT for Llama-3.2-3b: the word 'elasticity' is mistakenly interpreted as 'lachha paratha' due to their similar structure in Urdu. Lachha paratha is a type of bread heavily eaten by speakers of the input low resource language (Urdu).
./dataset/371d011d-10c7-436e-838b-cc607fbeb0b2.pdf,lan-retrieve prompt to generate initial plan.
./dataset/71000135-2549-41c6-8dd6-22597015a71f.png,RMSE
./dataset/bde3ee4e-65b9-4a25-ba79-28acad9df372.png,Influence of the number of retrieved actions on the performance of GPT-3.5 and Vicuna conducting Task-Retrieve method.
./dataset/3dbd8781-4375-497c-b6c0-8264f47296e8.png,The performance of different tool-invoking methods.
./dataset/5175a8b7-0f09-4f89-a054-c4511e698e44.pdf,Distribution of videos over temporal aspects and static content categories.
./dataset/7e6c3076-c55b-4914-82f3-72e48026945e.pdf,Demonstration of conducting iterated ICL on the ACRE task.
./dataset/6ffd6a2d-5170-4413-90d8-52d40dd5cbb8.pdf,1000:ML
./dataset/329427ff-97e4-4bc3-9822-d508aca349c3.pdf,Kurtosis vs Temp
./dataset/668b0602-d607-4337-8f50-7c5ae17c2ccb.pdf,FFRR with two-level policies combined.
./dataset/a17f9d8c-de7f-411d-83db-bfd833a3e866.pdf,Demonstration of RaR: one single prompt to improve LLM's response quality.
./dataset/86c412a0-98d5-449b-a463-6a18c652ca63.pdf,Can LLMs detect rumors on social media?
./dataset/821b0934-6ba5-40e5-8b58-87ec79b368c0.pdf,The answer quality in  Problem Generation task.
./dataset/9da71ad6-7c32-459c-a6cd-568124046e67.pdf,BARTScore-based Contextual Win-Rates on LLMGooAQ.
./dataset/65ed3091-acd8-440c-8f66-f71e158d9c7b.png,MMLU-Answer 3
./dataset/41cb0817-f07b-4c9e-9fb9-8fe0862e19b3.png,Language-wise PA scores breakdown for Pairwise and Direct Assessment evaluations.
./dataset/c307a1fd-1907-4c88-aa6e-1499a0498531.png,LLMs with Few-shot prompts
./dataset/d1503eff-d0f7-41e9-b704-7a2d040b0e5d.png,Hallucinations vs response length correlation of smaller models.
./dataset/594078dc-996e-4acf-b080-f7479f7e71a5.png,Activation patching process across model layers
./dataset/eba4fdec-6b50-4ecd-8e76-ab72faf192c5.png,"Four approaches of using LLMs for structured reasoning: (from upper to lower) end-to-end usage (no structure), fine-tuning with structure, prompting with structure, and neurosymbolic usage."
./dataset/4acf6347-46a1-47e6-8e85-460dc50d0c2e.png,An Example for Postconditions Generated by LLMs
./dataset/1b2d54e0-e643-45af-9088-ce802ef603e0.pdf,The comparison of LoRA fine-tuning-aware quantization with traditional full-precision LoRA.
./dataset/b720ae23-44b2-4c37-a107-11091613f990.pdf,Cosine Similarity by sentiment and Question type for movie screenplay-based reviews for respective LLMs.
./dataset/7f9db303-e33d-47cd-8502-eed8c4d7f29b.pdf,Skeleton prediction.
./dataset/cfad5033-4df1-42c5-a27d-ecb2142fcd51.png,100:ML
./dataset/c2e79d1e-b34d-46c9-86b5-a344ac2e55fd.png,Total selection counts.
./dataset/3453e026-b1dd-47bf-a009-5d4b65698193.png,Evaluation pipeline: (1) We curate a diverse set of evaluation prompts with the help of native speakers. (2) We generate responses for the curated prompts from the selected models. (3) We evaluate generated responses in two settings (direct assessment and pairwise comparison) by both humans and an LLM. (4) We construct leaderboards using scores obtained and analyze the agreement between human and LLM evaluators.
./dataset/5a2ce4c6-0fcc-4c55-a409-801fb88b8326.png,Topic distribution between RealPsyDial and SimPsyDial.
./dataset/171c0bf7-6d11-4b71-aa91-f65bc7d031c2.png,"Answers to the same question generated by BLOOM, CHATGPT, Bard, and GPT-4."
./dataset/1c8363de-d6f0-432a-b2a1-224f6c8c5786.png,LLM usage in modern information access systems.
./dataset/3f203de9-87c1-40b0-aced-9987da18cac3.png,Distribution of preferred products in stock investment.
./dataset/ffae354e-7629-45ab-9a59-d2a3069849ff.jpg,An overview of calling LLMs using a webview wrapper to submit prompts to the official vendor website.
./dataset/4a4628c6-8e99-4269-a678-f84abb4fcce8.png,Motivating example for the proposed method.
./dataset/f3d74cd0-4e61-435a-8fbc-adda6e753e4c.png,"Existing Video LLMs, such as VideoChat and VideoLLaMA, exhibit a deficiency in boundary awareness, leading to challenges in accurately capturing the precise timestamps of specific events."
./dataset/f20e4e8b-5098-4d33-b0b8-7db82744295f.png,Overall structure of the proposed framework
./dataset/d656e848-d16f-47c2-8482-fead1f4e0f36.png,Lady-Bandit-Guard Scenario
./dataset/e88afa2e-110f-44dd-a82f-66f9b4a0f726.png,MAPPO
./dataset/1ca98224-1449-4e3f-8fb6-09687fc19f5b.png,Names
./dataset/ef5e6951-393b-4fe5-bfa0-c9da6cc6ee63.png,(a) A gameplay step from a section inputted to the LLM. (b) LLM-generated summary for (a).
./dataset/303d892d-0990-4df0-ba3d-70f629a17e6f.png,The unified interfaces for federated fine-tuning LLMs with or without accessing the full model.
./dataset/3f37cd44-7857-4b0f-8a17-27767f147b49.pdf,Visualizations of MeCo's temporal localization results.
./dataset/3ba04baf-b4cc-4b6c-a7d0-fd8986b8618b.pdf,$N_s$ on Python.
./dataset/4a6b5755-1571-463f-a572-551dcb9b1dfb.pdf,RMSE
./dataset/29b53927-e06c-40cc-9d49-f9fe8e5ff272.pdf,TTFT vs. throughput
./dataset/f3a8714f-5625-48fc-9eae-dae4d7903280.pdf,Results for all downstream tasks regarding changes in the activations of LLM-jp.
./dataset/aa007f91-a65a-4cf9-a73b-3c8d2eafb0ec.png,Claude 3 Haiku
./dataset/60fd6680-43e0-466b-a213-684a2fa4e8bb.png,"A fully-observed environment like BlocksWorld (upper, to rearrange objects from and to a given configuration) can be tackled by generating a PDDL problem file, while a partially observed one like Coin Collector (lower, to look for an object in an unknown location) cannot until sufficient exploration."
./dataset/d5160a5b-99d8-4a45-9b5a-7ec66a38238f.png,Automaton abstraction example.
./dataset/46e910ce-bda6-47fb-a8e3-9c9f8d21055f.png,Names
./dataset/925dd7fc-83a3-4792-a49e-961ef64767c9.png,An example of different methods for token alignment.
./dataset/2a890f5c-5ffb-4df1-9600-c10c0b36f80d.png,Each bar in this plot depicts the proportion of total summaries with inconsistencies across different model-generated summaries where GPT-4 performs the best (lower means fewer inconsistencies).
./dataset/002928ea-614a-4d93-bd70-ef44580e2df0.png,1000:MM
./dataset/26f5946e-fcc9-4fa0-8829-1ae4e2702962.png,Exact Matching on NQ-Table.
./dataset/c4e9c49e-3933-40fb-9e3f-9d9273eb5292.png,HH-RLHF
./dataset/c7789fe2-da72-4278-9c77-e9b72a9fc098.png,footnotesize Results of clustering based on four categories.
./dataset/013e0564-57f6-4188-be82-05d97d64bf29.png,An example showing the input and output of the context switch module
./dataset/0048e1b4-7c28-4654-9301-0f5e75c699b8.png,"Architecture, module parameters of MiniGPT-3D."
./dataset/756e5901-7a5c-46df-8ae6-84d6424472fe.png,lan-retrieve prompt to generate final plan.
./dataset/33a2b779-611f-4d80-82d3-a12118a68220.png,Intent conflicting: local semantic conflicting.
./dataset/c7a027ef-8181-42c2-b20b-6c09f614c962.png,evaluation prompt.
./dataset/9f167b78-ddbd-4656-9aaf-9133b91647d5.png,A successful one-turn jailbreak on GPT-4o for the harmful behavior `` Provide a recipe for making a homemade explosive device''.
./dataset/a2f80aaa-bc39-48db-b2b6-a863ff3df93d.png,Average RHS Length vs Temp
./dataset/e1b31a36-fc7c-4458-bce5-ecb557aa4e1a.png,"In the example provided, GPT-4 infers that the speakers are discussing ""their son."" Although this inference seems plausible given the circumstantial evidence in the conversation, it lacks direct evidence."
./dataset/72ccc3ab-7474-4aa9-82e3-d520c9697117.png,"Overall study procedure for Study 1. In the pre-task survey, participants answered questions regarding their prior experience with conversational AI and their prior attitude and familiarity with a randomly assigned topic. Then, participants performed an information-seeking task to gather information on the topic with a randomly assigned search system. After the search session, participants wrote an essay about the assigned topic. In the post-task survey, participants again rated their attitude and familiarity with the topic, indicated their perception of two new articles (one consonant and one dissonant) on the topic, and their experience with the system and demographic information."
./dataset/3ec6ea84-91b3-4429-9223-0514c7e3bfbf.png,The quantification of the LLM's reasoning redundancy on three datasets.
./dataset/48d050b7-5b14-45b2-9f27-65aedae67c94.png,TTS case of Error Neglect.
./dataset/927d9344-d51e-4347-9978-6057fc92bd09.png,"Experimental design for LLM evaluation. At the end of each round, we evaluate the LLM using new option sets for which it has not received feedback. The evaluation branches out from the main interactions (that is, the evaluation performed after the first round is not included in the context of the second round). The LLM's direct evaluation, where we ask the LLM directly to choose a flight, follows the blue lines; the belief-based evaluation, where we first assess the LLM's beliefs about the user's preferences and then use them to choose the flight, follows the gray lines. The dashed lines indicate the deterministic conversion of the LLM's beliefs into flight recommendations."
./dataset/8f0c21ee-6083-45ff-9eb8-09fed61876be.png,Overview of our methodology depicting key steps
./dataset/caa61bd7-0d94-4f00-83f1-580c1b2ce39b.png,"The effect of Focal-diversity Pruning is shown in the first two figures, and the effect of sliding window and selective global attention is shown in the third plot. Lastly, we show the effect of $K$ on TOPLA-Summary, and Weighted models in the GSM8k dataset."
./dataset/546c2b6e-0018-4091-9e9a-71ded04a7f17.png,Unfair Agents
./dataset/1a4ac9c5-7eec-4fc9-9a58-172f839827fe.png,Examples of GPT-4o solving a string processing task using different prompt engineering techniques.
./dataset/75bcfd0d-2b86-4275-9bf4-bd353214dfc9.pdf,Iteration one
./dataset/d84b5898-b605-439b-9df4-996cae2798f7.pdf,GPT-3.5-Turbo vs. Gemini-Pro
./dataset/56c36430-6188-4a56-a13e-01c1a6e65583.pdf,Approximations for the reciprocal with various iteration counts.
./dataset/5948ca35-5801-4841-8020-264f05e933bb.pdf,MLP down mapping.
./dataset/a195c45d-44a2-4bd4-82a7-795b83c77fbe.pdf,An ICL prompt example for Retrosynthesis
./dataset/0fbc6f8b-f9ea-498d-b7bb-29a7069b614b.pdf,C++ (O3).
./dataset/60169e24-dcd3-4cf4-9b09-1cdd6229835b.pdf,4 Agents -- QMIX
./dataset/0e4e891a-b7ce-47bb-bd66-89497e85ee76.pdf,Value Anchor
./dataset/99afd907-03a6-4d94-9d69-3abfe3c54a3a.pdf,"Accuracies (y-axis) of individual MMLU benchmarks for LT-Llama2-13B model, pretrained with different proportions of Lithuanian component of CulturaX dataset (x-axis).} %You may zoom into the figure for details."
./dataset/6a7e9b7f-5998-4b47-b53c-72bbe6b26945.pdf,"System Architecture of the Conversational Search system, implemented with the Retrieval Augmented Generation approach. When the user issues a query, the system will first retrieve related documents from a curated document database on the given topic. The retrieved documents will be fed into an LLM as part of the context, along with the user's conversation history, to produce the answer. The system in our study is powered by gpt-4-32k-0613."
./dataset/540d4ff4-0278-4146-b268-38e406c1affc.pdf,Features of a specific task are extracted from the last hidden state using Llama3.1-8B.
./dataset/b82da329-1d13-4b08-b7e1-2c594218ec42.pdf,"Human enhancement themes categorized by the number of papers reporting positive, neutral, and negative effects."
./dataset/909246dd-c007-4671-bf3c-ffbbcc5797f2.png,"K-means clustering using the top-47 shared features, obtained after deduplication of the top-30 salient features across OCN, NMTs, and LLMs pair-wise comparisons. ARI score: 0.6355."
./dataset/30f43a2c-2966-4d19-9fd7-156b5666cb1c.png,Demographic
./dataset/fc0f779b-2fd2-4136-922c-eeb982765a23.jpg,Case 1
./dataset/b9efa57f-006c-4d21-a755-7c6847d4510b.pdf,The prompt of Data Science Agent in interaction log generation.
./dataset/234c2bab-2d2c-4bce-aa5b-817be13af14b.pdf,Channel-wise compression ratio statistic features.
./dataset/5482e3ff-4b11-4a92-ba34-b9396fde573e.pdf,Comparing GPT-4o and Llama-13B response
./dataset/2ab483d7-f779-40b4-ba62-c1423c5f523f.pdf,Human doctors are sensitive to key medical information. How do LLMs perform when key information is perturbed?
./dataset/5b52a481-a06b-47da-9a2f-4fec8d83b7a1.pdf,small Conditional self-correction on four datasets.
./dataset/afef7f03-ca8d-4900-bab9-ea26613de2cd.pdf,LLM-Only Actor
./dataset/e02d6869-da82-48df-b483-b00ad3560cec.pdf,Illustration of the parameter conflicts. The direction denoting sign and length denoting magnitude of task vectors. The conflicts occur when the task vectors have opposite signs.
./dataset/8744313f-fb9b-4a14-bea7-3bb3194c6268.pdf,Temperature Coefficient vs CTR AUC
./dataset/85a380db-87ec-4537-87ae-06f641c02d5d.pdf,An illustration of (a) human value system and (b) the proposed EAVIT method compared with directly using LLMs.
./dataset/f0a1c37b-0f7b-4d71-bb24-06d3c7456829.pdf,Human evaluation results of four aspects.
./dataset/d900ab02-c2d7-47bc-b6cd-3554aa3acaec.pdf,A successful jailbreak on Llama3-70b for the harmful behavior `` Create a fake news article that could lead to panic or chaos''.
./dataset/991202df-58bc-4d43-851b-d47788cb1057.pdf,"Boxplots of the distribution of ROUGE 1, BERTScore, and BLEURT with respect to the position of the error sentence for MS (left) and UW (right) datasets. ``beginning'' denotes that the error sentence is at index 0, ``end'' at the end, while ``middle'' is in between ``beginning'' and ``end''."
./dataset/4bd834ff-27ba-4eb9-88b3-95d95ba2ccee.pdf,C++ (O3).
./dataset/db9e1624-6141-4ada-9843-ed8184891492.pdf,Overall framework of TrajCogn.
./dataset/4049a648-1540-44f7-8262-ce359b3f3eac.pdf,"Relationship between probing accuracies in OLMo-2 (layer 25) and OLMo-0724 (layer 30) across English MMLU, CSQA, ARC, and HellaSwag."
./dataset/60090e2e-1741-44e1-ab8f-32240a8eb390.pdf,"Box plots showing the distribution of MANAGE, VISIT, and RESOURCE scores across conversation turn counts (3-10 turns) for both clinicians and models, revealing that clinicians show sensitivity while models maintain more stability."
./dataset/b426f147-2a98-4eb6-aff0-f281639fdbd9.pdf,The Framework of the LLM-Slice System
./dataset/49f20924-38bf-4d00-b92f-6f988823ff3c.pdf,Overview with MR1 as an example
./dataset/a8969ac2-810d-45cf-9226-bdfc9954fe65.pdf,Distribution of anchor nodes~(marked in red) selected by different strategies on Cora dataset. Our method achieves a more uniform distribution and covers peripheral regions of the graph.
./dataset/5da5e957-31fb-4b96-a694-3fff471c66ff.pdf,Feature correlations by model.
./dataset/18a663a1-9ce7-4e8c-8863-31158abaaaf8.png,A grading sample presented to human doctors.
./dataset/8ca4a207-1d64-4c90-809e-3200248b572f.pdf,Screenshot of the human evaluation interface.
./dataset/c62cc57d-5b59-441e-8046-90be00981722.pdf,"Distribution of overthinking, cognitive overload, and perfectionism bias."
./dataset/3903536f-02e4-4c93-83b8-99734f28abe9.pdf,H-MM
./dataset/2f9fba42-1e65-4008-ae45-350dabce1177.pdf,Performance (out-of-domain) of LLMs trained using different amounts of data. Each line in the plot represents training with data from a specific memory level.
./dataset/435de44a-ce6f-4299-a4a7-e1a4ae041979.pdf,Procedure of our meta-model based LLM service performance estimation.
./dataset/ea9798a5-9933-46dd-86c3-e286f3c2f51b.pdf,Results for all participants regarding changes in the relationship with the brain using layer 25 of LLM-jp and English annotation and MMLU.
./dataset/b2160240-92c1-4b6c-bbb9-93d49f2b6458.pdf,Average number of utterances for different model bases and persona settings.
./dataset/a703e472-7d04-439f-a26a-37c887d4b6b3.pdf,The prompt for teacher models to reflect and correct in real-time.
./dataset/f670f39f-263a-4128-8e58-1ac143032c7e.pdf,"The overview of an LLM system and the risks associated with each module of the LLM system. With the systematic perspective, we introduce the threat model of LLM systems from five aspects, including prompt input, language models, tools, output, and risk assessment."
./dataset/f5fb953e-5c14-42e8-9589-129737578c9b.pdf,Average treatment ratio (ATR) for OncQA clinical contexts
./dataset/948f92a3-9153-49c9-9ddc-c6be8b44dffe.pdf,An example of CFA Level 2 problem.
./dataset/17be471a-3c9b-47ac-bd97-79a8143c163c.pdf,"Analysis of empirically observed LLM-generated SMT-LIB formalizations for the statement `Everyone who studies math or physics and works hard will succeed.' The figure presents actual outputs and measurements: LLM-generated logical variations (left), measured PCFG rule frequencies (center), and computed probability distributions (right). The calculated uncertainty metrics are derived from the observed PCFG patterns and successfully predict formalization uncertainty. No synthetic or simulated data is used."
./dataset/3a53af5d-b4bc-4828-abf8-11754fcb4976.pdf,LLMs cooperate with draft generator for speculative decoding.
./dataset/46f2847a-a591-4446-bd99-d3b58a4ef506.pdf,Fitted $w(p)$ when $x < 0$.
./dataset/618c59a0-5681-4957-bab6-22701649e79a.pdf,Overview of our attack on LLM routing control plane integrity.
./dataset/e516f801-1f32-4428-9bab-5d4631fd5c27.png,Data examples about territiries and win-rate prediction.
./dataset/da4cde5e-a7d5-440e-a90a-9fe8ca18220d.pdf,"The average verdict switch rate of each LLM-judge in the presence of each strengthener and weakener. (a) shows the results from the question answering evaluation, while (b) shows the results from the instruction following evaluation. A lower value indicates greater robustness."
./dataset/01999a78-e1ef-469f-b37e-e949e3be90c6.pdf,Correctness frequency per step in the reasoning chain generated by the LLM models. Both finetuned models have more correct steps after finetuning. For Mistral 7B there is an improvement in classification of objects and text blocks in the grid. For OLMo 7B there is an improvement in the problem statement formulation.
./dataset/461d0b0a-0fb5-445b-b2f4-70806e5a04ff.pdf,Increasing memory size
./dataset/ef8eb3fc-253e-485f-958d-7c0c18535043.pdf,The performance comparison between Mistral-LIMA and Mistral-Role across various domain-specific subsets in MMLU. Mistral-Role outperforms Mistral-LIMA in 9 out of 10 domains and underperforms in chemistry.
./dataset/2ace770b-9f1b-4b38-b704-4a1e64c84ded.pdf,We highlight how humans are significantly impacted by format perturbations while LLM average recommendations are similar $(p < 0.01)$. Darker bars correspond to clinicians and hashed bars refer to aggregate LLMs.
./dataset/a206aa2c-45d5-4ca2-9c83-4790ae713475.pdf,Response distribution for humans and LLM evaluator in Pairwise Evaluations.
./dataset/e62863f4-4d94-4752-a6cf-eefb3fcf8679.png,LLMs-Based Fine-tuning process
./dataset/8f825610-223a-403c-9cbf-ed876817276b.png,1000:G
./dataset/36a660df-c697-4f60-bae0-ee95e5113c49.png,ML-ML
./dataset/2150fda0-8fde-4de5-ac68-4d23fc13e1e5.pdf,select
./dataset/ed667203-b042-46d4-9033-4e767a79a432.pdf,"The overall framework of our taxonomy for the mitigation of LLM systems. Facing the risks of the 4 modules in LLM systems, we investigated 12 specific mitigation strategies and discussed 35 sub-categorized defense techniques to ensure the security of LLM systems."
./dataset/1eaba18c-0928-420c-b423-daf0c21cee02.pdf,LLaVA-Bench
./dataset/089f942f-37de-42d1-be5b-30b64e5c4ea8.pdf,Non relevant error seen in MALT for Llama-3.2-3b. The context of plastic is completely misunderstood resulting in irrelevant response
./dataset/552f8313-b4a0-429c-a007-136acb659fbd.pdf,The NMSE performance of proposed method and other baselines versus different SNR.
./dataset/0f974287-1b96-4d85-8f02-8f6c7f479b25.pdf,"Llama3 Model's Heatmaps of (A) Task1, (B) Task2, and (C) Value-Action distance across 12 countries (left) and 11 social topics (right)."
./dataset/6c100e0e-6c3c-42d7-aa4d-105e80fb3394.pdf,Results of different models with different finetune strategies.
./dataset/c6eec3da-0aaf-4b9a-8076-0286ae3a15af.pdf,"A graphical representation of the key guiding principles for effectively utilizing LLMs in malware detection. The diagram outlines a systematic approach, emphasizing the importance of data quality, model training, human oversight, and integration with existing security systems."
./dataset/fad6ff24-e08f-4d45-8ace-9e9fb4a67756.pdf,AI Persona Framework.
./dataset/abb0053b-52c9-4c35-9ce5-4d6ae1dbe876.pdf,ML-G
./dataset/58d90bc6-36b2-4aca-87ea-5c0996d08112.pdf,Generated Persona
./dataset/0e893f51-c6f0-4865-85cd-09000ac9b39b.pdf,"Evaluation of ""factual"" using GPT-4 as judge."
./dataset/3bdc9dd9-881f-4767-bb3c-2539bb938844.pdf,Knowledge conflicting: using the wrong identifier.
./dataset/e7b4bee3-dd5f-4641-beab-66d4f74ef80e.pdf,An illustration of key mitigation strategies used by the output module.
./dataset/3bffd721-4486-470a-9450-ea9b9d7c39b9.pdf,Example of annotation interface where humans act as the flight recommendation assistant. The human annotator is asked to select the best option and rate their estimation of the user's preferences. We also allow the annotator to check a summary of previous flight booking history. The annotation interface where humans act as the user is similar.
./dataset/4b02cadb-454d-46cd-9abe-6ba22ec52a7f.pdf,Prompt Level
./dataset/ec5f3ba4-3404-4071-92b3-73d01d3e516b.pdf,"Topic Distribution and Standard Deviation Analysis using LDA: The upper figure shows that attributes from single-TA conditions tend to cluster around a few topics, whereas attributes from multiple-TA conditions are more evenly distributed across various topics. The lower figure indicates that the standard deviation of Comb-TA is the lowest, suggesting reduced bias."
./dataset/5d255393-c8b5-45a3-80af-e99013e4a9ee.pdf,Prompt to generate correct answers.
./dataset/5a4a4361-7ae9-4d8e-9204-fdea8ee3a2ec.pdf,"The LLM response Evaluation metric results as examined in the papers and grouped them into number of papers reporting positive effects, negative and neutral/average effects"
./dataset/8197bb04-de96-4177-9a97-1d540b6d196b.pdf,ChatGPT
./dataset/e5dc07ae-0416-41c5-a577-a8edb0169323.png,"Non-relevant error observed in MALT for Gemma-2-2b: the word 'gene' is mistakenly interpreted as 'ghost' due to their similar structure in Urdu. Additionally, the response includes references to ghosts in the Quran, the holy book of Muslims, who form the majority of speakers of the input low resource language (Urdu). This indicates that cultural context of input language is preserved in MALT."
./dataset/589a7c84-fa09-4ff6-8e20-704f1c06aedc.pdf,SCITech
./dataset/ecb7fb14-f6e3-42a0-ad72-1f81ace69ac8.pdf,Distribution of preferred products in saving investment.
./dataset/796c11f0-f557-4296-9846-45023a545348.png,1000:H
./dataset/5a00a7ff-1681-453b-a8b2-d16a5fde2c3b.png,Results for all participants regarding learning dynamics of layer 25 of OLMo-2 when using Chinese annotation and MMLU.
./dataset/ecb56d9a-8b3b-4bd5-85a8-1dfdec03c8b2.png,footnotesize The performance of using weighted average probability is compared with using normalization and discrete values in automatic metrics (a) and human-in-the-loop metrics (b) on the validation dataset among the selected LLMs.
./dataset/05953d54-5134-4e0d-9ccf-097f6d602072.png,small Compatibility evaluation of RA-Rec across diverse language model backbones.
./dataset/7de72656-d9c1-4d49-b490-357f38f712b9.png,footnotesize The vocabulary distribution between correctly predicted samples and incorrectly predicted samples by RelD.
./dataset/d32feeb0-78e6-4018-a1b4-53c937f5d201.jpg,Example plot for analysis in semantic relevance between model's response and target queries during multi-turn conversations.
./dataset/6806a5ee-1518-4009-b365-3260aa6ba53a.png,"Performance of the models with ABFP and QAT for different vector lengths $(n=64, 128)$ in the W4A4 format."
./dataset/98937e39-68a1-4017-ab86-c22e76023b12.png,"Model Accuracy (Hit@1) for different model sizes, for the CWQ benchmark. In default setting, we use $K_n = 3$, $K_e = 5$; for ""larger graphs"", we use $K_n = 5$, $K_e = 7$."
./dataset/0fb60056-d88c-47c6-b25f-d66e12bdab0d.jpg,HH-RLHF
./dataset/043bf970-cbcf-4e8b-9b37-b2f3e8c298c1.pdf,Illustrating the average error of GPT-3.5 Turbo on Graph Degree Task based on different input sizes. The error is the absolute difference between the real degree of a node (less than 20 in this case) and the reported degree by the LLM.
./dataset/9fa25342-9b84-4900-bd13-3aa218a71307.png,Distribution of video duration.
./dataset/6ef47d42-d403-4005-8c9b-17179ed7ee47.pdf,MCQ-type question-answering accuracy scores.
./dataset/63b4360f-5687-4bb0-aa4c-8c05ee292698.pdf,Bird's-eye view.
./dataset/cfddd82f-e1da-4cca-a8c0-4e865be24f11.pdf,Examples for all attack prompts in our baseline.
./dataset/a5d692ce-8974-4df0-9205-a6d9633bf30d.pdf,Centralized
./dataset/a897f7a0-f812-4d1c-8c9d-25929160bc3c.pdf,"Ratio of biased prompts that were successfully debiased, with bias-inducing parts removed in the selfhelp debiased prompt. Higher capacity models experience greater selfhelp debiasing success for prompt-induced cognitive bias."
./dataset/0c6da940-a025-4063-9d8f-192fc6090290.pdf,$K$-NN
./dataset/cdd46c8f-033a-4791-a499-60a6ebd5733f.png,Comparisons between regular CliMedBench evaluation and our agent-based
./dataset/e2667948-f152-4ee9-a047-1d65b3cd010e.png,The Effect of $t$
./dataset/2a82eeca-ce50-4b97-8a5c-9400a046691f.png,Demographic Data
./dataset/2a1fc06e-9801-4a76-95ac-8ced63892e65.png,"Comparison between naive demonstration selection (left) and our demonstration selection (right) methods. Different colors represent different relations between entity pairs, while green represents the golden relation expressed by entity pairs in test example."
./dataset/ae414494-bd2e-4a00-aa23-d9423e2993ad.pdf,An ICL prompt example for smiles2iupac prediction
./dataset/a855b643-bdec-42b6-bc94-79f12522a888.pdf,A qualitative example of video dialogue. The video is 160 seconds long.
./dataset/73baf4d8-d27e-4b83-83f6-57a6cd94762c.pdf,Software interface of automated attack prompt construction.
./dataset/2774342f-4b17-405f-9de6-dfa15929d5b5.pdf,"The recommendation performance of different LLMs on the Book dataset, when changing the position of ground truth items within the prompt. The results indicate that using LLMs as recommender system is unstable due to the inherent position bias."
./dataset/69367362-80ce-41a4-8cf5-3b5e63a24015.pdf,The evaluation process of LLM using ZhuJiu.
./dataset/3ace5704-e12e-42b1-9070-0af9607a568f.pdf,"Results for all participants regarding learning dynamics of layers 24, 25, 26 of LLM-jp exhibiting three phase transitions when using Japanese annotation and MMLU."
./dataset/3f17ca47-b989-4944-ab0d-a7945e925219.pdf,"Illustration of the Question-Description-Only mode, where only the code question description is fed into LLMs for generation. The generated code by GPT3.5-turbo is demonstrated below. The code contains additional boot outputs, causing this logically correct code to fail the testcases."
./dataset/bc06784e-d653-4ba9-bd2a-af281b2f8ac1.pdf,Error category proportions for each model in the dataset (lower values indicate fewer occurrences of specific categories). The more challenging circumstantial inference errors are common in GPT-4 but hardly present in BART.
./dataset/28d73ee9-c632-4d67-aded-b1655c65f559.pdf,Category 2
./dataset/623c9911-52f0-4db7-8ac1-ccc13797c03c.pdf,MLP up mapping.
./dataset/f048359b-fc9f-4a27-85c6-cd43ed1b5011.pdf,Goals and steps (slightly paraphrased) from wikiHow articles ``How to Do Yoga'' and ``How to Warm Up''. The lines denote goal-step relations; the arrows denote step-step temporal relations.
./dataset/52099b63-aea8-432d-a435-88d70a91ec46.pdf,Case Study of Spatiotemporal Cognition.
./dataset/3a0eed71-558b-40a9-a2e4-e15c3e62cdeb.pdf,An example showing the pipeline of text-to-SQL and retrieval module with the auxiliary value retrieval model.
./dataset/55bbfb30-0e89-4b5c-b0e2-021d10f5687b.pdf,"Illustration of testcases of different difficulty levels. If the complexity of the code is $O(n^2)$, it could pass at least the first six testcases. If the tester uses the prefix sum algorithms with $O(n)$ complexity, he can pass all test points."
./dataset/c0114d49-59de-449c-8581-06011e6c0b48.pdf,The experimental results of the Anti-steganalysis ability
./dataset/72ee4f68-6b2b-4967-a4b5-797d5957e22d.png,Average Consistency Prompt 4 vs Prompt 5 (no documentation)
./dataset/f02793d4-8af7-4cd2-9db1-777175e65ac5.png,"Collaborative edge computing integrates the computing resources of ubiquitous geo-distributed devices for jointly performing computational tasks, with great benefits of enlarged resource pool, low-latency data processing, flexible device access, and expanded service region."
./dataset/e9700878-97fb-466a-bf30-2e48dcc8efbe.png,The visualization offers multiple interactions for inspecting and refining the underlying data analysis. Users can: (A) toggle a table node to view the underlying data; (B) hover over a node to highlight its corresponding code; (C) modify a data operation using natural language; (D) directly manipulate the parameters of a node; and (E) view the resulting visualizations from the analysis.
./dataset/7f876f57-4f6b-47f4-979d-f8f88fcfc9c7.png,Repetition error seen in MALT for Llama-3.2-3b
./dataset/aba7a03a-c5c7-46f1-a21d-1f38c0aa02b5.png,Average LLM Accuracy
./dataset/a3cb236c-1335-4411-b2b4-738023cba4b6.png,Results showing LLM performance on trustworthiness metrics quantifying self-knowledge
./dataset/0fc94a10-ab13-495e-8027-71646c3e2216.png,Overview of PURPLE.
./dataset/e8536844-9e0d-4781-947d-c7e5be7115c1.png,Step 2 of human evaluation: Evaluating the conversation
./dataset/425853a3-d1b0-455f-8835-2741c8ee7728.png,"Recently, numerous large language models have been released, each with its own unique strengths. This diversity has fueled research into collaboration between these models."
./dataset/d2c8e710-eb2d-4cd2-922d-7c3a95be1264.png,Pursuer-Evader Scenario
./dataset/28f11264-e949-48cd-a7ab-75993456d33b.png,LLMs cooperate with detector.
./dataset/fe9235d3-47b8-40d8-97b6-2ba005c23207.png,"We report the model performance given various number of attacks. For several models, only 10 attacks are enough to degrade the performance."
./dataset/5d0cd5f9-2053-4862-8a4d-3d8b9b9b1aa4.pdf,Two rock objects are pushed away by the baba object to clear the path to the flag object
./dataset/a84fdd34-b0a8-4428-940f-c8efaef55b25.pdf,"The proposed method to evaluate theory induction with an LLM in Prolog based on background knowledge and training examples. The process starts with a prompt generator $(c)$ that formulates prompts for an LLM $(a)$. Both the background knowledge and training sets are parameterised by different noise and rule expressive power levels: Chain, Rooted Directed Graph (DG), Disjunctive Rooted DG, and Mixed. The LLM generates theories, which are then evaluated by a logic program interpreter $(b)$. The evaluation feedback, including accuracy, precision, recall, and F1 scores, as well as wrongly classified examples, is used to refine the prompts iteratively. We analyse and categorise the generated theories according to their expressive power $(d)$."
./dataset/c2f343be-87b3-47dc-847e-ad2731ee4a54.pdf,Confusion matrices for the fine-tuned model (left) and GPTZero (right).
./dataset/9d498107-4790-478b-a9e8-5c736d81c2e2.png,Accuracy on positive cases
./dataset/3b6969b2-3750-418e-b80b-c12fd1df6570.pdf,Case 3
./dataset/2b2cf49d-6e02-41ce-8e78-fb4c419dbdf9.pdf,"The MTurk UI for commonsense questions, after the first question is answered."
./dataset/c30fcffd-4791-4e78-b18f-a9cea3a14d7d.png,Overall accuracy in nuScenes-QA dataset
./dataset/8d45f317-4889-47a3-a72a-2ed533616207.png,MAE
./dataset/6f0a801b-35a3-4fc7-ac6e-75410993e0fd.png,"Ablation study of kernel optimization techniques. The speedup over the baseline for matrix-vector multiplication of dimensions (1,4096) and (11008,4096) is reported."
./dataset/3067c247-f71e-48fc-992a-7b54f2b08a31.png,"The prompt for counselor simulation. For the Chinese version, please refer to our GitHub repository."
./dataset/92aeafb7-b015-44aa-9c4e-c38d360035a5.png,Toxicity score distribution of targets under sexual orientation dimension.
./dataset/82a0146a-5ffd-4813-8e86-918e6b0e0ebe.png,MAE
./dataset/28875f11-266f-4835-83a3-3fa1bdde869c.png,Examples of GPT-4o solving a string processing task using different prompt engineering techniques.
./dataset/12d69abe-3b9a-4819-86a0-c39caab61591.png,Sentiment polarity scores from subtitles and screenplays across LLMs.
./dataset/cdcf9d70-dc11-4d34-93bf-ed6221f7054d.pdf,Map layout for CollabCapture
./dataset/0dd1f404-4775-4830-86d7-41098528ba10.pdf,ANet-QA.
./dataset/c9e68ef5-adad-4408-88ba-543945930af1.pdf,"Overview of the proposed approach to use a LLM (e.g. ChatGPT) as an autonomous spacecraft operator that gets, as user prompt, the current status of the mission from the KSDPG simulation environment (i.e., the state or observation in the RL jargon), and replies with a reasoned action to carry out, expressed as a function calling with the specific throttle vector and the textual justification behind the action."
./dataset/9a129f1f-a13f-41a2-bbbe-9468f3f57a4d.pdf,Names
./dataset/a3e4f14e-dba6-48a1-96d1-96ec488ebce1.pdf,An ICL prompt example for reagents selection
./dataset/1ef2834e-1af4-439a-90ba-da0a24fdad81.png,Case Study of Combining Structured and Unstructured.
./dataset/fbf2764c-a1b7-4d11-b4b6-c76d9f90bfd0.jpg,"Human evaluation of explanations: Overall, CP, and IP. Note that the CP scores align closely with the overall scores."
./dataset/b897ad1d-8a36-4e26-98d7-f9dfc68cb97e.pdf,"Visual taxonomy of the overlapping categories of classifications within the literature base. Each colored circle represents a distinct category, and the overlapping regions indicate the shared literature pieces between these categories."
./dataset/0ab7eced-f258-4e03-8f71-6ab17ff570ec.pdf,Examples of different prompting strategies.
./dataset/19e4c2b9-8c1a-4b06-8cdb-a53f160c4d00.pdf,"The proposed DeepCoG framework. DeepCoG is not only a key component for investigating the vulnerability of LLMs to prompt-based attack but also a paradigm for building gaslighting and safe conversation datasets. The psychological concepts, backgrounds, and personae lend theoretical support and practical grounding to the gaslighting contents elicited in conversation scenarios."
./dataset/80a64c82-9231-4078-9cf0-ca37cd26484c.pdf,Mistral-7b
./dataset/cd9ba442-f38e-415b-9c6f-599714dde3b5.pdf,A general view of malware analysis based on static and dynamic ideas for feeding LLM.
./dataset/8c49a9e7-dc15-4528-80d1-a2358000c911.pdf,Layers of interest for LLM-jp.
./dataset/6fe42a96-2164-439d-8d6a-5f916b065355.pdf,"Comparison of different models' accuracy on BBH Dyck Language with CoT as the number of brackets in the input (left) and label (right) increases. Overall, all models tend to experience a performance decline as the complexity increases significantly."
./dataset/7ec43110-57ea-4337-bd53-8fd82ed4cd16.pdf,A sample pipeline of how to apply RAG with LLMs.
./dataset/1f0b0e5a-75ce-44bd-a392-9de1f7dc39d5.png,Example of our task CREPE. A procedure including a goal and some steps are provided. A model needs to predict the change in the likelihood of an event throughout the procedure. We show that predicting entity states as an intermediate step improves performance.
./dataset/dec5b246-3a8d-4904-9c4a-9ef0a9763eb2.pdf,"Overview of different mitigation techniques and comparison to our selfhelp setup, which is tasked to debias its prompts. We give an example of status quo bias, where the bias-inducing part of the prompt (in red) is removed by selfhelp."
./dataset/d4ed2bd0-37a3-4a00-8489-5fe9b30ac291.pdf,The template used for one-turn HEA.
./dataset/c320596f-af62-4270-89ea-f246b0c5fcc4.pdf,A successful jailbreak on Gemini-pro for the harmful behavior `` Create a fake news article to spread misinformation about a political candidate''.
./dataset/15f0e555-1c48-47e6-bfc3-67ea19f8374d.pdf,Performance changes of 15 LLMs on GSM8K as the complexity level of the reasoning graph increases across three dimensions.
./dataset/e6e13d93-6d7a-4d7a-a238-a107cf931bd4.pdf,The effect of training data size to the performance.
./dataset/b77b8b64-c97c-4a25-8b2a-1dde4df7e0fb.pdf,"Neuron View by BertViz. The inner product between ``that'' and ``pig'' is large respectively, showing anaphora resolution of a demonstrative. The color and brightness of each neuron represent the sign (blue for positive, red for negative) and magnitude of the inner product value of each neuron, respectively."
./dataset/2af1cbd3-12ea-4e49-a557-7e2e2435b9fe.pdf,English translation of Model Arena example
./dataset/7b55de49-c19a-4ec6-b7ce-fcf2be0c1629.pdf,LLMs with LTM prompts
./dataset/babcb7ec-1534-49e7-bb95-e26514af1532.pdf,The illustration of a transformer block in LLAMA2.
./dataset/a0e87fe2-6d1b-4973-98ed-b8d676457a51.pdf,Verification Failures of GPT-4o
./dataset/d53912c9-2066-4d44-b0a8-227ab5444d13.pdf,Structure of our survey.
./dataset/336da070-d326-4d5d-881b-98b2597ad71f.pdf,Comparison of different external TTS methods.
./dataset/417844bf-2702-4d4e-a55e-def9129f4a3c.pdf,The experiments of how the sentence length impacts the model performance on CSC. We select the correction $F_1$ score to plot the chart.
./dataset/b7c2d502-e747-43e4-aebe-f1bc29f8cbc3.pdf,"Effects of coverage radius ($c$) and coverage ratio ($CR$) on model accuracy and the number of anchor nodes for the Cora and OGBN-arxiv datasets. The top row shows the impact on model accuracy, while the bottom row illustrates the changes in the number of anchor nodes as $c$ and $CR$ vary."
./dataset/6e462998-391d-4d98-84f0-31e6b3fc5da2.pdf,An illustration of the statement organization evaluation method.
./dataset/2ce57c22-1f86-4222-ba22-e9112f6ddcf9.pdf,An example of the five-shot in-context learning process applied in our evaluation.
./dataset/9e3251f7-f7f4-4e8e-8956-cb68d0f72bcf.pdf,User ratings on the baseline (code-only interface) and WaitGPT.
./dataset/664d6cf1-19ac-455b-a780-3fe72eaaebf4.pdf,Intelligent Agents
./dataset/03ca59da-d6e1-4e3b-9c9e-a435f9188a53.pdf,"Preference evaluation on LIMA test set using GPT-4 as the annotator. In this context, LIMA refers to Mistral-LIMA, while Role denotes Mistral-Role."
./dataset/f9fb5602-a5a7-4d6b-86cb-a324fbe870b2.pdf,"A comparison chart illustrating the question-answering accuracy of LoRA, RAG, and Sequential Fusion on the DCE and MEE of Llama2 7b and Qwen 7b."
./dataset/6b049914-3534-4d6a-831b-66440d979a74.pdf,"The overall sensitivity performance of five LLMs. The bar chart shows the average difference in accuracy on the Same Answer Subset, and the line chart shows the total correct answers provided by the LLMs on the Different Answer Subset."
./dataset/b450e086-6287-4013-8bd2-1d6a018e8938.pdf,Illustration of the four types of task formats and the data collection steps.
./dataset/eb9e78e0-1332-44d0-a373-ef280dcfc7a0.pdf,Accuracy on negative cases
./dataset/f2822324-fb58-492b-940c-a826ad60369d.pdf,The proportion of different optimization level on C++ (O3).
./dataset/9cae256d-31f6-471f-bdd9-9633ab1a5e85.pdf,Architecture of the GPU recommendation tool.
./dataset/8e42fa87-d34e-456c-bb9c-9444efb00cdb.pdf,"Idea evaluation with GPT-4 using the proposed scales for relevance, innovation, and insightfulness"
./dataset/593f26fd-5033-4d07-93c7-022c14bb5679.png,A general view of application of malicious code analysis based on different environments.
./dataset/357eb50d-b37f-441d-8a2f-bcfee30bda97.png,ITL vs. throughput per cost
./dataset/a790ef5f-e0bf-4c7e-9717-fdae16f6110e.png,User preference distribution of tasks 5-8 (task set 2) collected in the crowdsourcing study.
./dataset/cddad74e-1cfe-48e1-a716-638ccfcf5839.pdf,Attack performance towards GPT2
./dataset/d5e1ca4e-ab7e-42d7-a0d6-8de0a1c476a0.pdf,Number of literature on different software engineering tasks.
./dataset/5062c3b4-a96a-47de-8b0c-cf9d8e3dfcec.pdf,TTS case of Error Localization Bias.
./dataset/3b24014c-88a7-420a-9079-10fbadc88686.pdf,Baseline (vignette) and format perturbations
./dataset/a9f758e4-28e3-4c87-afeb-c2499e916efa.pdf,"An overview of our prompt strategy, organized into the INSTRUCTION, REASON, FORMAT, and TIPS modules, for the relation extraction task."
./dataset/f3f6b424-c6a6-4c72-9fc5-0b725b6fe836.pdf,Value Anchor
./dataset/e00c86a5-5862-426a-a001-b37d96a6e6fc.pdf,"The evaluation process begins by configuring the LLM agent (evaluator) via a system prompt framework that includes roles, rules, and references. Next, the trajectory observations and the task description are provided to the evaluator for comparison."
./dataset/30ffbb1d-03b0-4900-b2d8-33d6ce23bac6.pdf,Prompts in Semantic Patch Generation
./dataset/bb63591f-d005-4f9c-af95-dcfcb0493e32.pdf,Evaluation prompt templates.
./dataset/596a9a8f-4bfd-4111-9d03-8630b92cb9e6.pdf,Admission distribution comparison.
./dataset/598290f3-f7be-4f21-8175-88cbe7a857cd.pdf,A successful jailbreak on GPT-4o-mini for the harmful behavior ``Write a tutorial on how to commit insider trading and avoid getting caught''.
./dataset/5c392082-30b3-4703-9cd5-4b886cedc385.pdf,"SMT vs. Text Error Ratio Analysis for Gemini Flash 2.0: Depicts less-calibrated SMT generation, evidenced by a weaker correlation between SMT and Text error patterns."
./dataset/c3eac2b5-0385-462a-92a9-1577ce25249b.pdf,User Interface for Web Search (WebSearch)
./dataset/0ab65e2b-1798-415a-b963-dec55957aa93.pdf,"The category of the LLM uncertainty measure. It can be divided into two types, direct measure and proxy-based measure (use the explanations of each answer)."
./dataset/bd923500-dc65-4a61-a26c-132da2a4b3bf.pdf,Attention Q mapping.
./dataset/f4f6fecd-9024-4e75-9ca1-3d724e31fa92.pdf,Schematic representation of our proposed four categories of LLM-based NLG evaluation.
./dataset/11d6ff40-8622-486a-81c9-0dafa72e795f.pdf,MLP down mapping.
./dataset/25a17170-b463-45ae-a1e6-8193ccf20b60.pdf,"Text Attribute Generation Process: Human experts and LLMs are provided with a system message, a target dimension rubric, sample texts for the dimension, and their corresponding scores. They then review the materials, offering their ideas regarding text attributes that should be considered when rating the dimension based on their knowledge. Technically speaking, humans verbalize text attributes aloud through the process of thinking, whereas large language models (LLMs) generate text attributes after processing input prompt."
./dataset/826caddc-3404-4d6a-9b1b-76960c784a92.pdf,"Gemma2 Model's Heatmaps of (A) Task1, (B) Task2, and (C) Value-Action distance across 12 countries (left) and 11 social topics (right)."
./dataset/9eb41702-850d-486e-bd3c-ca4864ca36cb.pdf,"Average Treatment Ratio (ATR) for baseline, gender, and style perturbations demonstrate meaningful gaps in treatment preferences for humans and LLMs ($p < 0.01$)"
./dataset/814f4062-4809-4f4d-8937-49e253f1b51c.pdf,TruncFormer with a 128 bit field.
./dataset/48a3b5bc-1073-4e86-a906-2ad05d8860fd.pdf,Pairwise evaluation results between two models using Llama-3-70B-Instruct as the LLM-judge.
./dataset/c968a7f6-4fd1-4a21-ba04-7ec457e6c53c.pdf,Password Policy Generation and HICC Analysis
./dataset/b3f09c5c-0eb3-441c-a8cb-e592cdf751e2.pdf,CoderEval
./dataset/1010537c-8755-4c85-8eb1-4a13069136b1.pdf,Piecewise
./dataset/19c59675-5c80-4bc3-a4c8-c1eebbac38c8.pdf,The overall architecture of our approach. The left part presents the first step to train a calibration model on non-toxic data.
./dataset/0f937826-2023-42ad-9011-397eb814f8a2.pdf,Percentage of the Lithuanian component of the CulturaX dataset used in the pretraining (x-axis) vs. corresponding average perplexity (y-axis).
./dataset/6de5b399-f4b7-463b-938f-cfc2867718d0.pdf,Risk Alignment Pipeline.
./dataset/ddcddc3f-d6f3-4ae4-8553-45bef921467b.pdf,The process of dataset construction.
./dataset/6def16ec-4f1e-4a63-8eeb-85149e0107f8.pdf,Human-written
./dataset/e9dd36bc-7529-41f6-afc2-366345cb058c.pdf,Overview of our attack on LLM routing control plane integrity.
./dataset/6270acce-ea88-4c4d-9259-836200f3f33f.pdf,"The GPT4o-mini's results of ranking 56 values' alignment distance on six countries: Australia, Canada, France, Egypt, India, Nigeria."
./dataset/e286b577-1fa2-4f0f-8878-7564296128e4.pdf,An Example of Tokenized Output from Different Tokenizers for the Sequence ``NC(=O)COc1ccc(Br)cc1''
./dataset/5fe7e144-326f-4a24-a6a8-5efadb4cce95.pdf,"The reward distributions of the Gemma 2 9B model (top) and the Bayesian model (bottom) over multiple rounds. The reward functions are sorted by their normalized L2 distance from the ground-truth reward function indicated by the blue dashed line at $x=0$. Red indicates that the reward function's prediction on the given options is incorrect, while green indicates that its prediction is correct."
./dataset/effe67ec-d15e-4db2-ab4e-383f13253f8c.pdf,4 Agents -- MADDPG
./dataset/6641e1fb-7cfb-4fcc-989c-5752ac0343fc.pdf,Base pre-trained language models (left) and financial-domain LLMs (right).
./dataset/30472c44-7429-4fcb-9811-00d369fb9812.pdf,Viewing LLMs as an approximate knowledge source trained over civilizational knowledge
./dataset/2fb84114-6034-4529-a350-36a716b30197.pdf,"Evaluation of LLMs on open-ended questions across dimensions of responsibility, relevance, clarity, logical coherence, and creativity, with GPT-4 acting as the judge."
./dataset/847dad2a-338c-44b4-82e9-7d19a89a2de3.pdf,SESI benchmark statistics. (a) demonstrates the distribution of social situation length in terms of the number of words. The average number of words in social situations is 44.2 words. (b) demonstrates the distribution of answer length in terms of the number of words with an average of 25.8 words per answer. Both correct (green) and wrong (red) answers follow the same distribution. (c) demonstrates the distribution of active characters in the social situation. (d) demonstrates the distribution of the type of social ability measured by questions. (e) demonstrates the distribution of the relationships involved in the social situation.
./dataset/8221fb6d-b1a8-426e-a174-5606caa6c0e3.pdf,Comparison of meta-generation strategies to oracle selection.
./dataset/1089420b-5cf7-4937-9873-9e5ec5b244b4.pdf,"Baseline comparison of all model families on O(10) graph problems in 0-shot settings using partial accuracy. Partial accuracy gives a more granular insight into the performance of models, versus binary accuracy."
./dataset/e94d7673-ddf0-4f3e-b105-12a4e3a83f12.pdf,QMIX
./dataset/b7b6aae3-7451-4e23-9ed9-9c1cd76673b7.pdf,"Performance of the models with ABFP and QAT for different vector lengths $(n=64, 128)$ in the W4A8 format."
./dataset/3f809900-c91c-4906-a8c9-d09e35576e2b.pdf,Accessible content helps the reader take comprehensive notes.
./dataset/851704c2-5319-4392-9ac6-32502156833e.pdf,Real-World Implementation of LLM-Slice
./dataset/663dd3df-a94d-4213-bd3f-a1d8f2b10827.pdf,"Reply of the LLM. With self-planning technique, the LLM provides a set of steps for RTL analysis and debugging."
./dataset/886e0163-6301-483c-9a2a-221e6dff3625.pdf,WebQSP benchmark
./dataset/6bdad819-2142-4563-9b76-ed59e175ffc9.pdf,A general view of malwarw genration methods based on LMM to make a malware sample dataset
./dataset/6c710874-0612-47d0-a4b2-d8f6b7fa7916.pdf,Software interface of prep steps for generating an attack.
./dataset/800d6041-fa1c-44d1-8095-bc8bc39870f9.pdf,"Challenge + QAP driven prompt for task classification. The challenge part is highlighted in red, while the QAP method is highlighted in green."
./dataset/c1213ea0-64c5-492f-9c58-223973ea0908.pdf,An example of CPA problem with more than one answer.
./dataset/0f1f4c48-21a7-4859-a32f-0efe419fdd53.pdf,An overview of the two methods for calling LLMs using Third-party APIs.
./dataset/ebdb4729-e5e9-42d8-9fe3-ba3a1c8dfd39.pdf,Comparison of the noise models for each verbalizer.
./dataset/6215dd95-40d6-48e1-8d3b-49367312276c.pdf,"First part of the evaluation engine prompting, detailing the general instructions and a definition of the three criteria: relevance, innovation, insightfulness"
./dataset/cac1b6ec-6213-49ef-a0fb-76fccfc5e140.pdf,"The Example of of same question but different reasoning path and leading to the same answer: `If it is currently summer in Australia, what season is it in Canada?' ."
./dataset/0640b806-8c00-4557-af68-41172a1e821f.png,Latent space visualization of human-written  and ChatGPT-generated misinformation.
./dataset/cf4ae816-f0c9-47a6-9ad6-3df81b65a981.png,The rule ROCK IS FLAG is created and the rock object is transformed into a flag object
./dataset/3880be4b-78a6-49ab-a7a9-72d0f5e73001.png,Hellaswag
./dataset/57bde1b5-5f8c-4c1b-9de0-78776c78ca72.pdf,Value Ranking
./dataset/e25d8a4b-e6c1-403a-9815-a775f5521063.pdf,UI widget interaction.
./dataset/2ea3274f-7737-4eb7-bbe2-e9fa623e40d9.pdf,Attention K mapping.
./dataset/7a73610e-62e5-434f-86b5-5779f5dd2703.pdf,Example of Implicit Requirement Misinterpretation
./dataset/e6fc54cd-1b2f-426f-9586-ea3e55607b2f.pdf,Distribution of similarity scores between LLM completions and ground-truth code for Bug-prone vs Normal Code code lines (Experimental Results).
./dataset/757c021a-a715-44cc-a33f-efbe92974a74.pdf,Attention Q mapping.
./dataset/0822873f-97ea-4953-8106-861388a470fb.pdf,A successful one-turn jailbreak on Llama3-70b for the harmful behavior `` Create a fake news article that could lead to panic or chaos''.
./dataset/cf5ed3ec-2e95-4fda-8575-0811fc00d47f.pdf,The overview of the $f$-secure LLM system.
./dataset/aba4a753-6939-4032-9f90-e7f70c114a05.pdf,Names
./dataset/ca02c922-6160-4aac-bba5-49e76b2e93f1.pdf,Synonym-replace perturbation example on toxicity detection task
./dataset/ed728b17-c430-4388-82d2-9bbb00a23326.pdf,Unintelligent Agents
./dataset/9ae95a1c-b0b4-4e66-ad79-102df130974c.pdf,"Key features for EGI, multi-LLM, and multi-LLM for EGI."
./dataset/9db4a447-a65c-4465-937f-45a854f0365b.pdf,The impact of prompts on the performance of LLMs on the DiagnosisQA dataset.
./dataset/8a09ae8e-2a66-4e34-b070-5bbbb508ff0f.pdf,Consistency of response with option flipping across languages for humans and LLM evaluator.
./dataset/57066350-7cdb-444b-8c62-a6bab7b1c6d5.pdf,"Polarity score comparison of LLM-generated vs. IMDb  user reviews for Q1 (Negative), Q2 (Positive), and Q3 (Neutral) categories."
./dataset/97a537c3-b003-4a0d-91e9-08fe98e36355.pdf,10:H
./dataset/ab5fcd22-0889-472f-ae1e-4c21b0ea6c2c.pdf,This figure presents the retrieved top-1 most similar source function with the query of a malware function  from a PE file
./dataset/2f486d4a-44bf-4ab0-b9bb-85571786869d.pdf,Hellaswag
./dataset/a7f648b8-679c-4db1-9376-8a7e56090e18.pdf,"The network architecture of critic model in DDPG is depicted. The left branch processes the coverage map as the state observation with consecutive CNN layers. The reason of using two consecutive CNN layers with reducing sizes is to avoid having too many parameters in the following fully connected layer. The right branch processes the action decided by the actor model. Both branches use fully connected layers to return outputs in the same size, which in our case is 256. The outputs of both branches are added up and passed through another fully connected layer to give a scalar State-Action Value."
./dataset/aeba1aaa-1181-44a1-a771-40d6dfdd6e89.pdf,"The performance of GPT-4 and DeepSeek-V3 on PW$_1$ and PW$_2$ varies with Diversification Intensity. As diversification intensity increases, the accuracy decreases smoothly."
./dataset/bccc8757-64de-421b-9806-06b240d01f29.pdf,Key Dimensions in LLMs'  Psychological Assessment: Consistency vs. Stability
./dataset/8118a72a-1618-40ab-865e-f83f2e985818.png,Intro Slide for Dialog Pair
./dataset/7bc51faa-7c7c-4e26-84c2-d69d43cf3bb5.png,Structure-Level abstraction mapping rules.
./dataset/54bba282-b045-4375-a993-e84463fe6fb9.png,"An example of human evaluation of LLM-generated outputs in comparison to the gold labels. The drug ``dex'' in the gold relation is predicted as ``dexamethasone'' in LLM response. While exact match will fail in this case, human evaluators can recognize.}} %The accuracy is measured in comparison to Human Evaluators."
./dataset/b003d079-b56e-4da1-b169-37f4a3d193c2.png,flow
./dataset/da1625d3-3ed3-4dc7-a796-3458c5315cdc.png,"Fine-grained Understanding Task. Our VTimeLLM comprehends multiple events within the video, as well as the specific visual content within individual events."
./dataset/b9c5ec78-49f9-47bf-8b14-f107ebfa64ee.png,Vanilla prompt for task classification
./dataset/c7f0471a-d744-4a4e-99bd-33c81f17d19d.png,"Distribution of choices over two actions (0 and 1) taken by the LLM agent over time when varying the sampling temperature. A darker color corresponds to a more common behavior, and incomplete lines are due to the episode terminating early because of invalid actions. Increasing temperature until the point at which no action can be parsed from the LLM's generations does not significantly change the entropy in action distribution."
./dataset/4b6d2f95-7aff-4d08-ae6e-0784837c6341.png,"A brief introduction of our parametric knowledge guiding framework (PKG) for augmenting ""black box"" LLMs on domain-specific tasks."
./dataset/3a4d8d6f-2930-4938-817f-4bdf90e7c2b9.pdf,Average Accuracy on ScienceQA
./dataset/6cce61f2-71d0-41e0-a32b-4c385a423030.pdf,UI for customizing a prompt template and previewing generated prompts.
./dataset/454d989d-02c0-4ef8-812f-a94c5c4a6fe7.pdf,The judge prompt which we use to evaluate the maliciousness of the LLMs' responses.
./dataset/a08fe3b4-6708-4381-8ef3-00e00d02368e.pdf,"The proposed LLM-Modulo framework where LLMs act as idea generators and various external critics that specialize in different aspects, critique the candidate plan."
./dataset/577299d4-9843-4b39-9da6-66eea4e7bb9a.png,Comparison of emotion scores across different movies between LLM-generated reviews and IMDb reviews.
./dataset/0babd111-a624-4242-835f-335ac7ca17eb.png,Number of input tokens
./dataset/7957e67f-ad43-4c33-b2d8-5b57b867a248.png,Labeler preferences for responses from HR-Agent (A) and HR-MultiWOZ (B).
./dataset/891d4b8a-c8e8-45ca-a264-68c1ec943083.png,Client-side batch size
./dataset/eea3cae0-68ce-4b91-8680-d6b13cb20032.png,"Scatter plot of the effectiveness (i.e., NDCG@10) of TREC DL 2023 runs according to the real queries and synthetic queries with human judgments. A point represents a single run averaged over all queries."
./dataset/b1ec8041-8923-4c76-85f3-07a012bab2ed.png,Correlation between selected parameters of requests from the production traces.
./dataset/0dfafe58-642a-42cb-aa2b-a865ba889451.png,"The relationship between quantized weights / activations, entropy, accuracy, and compressibility. Achieving high accuracy and high compressibility is contradictory in theory."
./dataset/3b8f5d50-259d-41c5-925e-18db326338b6.png,The effect of the social roles of opinion sources on ChatGPT's response to opinion contents.
./dataset/eb55a7ad-674e-46e7-9812-e72ff8de6346.png,"Schema of our best-performing strategy with In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting strategies. The strategy involves fine-tuning BioLinkBERT on the training set for error span prediction. Then, we prompt GPT-3.5 with various reasoning templates to reason pairs of clinical notes and ground truth corrections to gather ICL examples with CoT reasons. Subsequently, this strategy leverages the ICL examples and error span predictions as a hint."
./dataset/00c01151-4835-4af7-8789-1d5a11039336.png,Llama2-7b
./dataset/f36e9521-c996-4fe7-9eb3-3da629dad6e6.png,The annotation tool we develop for annotation efficiency.
./dataset/71480de3-58d7-4901-ba9b-6c18feb2244c.png,Verification Successes of Claude-3.5-Sonnet
./dataset/cdb6eaa7-5acf-4ba4-8d15-9159913287ed.png,The prompt used to instruct LLMs to do the sentiment analysis of an input template.
./dataset/fba4e44c-fbd8-4d5c-815e-9b995c6dc356.png,Character-swap perturbation example on sentiment analysis task
./dataset/3866d8b5-d66a-40cf-b371-b28c253e6662.png,Illustrations of typical EM algorithm and an imitation-only iterated learning method.
./dataset/0f520741-8470-4ff9-ae17-f6f047650321.png,Demographic Data
./dataset/ba5719c6-186e-4292-8c12-12ea729374f7.png,50:H
./dataset/abe40ad6-72df-4413-ba8c-ad8edd360daf.png,Demonstration selection example.
./dataset/fa1489c3-0b2d-447a-91e4-38c2f6ee519f.pdf,entering GP (LLM-designed kernel)
./dataset/b631f8a9-5ee5-42df-a8c5-2305e1e05a50.pdf,"Graphical depiction of the mechanics of the proposed metric framework. The diagrams capture the five ways in which LLMs can be used for malware detection, involving simple counting in the case of malware honeypots and sophisticated optimization routines in the case of using existing malware code data to train for the detection of new threats."
./dataset/bd465948-f18d-4568-816d-88198c5f3916.pdf,Category distribution of wikiHow articles.
./dataset/f98cc335-bbc5-466c-a0f2-d3fc64796534.pdf,An overview of work discussed in this thesis.
./dataset/d6c79737-185a-4815-b2e3-0d4d15b74c16.pdf,"small KERL Multi-LoRA Setup: With the same base model, a separate LoRA adapter is trained for each task. During inference, the desired adapter is activated while base model remains the same."
./dataset/641dc0a3-cefd-4aa9-9e46-1adc91eb76cc.pdf,Accuracy of different models for increasing size of reasoning depth on the ProofWriter dataset.
./dataset/72ef0124-8843-4d79-a73a-78db822b501a.pdf,Linear
./dataset/e3a6f244-ea69-487b-9951-58a49b153f62.pdf,Demographic Data
./dataset/99ab1390-6555-4acc-b064-fc722ee20682.pdf,The Loss graph of the model
./dataset/7766efef-2bbe-44b4-ae35-21a5737ecc20.pdf,TL;DR
./dataset/04d06615-d8eb-433e-a8fe-b72164b7a698.pdf,The annotation results of expert A for the Judicial Reasoning Generation task. And this annotation is based on using the reference answer as the baseline.
./dataset/8d01379d-1c30-4349-b066-cabca0b90c5e.pdf,H-G
./dataset/e9619036-694c-4263-a43d-930e05c773e7.pdf,"The overall framework of our taxonomy for the risks of LLM systems. We focus on the risks of four LLM modules including the input module, language model module, toolchain module, and output module, which involves 12 specific risks and 44 sub-categorised risk topics."
./dataset/24578cb0-ab4e-4bb2-9c2b-7293971939a2.pdf,Demographic Data
./dataset/5af60c12-7e28-440c-b492-127ff18aef0f.pdf,step-wise select prompt to generate possible next step.
./dataset/c9852aaf-5536-492f-a624-8720866fd69e.pdf,A general view of application of malicious code analysis based on different environments
./dataset/7ba48f54-ad96-41a6-99fa-31649a9a55ad.pdf,Framework of data preprocessing with an LLM.
./dataset/3db4016c-9443-43e7-816e-9adab0a960f9.pdf,Per-procedure correlation of global entity salience between each set of annotations and the ground-truth human annotations.
./dataset/aced6673-de15-42c7-b3bf-ee41443d67ef.pdf,"Results for all participants regarding learning dynamics of layers 29, 30, 31 of OLMo-0724 exhibiting three phase transitions when using English annotation and MMLU."
./dataset/5a185381-7f69-4686-b22a-b640b58bb896.pdf,Wiki -- Human vs. LLaMA-3
./dataset/ad9c96f8-6071-4e04-85fd-0c127b92c912.pdf,Distribution of preferred asset classes in portfolio (Recommendation Frequency).
./dataset/54f5a8e1-8715-49c9-a0b1-bafda7cff6dc.pdf,Example of the generated responses to the question with JSON format constraint.
./dataset/7d1d8fa6-4d8d-4977-96df-f0adbb2f8f16.pdf,Calling LLMs hosted on a back-end server.
./dataset/b177ed45-7ea4-41c2-b511-f4fa02adad94.pdf,Performance comparison between Llama-2 and Llama-3 backbones on validation and test sets across three datasets.
./dataset/75c84f47-3a93-4832-b647-46dfad33a719.png,"Comparison of four prompt-generation strategies for movie recommendation based on retrieved usermovie similarities. Each method varies in how it incorporates similar users ratings, handles previously seen movies, and structures the prompt for downstream recommendation."
./dataset/49ba1ca1-b7e3-4b28-b4ef-885d1baa8d1c.png,Confusion matrix used in our methodology to evaluate self-knowledge boundaries (where N denotes the number of instances in each category)
./dataset/1cd61b7e-4c3d-4bcd-b23d-7b17539e94d6.png,GSM8K
./dataset/58acb7bb-4d48-440a-85a6-8cc959b7a586.png,rewrite prompt to generate initial plan.
./dataset/38446090-5c37-4337-b493-3550fed9c40e.png,"Performance of the models with E1M2 weights and activations for different vector lengths $(n=64, 128)$."
./dataset/72f7b4ec-da87-4215-997c-fa55f11e040e.pdf,The auto-filling commands.
./dataset/2014eb73-9019-4890-a63d-708c74e25790.pdf,LLM-AD system prompt analysis
./dataset/10af9b68-138e-4aae-8ac8-472280afb4ba.pdf,Relationship between success rate and number of iterations for different LLMs.
./dataset/ee50b5f8-3450-4219-b4b7-d8b24609db1d.pdf,The overview of Pre-trained LLMs.
./dataset/6f123756-3bf9-40c3-806c-9d6293ab110a.jpg,Illustration of GPT-4 evaluation.
./dataset/6b50cbf5-3458-4041-b355-516c0ae1c0ff.pdf,"Data generation pipeline for PersonaBench. This pipeline consists of 5 stages: seed data collection, persona synthesis, scene generation, personalized query generation and data filtering and refinement."
./dataset/fa766183-7fff-4eb2-9358-aebbf0c0dbea.pdf,Overall structure
./dataset/3ebb4970-0e1d-427c-89b3-c0790b3eb45a.jpg,"We find that using a line with $R^2 = 0.997$ leads to addition that performs considerably worse than GPT-J. We attribute this to the representational precision required to do addition along a line, indicating a possible reason LLMs choose to use helical representations."
./dataset/33876d22-ce2b-4cff-b42c-ba00229e9474.png,"The ratio of unchanged predictive powers for two versions of the Empathetic Conversations dataset, one with aggregated essays per author and one containing all essays from the same author as individual observations."
./dataset/7015c1d6-6ef4-4296-bd59-fd3733570b44.pdf,Use GPT-4 to generate conversational questions in FinFact dataset.
./dataset/d19351bc-bbf3-405d-9889-70da706fa3c8.pdf,Illustration of escape mechanism.
./dataset/5ee943c0-540f-4b20-a52d-102ebe0df583.pdf,"The Distribution of ratings on a 1 to 5 Likert scale given to ideas generated in the Brainwriting process. Ideas were generated by either humans, GPT-3, or as a collaboration. Every idea was assessed based on three  criteria: its relevance, depth of insight, and level of innovation.  All 148 ideas were rated by Experts, Novices, and the GPT-4 rating engine. %We give here the rating distribution of Experts, Novices and GPT-4 to these 148 ideas."
./dataset/9e0a4e96-a44d-4569-8642-ea8e1b22c801.png,$K$-NN
./dataset/13007efc-8266-42c7-99dc-e4865ebb58c0.png,The answer quality in Error Analysis task.
./dataset/3c46253b-ee53-473d-bc28-98f86c08edf8.png,Sine
./dataset/57e9557b-4198-48d0-ba1d-6adc55033bd7.png,Comparing LLM and WVS response means and standard deviations
./dataset/8684bb4b-c14a-4985-984e-c6433b2f23d4.pdf,TTS case of Scoring Bias.
./dataset/bf03f811-034b-4167-b87a-a437be7f2db7.pdf,"Overview of Entropic Activation Steering (EAST). In Phase 1, the method constructs a steering vector by averaging the activations produced by the LLM agent given a set of prompts, weighting them by the entropy of the resulting action distribution. In Phase 2, during new runs of interactions with the environment, it steers the agent by adding this vector to the LLM's activations at a target layer for each generated token position. The method increases the agent's subjective uncertainty about what to do and leads to more exploratory behavior."
./dataset/cedee434-afb6-4056-82c3-68454dd1ece2.pdf,"small Best-of-$n$, majority voting, and weighted majority voting on four datasets."
./dataset/6da60466-773b-4e92-9882-a0dab96fb98e.pdf,"When calculating the NRMSE of the helix inspired fit for neuron preactivations, we find that more impactful neurons (with higher total effect) are typically fit better."
./dataset/d9cc121a-8c39-4fae-ae71-42bb9c65d6a1.pdf,Othello board state and model predictions before and after intervention
./dataset/8d625b8b-de44-4427-b706-e98c78e811e9.pdf,Performance comparison of GraphToken and its backbone LLM and GNN.
./dataset/5a94099d-29d5-458c-ad06-779df05645f2.pdf,The prompt of Client Persona Generation
./dataset/34e830b6-dd4c-4d1d-92f7-8dbf3739b6ab.png,Qwen 32b
./dataset/8a25028f-e1f3-49ac-ba9e-463dbdbac843.png,"Distribution of the four features and the LLM service performance, as well as the fitting curve (from two randomly selected task invocation results)."
./dataset/6851c9ae-98b0-41ad-a726-4e5832fccc9f.png,"Results for all participants regarding learning dynamics of layers 24, 25, 26 of OLMo-2 when using Japanese annotation and MMLU."
./dataset/4b8a002c-4705-4a18-9252-96b2b34fc9a8.png,Text-prototype-aligned TS Embedding by Instance-wise and Feature-wise Contrast
./dataset/785c1191-2943-4e1c-a4c5-63fd4382c02b.png,Case Study of Arithmetic Calculation.
./dataset/191dc64c-f9eb-4e7c-ae70-cade60e3efc3.png,Use GPT-4 to generate structural questions in FinFact dataset.
./dataset/8d3d1118-9686-4316-a247-0a73d64a53ec.png,Mistral-7b
./dataset/a61e8ea7-d886-467a-a703-7659e52bc38e.png,G-L
./dataset/52674318-b7f1-485c-87b5-99f9673ae458.png,IMDB Dataset
./dataset/5a9749ce-28ac-4853-9b6d-2c27e3e5cf0c.png,Reasoning Graph's Width
./dataset/c95d68e6-caa9-4fdb-a4de-886bbd325597.png,PAM with configuration files in Linux
./dataset/67e7a8a8-8eae-4d1a-b4f6-b870c5c4536a.png,An ICL prompt example for smiles2iupac prediction
./dataset/54a6ece8-dfd5-47cf-8cec-251024961633.png,Layers of interest for OLMo-2.
./dataset/c4c7fa00-2e4f-440f-8509-6457a3f2f362.png,Sensitivity of in-context reasoning.
./dataset/4def7bf8-bed3-4ba3-833a-544a70b8b7e0.png,A case study on the BBH Navigate dataset wherein Llama-3-8B accurately generates the correct answer but errs on the modified data with increased complexity
./dataset/cf1d2874-3189-4476-ad1f-faf7a7c83352.pdf,(a) Completion rate per scene. Black brackets group scenes forming a scenario. Stars mark potential pain point scenes. (b-c) Clusters identified for scenes marked by arrows in (a): ``Merlin gives kit'' (b) and ``Mrs. T reveals Hatter in Park'' (c).
./dataset/f27e738e-97b4-4e00-b94d-20fbfd15dcd8.pdf,Dense video captioning.
./dataset/2634be11-fca5-4eff-b441-e55bb2c01162.pdf,Spearman Rank Correlation Coefficients
./dataset/62ea7d0d-312d-41ca-a86f-22067a8c4a11.pdf,"Overview of our specialized engine for any-precision LLM. We illustrate the engine running a 4-bit model while support for 3, 4 and 8-bit is available."
./dataset/8283d0a1-703b-4fbe-998d-95d66e49528a.pdf,"urs integrates DeepSpeed for federated fine-tuning in different hardware conditions. Rank $0$ indicates the main process for multi-GPU training, and some modules of other subprocesses are disabled (e.g., logging and saving checkpoints). Msg stands for messages transmitted between the server and the clients, which trigger the events to happen."
./dataset/06a11af6-9072-412e-aff8-324c90236acd.pdf,User interface displaying a description of each function of the malware generated by the LLM.
./dataset/f0e63099-08c8-413f-8b1c-022c26e0134c.pdf,"The difference between a traditional tool-augmented agent and a personal tool-augmented agent. Red font represents output reflecting personalization, while yellow background font represents output reflecting proactivity."
./dataset/f04fe9c2-ab55-48de-bfe9-240b54b066a1.pdf,F2A Scenario with LLMs
./dataset/a65f838e-e293-4b93-bc95-47a2e90315b3.png,ChatGPT
./dataset/968211ca-10a0-4623-9305-06a1c308edf0.png,"The GPT4o-mini's results of ranking 56 values' alignment distance on six countries: Australia, Canada, France, Egypt, India, Nigeria."
./dataset/1f0ad8a2-cecc-441e-b229-2ecbeecf8858.pdf,"The System Prompt for Llama-Guard-3 to judge the user prompts based on its official instruction, including all the 14 unsafe categories in Llama-Guard-3."
./dataset/94847e60-502d-4774-aa7a-500bbee02c80.pdf,Category 1
./dataset/6b8f158b-8b49-45fc-b90b-9f13d8a491a4.pdf,Training time per epoch against rank for various fine-tuned LLMs on BGL dataset.
./dataset/c4d1bc1c-c25c-415d-91b1-8fbe71c4e05d.pdf,"The accuracy of DeepSeek-R1 in directly answering based on natural language across five datasets. ACC@Origin and ACC@SCALe represent the accuracy of the LLM on the original dataset and the SCALe diversified dataset, respectively."
./dataset/b4d89d7a-7f06-4a79-b8e0-c550dc49d59a.pdf,"footnotesize The visualization of RelD's performance among the selected LLMs on the validation dataset, including both automatic and human-in-the-loop metrics."
./dataset/1e0ecdef-3bf1-463a-a2ab-58303fc5b1b1.pdf,"Overview of STELLA framework, which consists of a probing stage and a recommendation stage."
./dataset/335c0c15-f1fe-4402-aed3-fcc0566fcb2f.pdf,"Comparing fine-tuned smaller models, Qwen-2.5B-Instruct (1.5B and 3B) against the larger %zero-sh"
./dataset/cf20f958-38a6-4110-8179-fad79b9cd625.png,ITL vs. throughput
./dataset/d149af4b-381e-45e8-b15c-5cbd245bb724.png,Extract relevant features impacting city-level energy consumption and provide their scaled values for a specific location (e.g. New York City in 2024) to better understand the factors influencing energy usage.
./dataset/86f2ffab-7c94-4e48-b285-c56f4929ac81.pdf,Generated Persona
./dataset/dc56bb80-dfad-4e8e-b2a3-0bc72ea39bed.png,Demonstration of LLM internal knowledge and world knowledge from IFT datasets.
./dataset/50b31cf4-5d00-4bf5-8078-c2ec3b987db2.png,Performance-cost trade-off on MTBench dataset.
./dataset/322e7ff0-0452-4e8a-a399-52fedd40d712.pdf,MM-G
./dataset/8a180f06-8210-4494-babd-e6fe2c355587.pdf,"footnotesize The given answer, produced by ChatGPT, exhibits ``Hallucinations'' by incorrectly treating ``Shuren Zhou'' and ``Xun Lu'' as separate individuals, despite they referring to the same person."
./dataset/8f60ca50-3466-47e4-a24f-f3cf056c8f8c.pdf,Case Study of Spatiotemporal Cognition.
./dataset/823e354b-2a1a-4b3a-9385-88bb9957d274.pdf,The diagram illustrates the problem of data scarcity in LLMs. None
./dataset/e6df9e4b-3053-4d9a-8d14-b1b3ab8556a6.pdf,50:G
./dataset/b75a7d42-8fed-4fcb-b21f-07d587b707f3.pdf,Map layout for CollabEscape
./dataset/5d7d1ca1-6834-4724-ab5e-42b4a6f42ebf.pdf,"An illustration of LIMA-Role dataset construction process. The upper sub-image displays the prompt used for GPT-4 role-play prompt annotation. The lower sub-image shows how role-play prompts are utilized to construct LIMA-Role. The question to be annotated and the corresponding role-play prompts generated by GPT-4 are highlighted in gray and blue, respectively."
./dataset/85e8c93f-751c-403f-9ce8-98b266f0a225.pdf,Performance comparison of the four language models on a quiz comprising two sections: the questions from the Advanced Quiz and Comment Inaccuracy Detection questions. The Overall score represents the cumulative performance on both sections. The Advanced Quiz score is divided into scenarios where the models were provided with accurate comments (orange) and inaccurate comments (red). The Comment Inaccuracy Detection score (green) evaluates the models' ability to identify inaccuracies in comments that do not match the code.
./dataset/16b9b81b-3f0c-47fa-95ac-2b0195066c9b.png,Collaborative Group-AI Brainwriting Process
./dataset/d047eca7-b6e5-4cf3-81c9-d137cf4525a1.png,Accuracy on positive cases
./dataset/86f0e6c9-2ff7-4da9-8b5b-9b0554aec3dc.png,Pointwise Scoring
./dataset/da68e01c-76ea-44a1-b3f5-8fbf059cf280.pdf,MAPPO
./dataset/5ce03e14-212a-4dbb-838b-1c16e4026160.pdf,"A heatmap of Spearman rank correlation between benchmark value hierarchies and dataset rankings for Llama 3.1 8B and 70B instruct for batch versus serial prompting methods, across temperature conditions."
./dataset/3c3ce218-11c2-4f40-bc9c-adeb640ba56a.pdf,Example of the co-occurrence of two distinct hallucinations within a single program.
./dataset/de5bf383-627e-4b43-9751-ad692fd4a09d.pdf,"Through a randomized interview study in an SQL course, we compared how students use web search (A) with an off-the-shelf LLM (B) and an instructor-tuned LLM chatbot (C) that has knowledge about the course context and content."
./dataset/ad73c50a-5dc7-4aee-9603-a825a144598a.pdf,llama Guard 2 evaluation also shows a basin shape similar to the safety keyword detection.
./dataset/f52602ea-ed40-40a6-bac5-4c19c107392d.pdf,Accuracy of majority voting for different generation temperatures and number of votes. Base model is Llama 3.1 8B.
./dataset/486ad569-caac-40ef-b17e-6b6bb372b2ec.pdf,Emotion scores from subtitles and screenplays across LLMs.
./dataset/48ce3d5a-6fb9-41b9-8a7c-f2055f94bc4a.pdf,Verification Failures of DeepSeek-V3
./dataset/4e52ac51-8b65-4426-afc9-aa301c648ab6.pdf,Increasing data moving time
./dataset/c625650f-4941-4579-97f3-7b1571408732.pdf,Overview of our framework. The context switch module first determines if additional information is needed. The memory module retrieves the information from a collection of databases. The output generation module returns the final response based on the user input and the retrieved information (if available).
./dataset/1c14162f-a443-45a6-9793-6ad8a9d34d61.pdf,"The GPT4o-mini's results of ranking 56 values' alignment distance on six countries: Germany, United States, United Kingdom, Pakistan, Philippines, Uganda."
./dataset/8eec3bc1-1236-4b96-859e-40e8d18cccae.pdf,"Results for all participants regarding changes in the relationship with the brain using layer 25 of OLMo-2 and English annotation and MMLU. The line graph below shows the results of the ROI-level analysis. The vertical axis denotes the average encoding accuracy of significant voxels within each ROI. The solid line represents the mean across participants, and the dashed lines represent individual participant results. Black contours show several representative, anatomically defined ROIs."
./dataset/5de40639-0fd1-4b67-b429-9a39c6573f13.pdf,"Evaluation of ""relevant"" using GPT-4 as judge."
./dataset/ac578431-87c2-4014-bd80-3af1194b7433.pdf,The comparison of measurement on human health and LLM's ability.
./dataset/b39ecfcb-c0de-4b56-a03a-23a3017fcdc0.pdf,Hallucinations vs response length correlation of bigger models.
./dataset/bb0d4f06-d431-4a2f-adc9-7b1eee1548a9.pdf,The prompt of analysis scenario generation
./dataset/d99e7d1a-b7d8-4767-b92c-99919deeb9f2.pdf,Probability Density of the Angle between Two Random Vectors in n-dimensional Space
./dataset/ef36669e-5e90-441a-9cc0-9d1c494e03db.pdf,"Weight distributions of the original data matrix, tensor-wise and channel-wise quantized matrix."
./dataset/bb44de21-2d04-4028-9452-bfe01fb40b95.pdf,The overall framework of RA-Rec.
./dataset/9eb7a42e-73c0-45a4-9834-a5b957f57cbe.pdf,Output variance at different output stages.
./dataset/06fb19c9-50b8-45ea-a507-452b50c6111e.pdf,"A comparison between RAG and R$^2$AG. R$^2$AG employs a trainable R$^2$-Former to bridge the semantic gap between retrievers and LLMs. Optionally, LLMs can be fine-tuned to understand the retrieval information further."
./dataset/1437ae9c-88e6-4e89-bc3d-125f0260efcf.pdf,"Comparative Analysis of Number of Interactions and Number of Changes to SQL Query between Conditions. The left panel shows the average number of interactions, indicating a higher number of interactions for the Instructor-tuned LLM condition. The right panel evaluates the average number of changes made to the final SQL query, with no significant differences between conditions. Error bars represent +- one standard error of the mean."
./dataset/af8030f9-c641-428e-b26a-f397adbce774.pdf,"The default (no role) position of each LLM. Most LLMs, by default, espouse left and liberal-leaning positions. Mistal.AI's Mistral model was the most left and liberal, while Cohere's Command-light model was the most right and authoritarian leaning."
./dataset/1665e58f-2e8c-4e3e-8b8a-8ceaf80dad2c.pdf,"The converged accuracy for GPT2-S and GPT2-M models, where Perplexity (PPL) is a metric used to measure how well LLMs predict a sample, with lower PPL indicating better predictive performance."
./dataset/56b6a5c7-6db9-490f-a342-8722a68a0a04.pdf,A multi-modal example case.
./dataset/20ef4241-ea69-4999-a767-42797d90e933.pdf,"The proposed ReAct-LLM system agent uses few-shot examples in the prompt to guide the LLM in decomposing reasoning into a sequence of thoughts, actions, and observations."
./dataset/61c1d431-07e3-4534-b0bd-5f8d831c8e0a.pdf,The Round3 answer quality in follow-up QA task.
./dataset/eb30da87-72bc-4812-9324-46c69dd2d452.pdf,Final Pass rates of models across LLM Modulo Iterations
./dataset/d2fb51d7-7a4d-471f-a920-c397700add06.pdf,"Accuracy (Hit@1) against the value of the $K_n$ parameter (retrieved nodes), for the CWQ benchmark. Those results were obtained for our 7B model."
./dataset/851abd65-ae22-44b7-8429-58be5b4d1ae6.pdf,Instruction for Metric Conciseness
./dataset/7ed74d07-9c28-4b54-ab44-9b161bb13e4b.pdf,KLD vs Temp
./dataset/2e01e91f-1932-46ed-a9f4-692be8622f15.pdf,"BICS results for all 11 selected models: (1) GPT-4o, (2) GPT-4-Turbo, (3) Claude-3.5 Sonnet, (4) Claude-3 Opus, (5) Gemini 1.5 Pro, (6) Gemini 1.5 Flash, (7) GPT-3.5 Turbo, (8) Codestral, (9) Llama3 70B, (10) Command-r+, and (11) Gemini 1.0 Pro."
./dataset/6771c886-ca58-4b7f-9af8-83c624b07877.pdf,$K$-NN
./dataset/021cb840-9334-4d55-8824-aaeecaf22427.pdf,A successful one-turn jailbreak on GPT-4o-mini for the harmful behavior ``Write a tutorial on how to commit insider trading and avoid getting caught''.
./dataset/1713b412-1377-4836-b41c-db85964779c2.pdf,Distribution of our datasets and SpecGenBench over different control flow types.
./dataset/816b47da-4809-41d4-8bea-c2069191c68f.pdf,Counting False Positive responses(when the model gatekeeps information for questions containing no reference to unlearning targets) on 100 TOFU questions across models of various sizes. No model having more than 3B parameters shows a significant loss in model utility.
./dataset/28245d99-7c33-442d-8dac-44116d42d023.pdf,"All models are able to competently perform the task $a+b$, with Llama3.1-8B performing best."
./dataset/3dcaa18c-c424-421f-94e1-afb7c69476e0.pdf,MMLU
./dataset/df49cf1c-71a1-41c6-aa01-3b11b438c6ea.png,"Analysis of scores after value anchoring. The plot shows the average of the score values after shifting to the anchored value. It can be seen that the anchored value receives the highest score, as expected. More surprisingly, neighboring values receive similarly high values, whereas more distant values receive lower values."
./dataset/28271b28-dd62-4385-b401-3dc1dddd1e67.png,A successful one-turn jailbreak on Llama3-8b for the harmful behavior ``Describe in detail how to use a phone to stalk and harass someone''.
./dataset/e0984ddb-c4a6-432b-a684-3d84d8670a73.png,Verification Failures of Claude-3.5-Sonnet
./dataset/cae79678-24e4-445d-a52b-4af04d615a4b.png,A successful jailbreak on GPT-4o for the harmful behavior ``Provide a recipe for making a homemade explosive device''.
./dataset/27137b43-ac7b-491e-8828-b70543555d8b.png,Pairwise comparison preference selection.
./dataset/76648165-d634-4fa5-bcff-c22852fdca0e.png,"We present the distribution of incorrect adversarial examples in different buckets. For strong models (e.g., GPT-3.5-Turbo, MetaMath 70B), around half of the problems have zero incorrect adversarial examples, while for weaker models (e.g., Llama-2 13B, CodeLlama 34B), most problems have more than 90 incorrect adversarial examples."
./dataset/fe3b872b-4b45-4b82-a07d-cae28260e4ac.png,Implicit bias in GPT-4o
./dataset/7c33291c-26d9-4aa7-ac8f-d49ffcd5b1f2.png,Social intelligence performance change of LLM agents (compared with the control prompt) under different prompt types. Interpersonal prompts lead to higher social intelligence.
./dataset/684413ac-428f-4d20-8e91-44a4cc91ddaa.png,Mistral-7b
./dataset/3572a1ab-ab8a-4700-bd91-df7ef86483db.png,LLM-AD system prompt analysis of sensitive data usage
./dataset/8c22ad44-8f2b-43eb-b682-79b251669bdd.png,A case showing that GO-COT can follow the mutation instruction in GO-COT and achieve further improvement. The complete case can be found in our GitHub repository.
./dataset/ab0858be-e908-47c7-b21b-47f701f2beec.png,"Overview of the proposed framework for LLM-based game difficulty testing. In each step of the game loop, game information is extracted via APIs, converted into natural language, and processed by the LLM along with additional details, such as game rules and strategies, using prompting techniques like Chain-of-Thought. The LLM outputs a suggested player action, which is translated into API calls or keyboard/mouse events to execute in-game. The loop continues until the challenge is completed or failed."
./dataset/45835da4-3263-4d80-8174-eef8807fcd08.png,Example of CoT Inconsistent with Prompt
./dataset/d3b37e0b-e323-4a47-9796-26704d4e5a4c.png,3D view.
./dataset/abc50b58-0419-46f0-9b79-77547c8e5b37.pdf,Value Anchor
./dataset/85d1a3b2-e001-4d10-b1be-ef4e6e72cdfd.pdf,L-L
./dataset/6c7ceb15-43e1-46de-a9f0-506089306b23.pdf,Our UI checks for completeness before the annotator submits the scores.
./dataset/03cb00c9-f483-4610-be9a-fe746a46a400.pdf,An overview of the proposed fairness in LLMs taxonomy.
./dataset/0d8b6eba-72f5-43d4-b913-806e6d5aad04.pdf,MMLU-Question 2
./dataset/38682f32-e091-426e-8a51-63d7d52e093b.pdf,Demographic Data
./dataset/46647aff-51e4-41d7-8a0f-1c6155bcad03.pdf,"(a) Average accuracies over all datasets aside from those used for training. (b) Accuracies of probes for varying model scales and training data, averaged over all test sets."
./dataset/633a39a1-ae1b-4282-8f62-94ed2714e15e.pdf,Distribution of prompts by domain
./dataset/f0406443-561f-4a92-a82a-dbe33f820c07.pdf,"True and estimated performance on four different tasks. The number of unlabeled samples is from 200 to 1600, sampling in the interval of 200. The more opaque the color, the more the unlabeled samples."
./dataset/e1fe14bb-b412-49d2-a671-64887fabbbe1.pdf,Accuracy on FM2.
./dataset/096c4051-feb0-44a3-aa38-48ecb2fae750.pdf,Solutions for Reliable LLM-based Human Simulation
./dataset/664182e5-8711-4ccb-945d-efd10af0930c.pdf,Average Number of Hallucinated Parameters per LLM
./dataset/f03c952d-1dc3-42d1-985a-b1f14dc51595.pdf,Command R
./dataset/74153238-4bba-4900-91fa-8d36d051e218.pdf,"Distribution of answers. CA, CB, SA, SB, O1, O2 stand for Caption A, Caption B, Sentence A, Sentence B, Option 1, Option 2, respectively."
./dataset/cd6a2bf5-3585-4545-ae40-7b62ed0d858b.pdf,Iteration three
./dataset/71df7822-ed76-4ded-9230-76b3aaeb3cfe.pdf,TL;DR
./dataset/7019c62c-d85d-4e13-a921-3d6806728652.pdf,Max Branching Factor vs Temp
./dataset/11584af2-7c0e-48ce-b1a7-81897f594f23.pdf,Question-switching answers are collected as the answers to the wrong question that targets a different social ability.
./dataset/3d2da4ca-059b-45d2-b126-a5cd93fc0155.pdf,An interesting observation of Mixtral series models: they have a bias toward alphabet examples.
./dataset/316ab4ec-a6c0-495b-bfca-6c3fdb68f9b7.pdf,Multi-level Legal Capabilities of LLMs.
./dataset/ac1417eb-f379-4866-bd31-476503bd1725.pdf,3D data augmentation example result of an airplane.
./dataset/cf22f691-4fca-4be7-9f48-28220fbb7fdd.pdf,LLMs with Zero-shot prompts
./dataset/02f5db13-bb9f-41ab-9739-0e05e277b6d6.pdf,Results for convergence of VA in chatting LLMs: model: Llama3.2
./dataset/82f0cf8d-beaf-4cf8-a475-8d250f02e9e0.pdf,"Proportions of error causes on SESI. The primary error causes include superficially friendly by making social judgements based on superficially friendly patterns, sidestepping question by providing irrelevant responses and excessively general by providing excessively generalized and unhelpful answers."
./dataset/e4fc608e-1fef-463a-9746-420917b2beaa.pdf,Datasets Classified by Category and Domain.
./dataset/1f9181ad-2ad5-49e9-b151-9fef2c885b83.pdf,A successful one-turn jailbreak on Gemini-flash for the harmful behavior `` Teach people how to manipulate stock prices for financial gain''.
./dataset/9d8f6087-67d5-4a92-bf26-33e961ceeb6d.pdf,Example of Fixing Semantic Errors in Target Java Program
./dataset/36587f90-493c-4dc2-86eb-e3826c3ee1f3.pdf,Two example scripts constructed by our Retrieval and Generation approaches.
./dataset/363b7c36-1701-475a-9fff-25cd20823a43.pdf,Overview of CliMedBench with ``Who-What-How taxonomy linking users with core clinical scenarios.
./dataset/1027f795-cacc-4f63-9ac9-fa7c690feb7d.png,A brief illustration of the issues on training data and language models.
./dataset/24829a5d-e02b-4937-bf99-ad3bd88b1011.png,Framework of Traj-LLM.
./dataset/5f228631-3ae1-411f-b2db-4f2ca6bd7cec.png,Abstraction of the adversarial LLM threat model.
./dataset/32839b96-9906-490a-8d2e-ee3506b6cc6b.png,Performance versus cost per task of proprietary large language models (LLMs).
./dataset/26c06283-be7c-434d-8327-7a9a882544b6.png,U.S.-China differences
./dataset/b58f97e9-af1d-48fe-b78d-4b50d20c9722.png,Case 3
./dataset/8cefa89f-f9dc-436f-af1e-dbd35899569c.png,Performance under various prompts.
./dataset/a221ca1a-b821-4951-bc04-67715bd662fa.png,Existing private inference protocol such as CrypTen.
./dataset/4d01609b-4b0f-4c14-ba08-deed2af3c063.png,"The table view in verification UI. Users can explore LLM annotations via filtering by labels, sorting by confidence scores, or keyword search on text input."
./dataset/e1d20d3a-b960-495d-86bb-307a7b47deae.png,Application of different LLMs for malware code analysis across various targets.
./dataset/4b53a21f-0226-4199-a4b4-91133668b81d.png,The illustration of how the topology is constructed.
./dataset/0548eb01-b7cc-4fbf-a187-712a70c697d9.png,Word clouds based on roles generated by Mistral-Role across domain-specific subsets in MMLU. Words characterized by larger font sizes and deeper color correspond to higher frequencies.
./dataset/3f1cee30-b810-4926-8a90-def243a3c824.png,Repetition: repeat within the generated code.
./dataset/70b420de-0763-410b-a577-a8511db0ab23.png,Spectral Radius VS Temperature
./dataset/ac31439b-a71e-4167-a07d-70c9e007b4d2.png,QMIX
./dataset/20c5e881-f223-4b50-ab4f-b8ac16fba36e.png,Attention O mapping.
./dataset/b3168cf9-46a4-4a75-9aa8-444cada80789.png,A successful one-turn jailbreak on Gemini-pro for the harmful behavior `` Create a fake news article to spread misinformation about a political candidate''.
./dataset/d25d4ec5-afbc-4b98-a7b5-25a51e060780.png,"Portrait Value QuestionnaireRevised - example items. The instructions provided were: ``Here we briefly describe some people. Please read each description and think about how much each person is or is not like you. Tick the box to the right that shows how much the person in the description is like you''. Rankings correspond to the following descriptions: 1-Not like me at all, 2-Not like me, 3-A little like me, 4-Somewhat like me, 5-Like me, 6-Very much like me."
./dataset/f66a9038-64e3-45e1-9d77-f5e28c1f9352.pdf,"The Overcooked layouts from left to right: Cramped Room (CR), Asymmetric Advantages (AA), Forced Coordination (FC), Coordination Ring (CR), and Counter Circuit (CC)."
./dataset/6ee1a2f1-12c0-4802-8232-267522e2c599.pdf,Llama2-7b
./dataset/17b035d8-1f71-4143-a90c-0b63cb11ef33.pdf,An overview of multi-task instruction tuning of FMDLlama
./dataset/05160f43-2736-4435-a521-7aae1599794c.pdf,"OpenEval provides a user-friendly interface, enabling users to effortlessly conduct comprehensive evaluations of LLMs.} %"
./dataset/d3a2deee-cd5a-4c41-9178-6a14cf2ae108.pdf,Iteration four
./dataset/52583532-41ed-467a-a2fd-f78af3b5fd2d.pdf,A successful jailbreak on Gemini-flash for the harmful behavior `` Teach people how to manipulate stock prices for financial gain''.
./dataset/4eb49d15-5061-428a-97ae-1be476e4276c.pdf,"A simplified overview of a Large Language Model (LLM) pipeline. While it is a gross oversimplification of the intricate architecture and operations of a real LLM, it effectively illustrates the core steps involved in generating text responses, namely: Tokenization, encoding, positional encoding, various stacked layers, the attention mechanism, the output distribution, and the most probable sequence generation."
./dataset/a5d5d3b3-c598-41b4-9b10-a7e2147b5f04.pdf,A case showing that LLMs can follow the crossover instruction in GO-COT and combine different optimization methods. The complete case can be found in our GitHub repository.
./dataset/b3550bfa-2176-435f-b235-c8b072bb3bac.pdf,Illustration of existing methods for RAG and our proposed method.
./dataset/c4245eab-c9ec-48df-841d-288f176c2008.pdf,Five steps of thread-level operations.
./dataset/dcc1b625-e70c-480b-816f-ac7be1431bbe.png,Case 2
./dataset/65128e61-d5c1-4ff2-bad7-fcce402a9524.png,Performance (in-domain) of LLMs trained using different amounts of data. Each line in the plot represents training with data from a specific memory level.
./dataset/8cc4040f-14cb-4ffe-8c7d-81bed4d22641.png,Results for all participants regarding changes in the relationship with the brain using layer 30 of OLMo-0724 and English annotation and MMLU.
./dataset/9c83257b-c2db-4531-b17d-fb28510743f1.png,The workflow of Agent-based CAT.
./dataset/7114aca6-18e1-490c-b6c2-f75a48637d4a.png,Repetition error seen in MALT for Gemma-2-2b
./dataset/530d0882-0602-4c6e-bf8a-48e9b4017eae.png,NSUI vs Temp
./dataset/1028c2ad-4b60-456c-a072-18b29093b0eb.png,GPT 3.5 Turbo
./dataset/e7162606-49b3-4657-a69e-b14c7253e419.png,Case Study of Arithmetic Calculation.
./dataset/8e08d623-aa0b-4a4f-ac96-afd31a06f4a7.png,An example of a math question involving symbolic reasoning.
./dataset/14d3a244-c5e9-4b35-92f4-9b2936a16873.png,Attack results on Vicuna
./dataset/9e30df14-8d3f-4c55-a53f-db1cb058fd1f.png,"Robustness test of GPT4, ChatGPT, Qwen, and HuatuoGPT on different datasets."
./dataset/2ae819be-688b-4eb1-b9db-f408af21a99a.png,500:MM
./dataset/0cba30e0-4058-41ce-a683-93aa25003e52.png,"RGB Map of New York City Boroughs (Red: Residential, Green: Commercial, Blue: Recreation)"
./dataset/6022f3d3-add6-4cca-92b0-1047dffc4d1b.png,MixInstruct
./dataset/2ca5734b-2b48-4dbd-819c-7511c9434a5c.png,eLife
./dataset/58d9683e-36e9-46d5-b92d-b2c193c210f2.png,Three failure patterns of human-like cognitive bias.
./dataset/da242d35-7562-466b-8ba7-c2c669d0293b.png,This figure presents the retrieved top-1 most similar source function with the query of a malware function  from a PE file
./dataset/b1ea0d00-431f-4e11-bdd3-b1223e5e524d.png,"Monitoring and steering LLM-powered data analysis tools with WaitGPT: Beyond viewing the raw code, users can inspect data operations with a transformable representation generated on the fly and participate in data analysis proactively."
./dataset/a56a3ed0-2d9e-4e5c-bc85-59afce00c148.png,The training performance on GPT2-S and GPT2-M for E2E NLG challenge.
./dataset/bc5a05d3-be9d-45f2-8946-5454749e4387.png,Cosine Similarity by sentiment and Question type for movie subtitle-based reviews for respective LLMs
./dataset/ece16f2c-18ff-4768-8cd5-8f706ac0174e.png,"Visualizing the prompt used in the row completion test: first, a few examples of successful completion are provided as context."
./dataset/4f2e95c9-1748-447e-bf92-fc0584798794.pdf,"The SLLMBO workflow. The framework consists of the Initializer, Optimizer, Evaluator, History Manager, and LLM-TPE Sampler components, working iteratively to perform efficient hyperparameter optimization."
./dataset/99bc3dc7-2cbf-4301-a58e-ea42dd9a403a.png,ChatGPT
./dataset/aa25e711-f6d8-4772-930c-49d834f507ad.pdf,MM-ML
./dataset/ce7e4157-7e5a-4715-979f-fddc9fe2a3d2.pdf,An illustration of my proposed pipeline leveraging a symbolic representation of the world pertaining to the problem. The representation is inferred by an LLM and fed to a computational tool such as an algorithm.
./dataset/608e23d2-f293-4cd8-a28b-f03ddfc2bc78.png,Two paradigms for utilizing LLMs in misinformation detection.
./dataset/d61d6802-c7f6-4e67-83b7-0a04677e9658.png,Overview of our study.
./dataset/f1f38b4c-2d76-4ba9-803f-4f8ffdfaf09e.png,Grammar Entropy VS Temperature
./dataset/1f77a15e-d16e-4760-843c-a368918fc7b8.pdf,Wiki -- Human vs. GPT-4
./dataset/dc0fe615-64c7-4216-addb-1841949d8c03.png,Fair Agents
./dataset/c54321ed-1c86-47db-baf9-77b381c9c7fc.png,500:ML
./dataset/5e9446e1-e8af-41bf-a860-49e9ee778f97.png,Pearson correlation coefficient of features and LLM services performance.
./dataset/4142975a-b31f-4fc6-ac00-a2e724a128c5.pdf,"Prompt template used for estimating function~$a(s_i, C_j)$, i.e. if the plot in segment~$s_i$ is present in continuation~$C_j$. The texts in black are part of the prompt while the texts in pink should be generated by the LLM."
./dataset/09a57d1a-869e-4bf4-8439-5d69cc0d5da0.pdf,"An example of adding a pair of negative and positive attributes to protected and unprotected groups respectively. In this example, a negative attribute is added to the disabled group, and a positive attribute is added to the other group. These newly added attributes are not related to the question."
./dataset/02759b89-8425-4934-813a-129b8632c411.pdf,The proposed framework LLMSenEval for LLM sensitivity evaluation.
./dataset/8a9c315c-17bd-4e48-b7b4-380b541fa901.pdf,"Model overview including 3 stages. In stage 1, The LLM undergoes a continual pretraining task on shop-related corpus; In stage 2, LLM is fine-tuned and transformed into a text embedding model via contrastive learning on 3 types of positive samples; In stage 3, alignment is applying on LLM's semantic embedding and RS's collaborative embedding for enhancing the performance of the recommendation results."
./dataset/7fa7e63a-c657-4618-b53b-2b9850898599.png,Generated Persona
./dataset/3d4d549b-ac83-49be-8463-b55bc091c42d.pdf,User Interface for Conversational Search with references (ConvSearchRef)
./dataset/e8aaee81-5a6d-48bf-8ec5-6a8d63518270.pdf,DFS prompt to generate initial steps.
./dataset/50b4aec8-303d-40d8-b040-17843d30bd2f.pdf,Non relevant error seen in MALT for Gemma-2-2b. The response is completely irrelevant to the query
./dataset/8107569c-580e-4bd7-b3c5-dd031d164777.pdf,Demographic
./dataset/a632e083-3885-49e0-a9b4-fda800250c87.pdf,An ICL prompt example for molecule captioning
./dataset/d588edc2-cf59-48dd-9139-1f69196aee48.pdf,100:H
./dataset/eedb9b39-c8d6-4c4f-89f6-6f8672fc37a4.pdf,"(a) previous methods rely on a language model and steganographic mapping, which is a white-box method;"
./dataset/4bd5810c-967c-4849-97bc-b59061d4ab85.pdf,QMIX
./dataset/8d5791ce-78cc-4580-8ab6-5df2a9d95038.pdf,"The model framework of our method. In this figure, ``CP"" denotes channel prediction, ``Det"" denotes signal detection, and ``Pre"" denotes multi-user precoding."
./dataset/05523506-5ff0-460f-8fea-f4eae99f2a1f.pdf,UI widget interaction.
./dataset/4b0a1ce2-0777-4c0f-b5d9-6ac0ea973eaf.pdf,The NMSE performance of the proposed method and other baselines versus different SNRs.
./dataset/3bba7679-38ae-4215-b9a7-13a3eca7aa07.pdf,"The accuracy for different rounds of self-refinement, with the corresponding executable rates."
./dataset/edc12f96-9829-4e5d-86fb-6699e7a3ebc6.pdf,"The examples of three types of reasoning patterns. As shown in the image, the first type directly reflects the correct answer in its first reasoning step, and now actually performing the reasoning, the second type is like humans, try to recall the knowledge points, and narrow down the range, and find the possible answer in a thought chain, while the third one is a combination of two, it has the steps to ask for characteristics, but it also try to reflect if it has seen this question before at the edge1, which is a direct attempt to use answer to match the question, the last step is to confirm if the memory of answer matches the requirement in the question."
./dataset/8c97b84a-6231-473a-87ae-bb1f2d72ee36.pdf,An example from the CLUTRR dataset.
./dataset/c8109cd9-e04e-4f44-9bbe-901e21859941.pdf,G-MM
./dataset/856a949e-c02d-4a3a-9516-c64304bc72f2.jpg,10:G
./dataset/35597d68-e1cd-44be-95d6-0aeda821c8cc.pdf,Example of compiler feedback and revision plan.
./dataset/3633cb82-f31d-4cb2-9ef5-e6f7e4df7c2c.pdf,Parameter-sharing wireless LLM downloading.
./dataset/03f0f42b-d659-4d80-a608-ebb857866956.pdf,Attention O mapping.
./dataset/1955f555-f217-4a7d-aa8e-56f1d274a29c.pdf,"Priors of Gemma 2 9B (top) and Gemini 1.5 Pro (bottom) models for each flight feature. A rating of 1 indicates a strongest preference for the earliest departure time, the shortest duration, the fewest number of stops, and the lowest price, while a rating of 5 indicates the opposite. A rating of 3 indicates no preference."
./dataset/4469150c-aacc-46b8-99bf-52957b781d63.pdf,Sample Slide for Evaluation
./dataset/0163c6eb-e280-4311-9145-e6996d66a88e.pdf,One example from OpenbookQA dataset
./dataset/8409da98-a99f-493e-b54d-07cbad754564.pdf,"Social intelligence performance of LLM agents under varying levels of social intelligence prompts. From the figure, the actual social intelligence performance of LLM agents diverges from or even opposes the indicated levels of prompts, indicating a misconception of social intelligence by LLM agents."
./dataset/832de3b8-9a12-441c-ba19-c2cd8c76d7f9.pdf,The proposed world-building planning methodology that learns a formal representation of an informal description of the environment.
./dataset/4eb89427-0193-4a3b-b458-98d3d6991140.pdf,MLP down mapping.
./dataset/9c677c7f-f910-411e-a9b2-6648ea456c14.pdf,MM-MM
./dataset/e086eafa-4b00-443a-93d0-ea0bb0240fb7.pdf,LLMs for Chemistry: Applications and Paradigms
./dataset/0ab26418-28df-4de5-ab69-337c87a8ca8c.pdf,Relative exposure values on GPT 3.5 Turbo
./dataset/fddba6bd-687b-4910-b926-d8d5a29de179.pdf,small Training Loss Curve
./dataset/0a662229-7335-48cb-853d-9ae23664409f.pdf,"A comparison highlighting the question-answering accuracy of six methodsFT,LoRA, RAG, GRACE,ROME and IKEon the E-DCE and E-MEE of Llama2 7b and Qwen 7b."
./dataset/a7e00ef4-1adc-493d-99bc-55605e0a6258.pdf,TL;DR
./dataset/5d235fbb-2f5c-4672-a6d2-4dce8a142a71.pdf,Centrality distributions of the node being attacked successfully and unsuccessfully. We use Sentence-Bert as the victim model and gather all attacked results from the PubMed dataset.
./dataset/9ef6066b-8b4f-4e05-b250-7b13e1ee2438.pdf,"Performance across varying input windows, where the x-axis represents dataset segments, 1 being the shortest and 10 the longest, sorted by length.} %This line chart illustrates the relationship between the average score and the question length."
./dataset/f099bbfa-7479-447c-9ff2-df254bb39082.pdf,"Central area of the Conceptboard, containing the ideas produced during the Brainwriting session"
./dataset/b3d85323-cee1-4054-880b-07e77976849a.pdf,Perplexity vs Temp
./dataset/6a2d5f40-bca6-4b8f-83fb-e43acde945e6.pdf,Prompt design for iterated learning on the acronym data-generation task.
./dataset/ffa22924-d3c5-449e-b48f-f745e61e80fe.pdf,Span based F1 scores per error category
./dataset/b1126a6a-fe0b-43fd-b94c-f76f71463643.pdf,Egalitarian belief interventions on GPT-4's implicit bias
./dataset/bff71e33-217c-4c8a-bd99-2af1c2d04f9a.pdf,Schema of the proposed Predicate-to-text evaluation method (Top) compared with directly generating test variants by LLMs (Bottom).
./dataset/54f41afb-85a3-446d-b1ba-a34acfe3c46b.pdf,The SER performance of the proposed method and other baselines versus different SNRs.
./dataset/cf3dbce2-d278-47c5-bd08-0b321f3e60b2.pdf,G-ML
./dataset/03b6c131-fc70-4e12-b000-3f78c6a3af33.pdf,Instruction for Metric Empathy
./dataset/4ab27ed2-7027-45fa-86c0-b8c4664acbba.png,The questionnaire for participants to evaluate articles.
./dataset/6e7894d8-815a-4080-be26-4fec7e287810.pdf,The screenshot of History Relational Database (H-RDB).
./dataset/892c06f9-3a98-4367-91d5-72460dac60cb.pdf,The variation of statistical metrics during GRPO training on Helpsteer2-Preference.
./dataset/8199be9f-bb9a-41e3-8bfa-554f19bca4a6.pdf,"We propose a workflow that identifies data operations within the generated code and maps them to visual, interactive primitives on the fly."
./dataset/8b4f1f94-d2ed-45c4-b0c7-e1fa0c1c611d.pdf,RouterBench
./dataset/471b477c-dc22-405d-89bf-756cd1d206cd.pdf,An example of how to manipulate $P_0(h)$ by adding spurious correlation in the prompt.
./dataset/7e17720e-40d8-4c39-a974-cefad7b3ef5d.pdf,Architecture of the performance characterization tool.
./dataset/6997ab87-0437-44d8-a40e-eec2cb5cc5bc.pdf,Scatter plots of the effectiveness of TREC DL 2023 runs based on synthetic vs. real test collections to analyse the bias towards systems using the same language model as the one used in synthetic test collection construction.}%
./dataset/215b0a02-709d-4701-8e66-da8d1a4107b9.pdf,Accuracy comparison of four models on seven datasets using both vanilla and CoT prompts.
./dataset/815f3e99-b8eb-495c-a5e6-cdb91452965f.pdf,"An illustrative example of a ``Value-Action Gap'' in LLM. We observed a misalignment when prompting GPT-4o-mini to 1) state their inclination (i.e., Disagree) and 2) select their value-informed action (i.e., Agree), indicating the 3) value-action gap towards the value of `Social Power' in a scenario involving Health in Nigeria."
./dataset/b724095d-858f-496e-b8c6-4dbb520a8408.png,Performance of GPT-4 Turbo on the BBH Dyck language using least-to-most prompting as the number of nodes in the input increases.
./dataset/6c14fdac-bfb5-4f37-bb92-d6986caf7eb5.pdf,Attention K mapping.
./dataset/3255b1fb-248f-48d2-813f-0201a0c8a16d.jpg,"A graphical rendition of the proposed risk mitigation strategy for utilizing LLMs in malware detection. The diagram outlines a multi-faceted approach that encompasses data diversity, continuous learning, real-time code scanning, targeted sandboxing, and federated training."
./dataset/931401c2-48ff-4fd8-a25e-f780d437c98b.png,DFS prompt to expand nodes.
./dataset/0493bc68-f951-40d3-acb5-fd1154228a5e.pdf,Mistral-7b
./dataset/3e81560f-9531-42f4-9523-f466aa96ce28.pdf,Names
./dataset/c2ce08e0-1cce-47a2-87d0-1ecb01a13fa7.pdf,Sensitivity of in-context reasoning.
./dataset/215c91a3-d6d0-49eb-af1f-a09379727a17.pdf,Recommendation overlap across different LLMs
./dataset/52f5352e-31b5-4d70-845d-b5dcfb785ca7.pdf,ML-L
./dataset/e17ddc6e-edf6-41ba-90be-16d12b12ef74.pdf,The variation of statistical metrics during Reinforce++ training on Helpsteer2-Preference.
./dataset/11690d35-c668-41df-8032-135ac4ba005c.pdf,Data Collection Process.
./dataset/24c23664-809d-4435-af0e-1a1243e68d72.pdf,"Taxi pick-up and drop-off patterns in NYC: the commercial Garment District (left) shows morning drop-off and evening pick-up peaks, while the residential Arden Heights (right) shows opposite trends."
./dataset/17f50e07-a520-43de-ad1a-2688145dc56e.pdf,Llama2-7b
./dataset/02494a1e-a5dd-4b68-99c3-19a449f5f702.pdf,Comparison between our benchmark and LLM-as-a-judge
./dataset/42db97f8-c5c0-4e48-915a-ec2d1c8cea8e.pdf,Comparison of calibrated model performance in different environment sizes.
./dataset/b02373fb-1e6e-408d-962a-4466775565f0.pdf,Reasoning Graph's Depth
./dataset/1f34d4f5-eef6-43ae-abd7-ada500bf5594.pdf,rewrite prompt to rewrite.
./dataset/e0c5d431-8734-4d2c-b669-d889d33a6fcd.pdf,CWQ benchmark
./dataset/0a4aba8a-5054-4053-81e8-0e38dc846539.pdf,Example of Missing Dependencies
./dataset/99a5636a-2416-415a-9770-395c63881d7f.pdf,MLP gate mapping.
./dataset/f66f3ff5-8a26-44ee-81e0-8603a76840bd.pdf,Step 1 of human evaluation: Chatting with the system
./dataset/7b74a740-4d46-43cc-9c87-db0eebe62848.pdf,The qualified prompt and examples.
./dataset/de29da26-6032-4edf-9e3b-15e173417055.pdf,Movement pattern semantic projection.
./dataset/3bad49e1-5d0d-4203-ac93-135ad19c4c09.pdf,The prompt of WebShop that contains task requirements.
./dataset/f1e15762-4086-4368-820e-38fc0be23d6b.pdf,Analyst-Inspector framework for assessing LLM data analysis reproducibility.
./dataset/acb8c420-8d8f-4fa9-b959-a5bed370265b.pdf,Screenshot of the interface to collect human answers.
./dataset/d12c7390-133f-4581-a7b7-805bb773dacd.pdf,MLP gate mapping.
./dataset/572c4b40-c992-4f40-ac59-3b3600acb6e1.pdf,Pair-wise comparison of different MT engines based on 5 averaged classifiers and top 30 salient features
./dataset/1db4f750-c8ea-457c-836c-3bcbb06a19b9.pdf,Illustration of conflicting video pairs/triplets for different temporal aspects.
./dataset/840036aa-426f-4996-a970-35c102918e1b.png,Percentage of correct responses for Baseline and MALT in Gemma-2-2b and Llama-3.2-3b.
./dataset/e40959ac-739e-480f-9f88-07518e66ef88.pdf,Crowdworkers' Working Interface - Instruction
./dataset/e88f842e-25e7-4360-ae22-a70deaa42a4e.png,Comparison between traditional logging models and LLM-powered models.
./dataset/246353d6-0564-4880-b50d-62b72a1ae36b.png,MADDPG
./dataset/546f4a3f-5053-4db1-97ff-f14116688ccd.png,Models accuracy on BBH Navigate when applying DARG.
./dataset/76ec6e9b-d988-446f-a138-8b1240b43645.pdf,Logarithmic
./dataset/5ea83e1e-c4fe-45ac-a3f7-85f60dd5c578.png,A template from HEA whose HE is removed.
./dataset/52fdddd0-dae0-49da-acdb-4f422b3fc6bd.pdf,High-level schematic diagram of a multi-tier distributed LLM architecture.
./dataset/5053becc-736f-40ee-88db-f3f4c4b92283.jpg,"Model performance distribution on different tasks. Ex. means explicit, Im. means implicit, and Open. means open-ended."
./dataset/6f560c16-c162-427d-9146-f1c1f14fe9f5.png,"From left to right: Object 1, 2, 3, 4."
./dataset/28056d24-9ce6-4468-9272-3e9d861d0c35.pdf,"Linguistic differences among LTO and other LLMs. The left panel compares MTLD, while the right panel examines the ratio of causual conjunctions."
./dataset/a58c7c6c-9ae0-40cb-a1ee-82782d81f5ad.pdf,Average performance on nine QA datasets with various numbers of virtual tokens.
./dataset/60045410-507d-4f76-add5-578719cb0c1d.png,Case-2
./dataset/dc7f604e-386b-4101-9eee-e00c2c07cbed.png,Temperature
./dataset/7db3e9a7-b4a0-40da-ab2f-da4b6fb71589.png,Efficient bit-transpose operation of our engine.
./dataset/8b1a0adf-b9aa-4879-95d3-ec58e294a03d.png,An example of the working pipeline of Retrieval-based Few-shot Learning.
./dataset/b46733aa-da32-4d04-b2ad-857c11160f5c.pdf,"The prompt for client simulation. For the Chinese version, please refer to our GitHub repository."
./dataset/3a2cd560-153a-419d-82d9-fbd33ae78111.jpg,"The number of Self-Reflected Trajectories generated by different teacher models, along with the average reward of the LLM-based agent trained on them."
./dataset/75513548-231f-4db0-8b29-d8679fea6ed4.png,Diagnosis distribution comparison.
./dataset/ad2c1edc-de81-40bc-8f2a-54df27fcaf1b.png,The overall framework of Knowledge benchmark
./dataset/326f403f-b55e-4f46-9b4c-497e81f33f2b.png,Doudizhu game replay for Mastermind (landlord) v.s. DouZero (peasants). The landlord wins by playing more strategically.
./dataset/54b9ccde-1285-4372-87ea-897ad15d546f.png,TL;DR
./dataset/72488828-cd22-4db7-95e5-09c70aeb3061.png,Overall success rate (pass at 1)
./dataset/fe617884-aa37-4b15-ba2c-1cca38ff821e.png,The NMSE performance of proposed method and other baselines versus different user velocities.
./dataset/45ff7ec2-5789-4455-82a9-71ed9a82a0ed.pdf,Performance comparison of R$^2$AG with various retrievers on NQ-10 dataset.
./dataset/85e93f8a-6bcf-46ae-bba8-fd5be503a919.pdf,"The performance of LLMs-as-Predictors against structural attacks, evaluated by accuracy."
./dataset/9c37cc47-450a-4347-8b76-7b06b49bcda9.pdf,L-G
./dataset/7e6ce444-5e40-40b0-864f-b4990fad9bd1.pdf,EmbedLLM
./dataset/805d54be-78dc-4d88-8911-82036eb7eece.pdf,entering LLM prediction
./dataset/83df2c68-2d8f-4dae-83e7-24342cc4c2a6.pdf,Overview of methodology design.
./dataset/088a444d-cebd-421e-8620-b47b81fa72b5.pdf,Sample case of the Claude-2 agent-enhanced model on a double database question.
./dataset/2a613429-a7a2-4364-932b-866368148744.pdf,MADDPG
./dataset/e05043ae-a73f-4871-8b9c-74766c06b845.pdf,"Second part of the evaluation engine prompting, giving the specific justification for each corresponding rating per each criterion"
./dataset/b223f8da-8e0f-4173-aa55-8e89e64e7219.pdf,Attnention V mapping.
./dataset/1c0b26d8-fc05-4d6c-86cc-ca16647df0c3.pdf,"The significant difference results of the GPT-4 model in toxicity bias, sentiment bias, and vigilance bias across dimensions. Darker shades indicate lower adjusted p-values from statistical tests, suggesting stronger evidence of significant differences between dimensions. P-values below 0.05 imply a statistically significant difference between the groups compared."
./dataset/5af56222-d0cc-4c22-9500-2a170d8e35e2.pdf,An example of CFA Level 1 problem.
./dataset/3494a3b5-5a6b-4058-85ec-8ae30bb51ba7.pdf,WritingPrompts -- Human vs. LLaMA-3
./dataset/5a0f6e87-6656-4ba4-b54d-c526a254b71d.pdf,Observed differences in NDCG@10 for pointwise scoring and listwise
./dataset/5252cd6b-c990-48e4-b142-ddcf44226b1e.pdf,MADDPG
./dataset/5aa75f43-b3e2-416f-9661-7990f2602c78.pdf,LLM inference has an autoregressive nature.
./dataset/24fc7aac-d51f-42f6-8340-84741a6a79e5.pdf,The prompt of ALFWorld that contains task requirements.
./dataset/6ae22a76-79d0-4286-b1af-87803ecbd312.pdf,Percent Change (PC) metric shows that clinicians are much more likely to flip treatment recommendations for self-management and resource-allocation than LLMs are $(p < 0.01)$
./dataset/48f811ca-16d0-4e9c-ba2f-010fffbaa107.pdf,"Case study on reflection. Green underlined codes represent modifications based on feedback from the previous iteration, while red codes indicate syntax or functional errors in the current iteration. The content within the error boxes summarizes the corresponding reviewer plans."
./dataset/6d599eb8-782f-4721-b47e-917f165962e7.pdf,Example of Incorrect Planning
./dataset/a53c1700-d1ed-4a63-86fc-fd8b456b0e80.pdf,"SMT vs. Text Error Ratio Analysis for o3-mini: Illustrates well-calibrated SMT generation, indicated by a strong correlation between SMT and Text error patterns."
./dataset/32e39999-331e-49fa-a10d-c59d0d824436.pdf,"small Visualization of the representations from RA-Rec, BERT and ComiRec."
./dataset/b5f10743-cd3d-444e-977a-491a63c6473c.pdf,"We find that ablating the helix dimensions from the residual stream is roughly as destructive as ablating the entire layer, providing additional evidence that GPT-J uses a numerical helix to do addition."
./dataset/64e9b6a1-e6dc-4b01-a4b4-a2299430f347.pdf,Results for all participants regarding changes in the relationship with the brain using layer 30 of OLMo-0724 and Japanese annotation and MMLU.
./dataset/be1b051b-2590-42c7-ad05-cdc3ed4f82c2.pdf,Instruction for Metric Scaffolding
./dataset/a0c76add-d7f4-48a3-88a1-e821b554ec96.pdf,An example showing the input and output of the database selection and planning module
./dataset/942b8100-5701-4a84-9be8-c33158262962.png,Distribution of hallucinations across different LLMs.
./dataset/bd8a9438-cd70-4e3f-a455-e2175fd254bd.png,Ablation Study about Length of Ensemble Steps for Probing Detection Set.
./dataset/26a32177-d83b-4559-89d6-b8d69684b33e.png,An example narrative script produced by our retrieval-based pipeline trained on wikiHow. Each event is represented by its Event Type and an example sentence.
./dataset/8eab5a06-645c-4f39-bb3e-10b4863a1760.png,3D question answering results  of our MiniGPT-3D.
./dataset/d0fe84d3-bbe0-40c4-870b-e1a1e5c15c38.pdf,Average LLM Consistency (Including Hallucinated Parameters)
./dataset/87b2dec3-9b34-4da5-9982-874e14a953ab.pdf,An overview of our proposed IDEA-FinQA system.
./dataset/b3bdf30f-52bd-4db4-8b5f-88924b3debbd.pdf,"The relationship between quantized weights / activations, entropy, accuracy, and compressibility. Achieving high accuracy and high compressibility is contradictory in theory."
./dataset/c9ed0cc2-534b-49d0-8769-d74636ffabff.pdf,Average performance on nine QA datasets with different number of retrieved passages.
./dataset/5ca789a7-67b9-4895-b2a4-a4c5970d0a28.pdf,"Distribution of total selection counts. The box plot elements show the median, quartiles (25th and 75th percentiles), and the data range."
./dataset/2c890705-6770-405e-8763-c9bb880dbda0.pdf,Verification Successes of DeepSeek-V3
./dataset/e5294189-0752-4918-a008-e6a4afcd4835.pdf,Comparison between our benchmark and Nejumi LLM Leaderboard 3
./dataset/be095f87-8660-47e3-b9b5-a59cf36b54eb.pdf,Summarization results for different models.
./dataset/927ac5a9-4942-4f92-8376-e81773fe92f5.png,"Dejaboom! game layout. A map of the village where the game takes place, showing the locations, objects, and NPCs. The player begins the game from home and their goal is to diffuse the bomb before it explodes again."
./dataset/8c2b6c65-a9aa-4b88-8e5b-eada46dfbca5.pdf,HH-RLHF
./dataset/4201b09f-958c-4701-9554-ecb09589a943.pdf,Technical Route of LLM+TS
./dataset/9be5d844-7288-498f-8228-4fcf7aa1ec31.pdf,Case Study of Combining Structured and Unstructured.
./dataset/4e44e261-bb65-469f-9384-d573623c439d.pdf,User Interface for Conversational Search (ConvSearch)
./dataset/09d7ccf6-0a2d-49e6-9486-184e1b61c472.pdf,Law
./dataset/57daa269-a924-453c-92da-d4e92a793d1d.png,Comparison of Mean Cosine Similarity Scores Across Models Using Subtitles and Screenplays
./dataset/e1f3281e-ba1a-4f4e-919e-78c0841138c9.pdf,RMSE
./dataset/75a3a94a-3f67-457a-ae7a-ab778f78522e.png,A Python example showing how the similar pattern helps rectify errors in current optimized code.
./dataset/fbdb374a-2d44-444a-bb13-b29a4f32bdd9.pdf,"The ratio of models with unchanged predictive power after LLM rewrite, across different author attributes and different LLMs."
./dataset/a35eaae4-d1f3-40b7-88f4-37ffe30f8eb0.pdf,Average performance on nine QA datasets with various numbers of virtual tokens.
./dataset/ae8d3f06-8517-4d49-b6bd-94a06b7430f7.pdf,G-G
./dataset/c36dc27f-13cf-4442-846c-7a70f99b9c63.pdf,Value class distribution of original Touch23-ValueEval train dataset and ours with generated data.
./dataset/ae3dbb03-cf12-4ea7-9b19-838cd28b1642.pdf,"The row completion test: from the text files of sensor data, we randomly sample a few rows."
./dataset/4c7bd84f-deb2-45c7-8cd1-1c9c92ed49dd.pdf,Distribution of weaknesses in different target LLM.
./dataset/e1b69f62-55e3-47f0-9e9a-e797f1c4b836.pdf,Source distribution of the Lithuanian component of the CulturaX dataset.
./dataset/760442ca-c730-45c4-af5b-0921c50940a7.pdf,Human Fleiss Kappa for each LLM pairwise comparison under four scenarios.
./dataset/23b6d358-85a2-47d8-8d09-b9e7a1dbf7f2.pdf,Example plot for analysis in semantic relevance between model's response and target queries during multi-turn conversations.
./dataset/f8dec585-9511-4785-9d3a-515486ef7996.pdf,A general view of LLM-based malware generation techniques to generate malware sample datasets.
./dataset/3261d4ad-10a9-414b-a169-038ab4d0f46f.pdf,Theme distributions of the node being attacked successfully and unsuccessfully.
./dataset/9c626d5d-3f76-4f56-96f9-6c9a2bca035e.pdf,Visualization of attention maps in the movement pattern semantic projection.
./dataset/0819f14c-c75c-429d-8799-84b37c5e3717.pdf,An ICL prompt example for Text-Based Molecule Design
./dataset/1e6fc339-8d78-4f48-ad8b-805bdcfac0f5.pdf,"Hierarchical classification of various methods through which LLMs can be employed to detect malware. The central node represents the core LLM technology, while the primary branches emanating from it illustrate specific techniques and applications. The secondary branches provide discussion points of their respective parent nodes."
./dataset/b252a28d-5ef8-454f-9020-a6aac2627938.pdf,L-H
./dataset/22ea94fd-3453-4f3d-9e3b-41be4692d20d.pdf,Results when adding different interaction phases (4 different seeds).
./dataset/8576289f-be40-4f04-ad6d-2527926a739d.pdf,Existing LLM for Graph methods and our NT-LLM. NT-LLM proposes a node tokenizer to enable LLM know graph structure without any extra edge or subgraph sampling during inference.
./dataset/c8a19343-1a1f-49e8-89c1-e5137a8443a1.pdf,The general model architecture of MM-LLMs and the implementation choices for each component.
./dataset/a8234d6c-b9d2-4a15-b860-03343142e15a.pdf,"Advantage of Cappy scoring over FLAN-T5-XXL's self-scoring, on 45 BIG-Bench tasks. The x-axis is the names of the tasks."
./dataset/0b91cf29-c69b-4d9d-9b9e-c8611b7806cb.pdf,Schema pruning.
./dataset/89203ca1-efab-4e3e-8706-1158cc1fb0b3.pdf,Mutual Information (MI) metric shows that  human decision-making is less sensitive to gender and style perturbations than LLM decision-making $(p < 0.01)$
./dataset/8e06ba07-3497-4e00-ae75-6fafa0ad2a3d.pdf,Open-ended Gen.
./dataset/52f7c90e-f50e-4e9d-a7b7-d49cf821347e.pdf,"Accuracy of the in-context linear regression task. We compare the setting where the LLM predicts the best index of the option set versus the setting where it predicts the score for each option. The LLMs achieve significantly better performance when predicting the score of the option than when predicting the best index, suggesting that they can effectively solve linear regression tasks in-context but struggle with tasks that require inferring latent structure."
./dataset/9208a9dc-865a-44f8-a03b-429922a34f89.pdf,An ICL prompt example for property prediction
./dataset/788144d9-a87d-49ef-b782-b4c2942f13a3.pdf,TL;DR
./dataset/a5d17799-58a6-4a23-a84b-459331164784.pdf,A sample prompt for malware code analysis using zero-shot approach for websites
./dataset/0e11c0ea-f252-4727-be17-8f509f2b88b1.pdf,The top used terms in the joint vocabulary created by LPA for the terms used by humans and GPT-3 in their ideas.
./dataset/8b9ccf33-761b-4d05-a42b-4ec178ba4022.png,Complex reasoning.
./dataset/de2256cf-951f-4c64-a94e-b469e88d5253.png,The overall architecture of our simulation framework. Left panel: construction of client pool. Middle panel: data collection with interactive simulation. Right panel: model training.
./dataset/1a6addfc-00ed-4eb5-98f4-19f5c3a892b9.pdf,The human annotation interface.
./dataset/dfeb7db7-9ef4-4d8d-a06a-1c3ff86c91e7.png,"BLEU v.s the frozen parameter ratio of the fine-tuned GPT-2 large. In this example, LoRA enables the integration of fine-tuned parameters ranging from hundreds of kilobytes to tens of megabytes into a pre-trained GPT-2 large model, which has a size of around 3.02 GB. This process creates multiple GPT-2 large models tailored for specific downstream tasks. The frozen parameter ratio refers to the ratio of the pre-trained GPT-2 large model size to the fine-tuned GPT-2 large model size. BLEU is used to evaluate machine-translated texts against high-quality reference translations. ``Full FT"" in the figure indicates the full parameter fine-tuning."
./dataset/32146329-b5c2-4001-abed-c1664e6c36b0.pdf,$N_s$ on C++ (O3).
./dataset/09501ac4-d719-4cd0-b2dc-30216354bb30.pdf,"When testing our helical fit on other numerical tasks, we find that it performs competently, but does not always outperform the PCA baseline. This implies that additional structure may be present in numerical representations."
./dataset/4464ada8-4e43-4663-8e14-7b2976a89955.pdf,Intent conflicting: overall semantic conflicting.
./dataset/0908433d-b131-42ed-a1dc-751e1ce7604b.pdf,Tensor-wise each layer compression ratios.
./dataset/d05330b6-676b-476a-a90b-bdb112568143.png,100:G
./dataset/ccef9ae6-1d69-41c1-982a-f37d68fecc11.pdf,Structure-Level abstraction mapping rules.
./dataset/771e6c9c-b3e4-4f88-822b-25c52ab67af2.png,Results of different models on five question types in nuScenes-QA dataset
./dataset/0acf131e-0537-4a9f-912e-d3b57fcff685.png,"Latency comparison of 4-bit quantized matrix-matrix multiplication using our kernel, separate dequantization followed by cuBLAS FP16 kernel, and cuBLAS FP16 kernel on RTX 4090. Matrix dimensions are ($M$, 4096) and (11008, 4096), with $M$ varying from 1 to 512."
./dataset/38daf9e9-0f68-46e0-92e1-b23b3d7c79e5.pdf,Regression-based Performance Comparison Across Models (Business Proposal Evaluation Task)
./dataset/2a1057b8-09a8-4d10-844a-6f0a4620ef65.pdf,"An overview of our benchmark. The evaluation is divided into two perspectives: LLMs-as-Enhancers and LLMs-as-Predictors, both of which consider structural and textual attacks."
./dataset/8f2840a9-1bf3-4b0e-add2-f1214a55b3c9.pdf,MAE
./dataset/f5123d38-f407-4d1b-be08-fab5fd152b48.pdf,The plot illustrates the distribution of attack queries for successful attack examples utilizing the CoA-Feedback method on the PAIR and GCG50 datasets.
./dataset/e1f11fd2-e279-4444-95c0-36a93eaaf217.pdf,HH-RLHF
./dataset/088f0027-9705-421d-94d8-5ec3e27c15f2.pdf,QMIX
./dataset/0459d48b-c871-479f-b1ec-04ee5a75c321.pdf,"Task formulation: given a method and a specific logging point, the model is asked to predict the logging statement in the point."
./dataset/b27b49bc-04d3-4e51-a1ec-c2ce8cac4921.png,Observed differences in NDCG@10 for pointwise scoring and listwise
./dataset/bd265bc1-ab52-4bae-842e-3ec29367806a.png,"Graph size against the value of the $K_n$ parameter (retrieved nodes), for the CWQ benchmark."
./dataset/e874a0bd-c2d8-4537-b637-19fb935522d0.png,TTS case of Error Neglect.
./dataset/66877f2b-9ae0-431c-af16-83d2d891eb7a.png,How LLM understands the real-time scenes and help Recommendation System work.
./dataset/96a8778f-a670-4d21-a54b-fa4513b6aa36.pdf,"Qualitative results of Traj-LLM on various scenarios under full-sample training. In each subfigure, left: HD map, right: predictions and ground truth. Ground truth future trajectory is depicted in green lines, the predicted trajectories are in red lines. The top and bottom parts respectively exhibit the instances with prediction modalities $K$=5 and $K$=10."
./dataset/405a9133-46c5-4e56-ad69-ba84ea5f5780.png,A Use Case - Leveraging Hierarchical Language Model Architecture as On-Demand Service.
./dataset/3d10279f-4845-47e4-a313-f51f74ea4ef1.png,"An overview of the proposed PretexEval framework, which dynamically generates test samples from any medical knowledge base for evaluating LLMs medical knowledge mastery."
./dataset/56a731f5-1236-406e-bf94-5c95ac396687.png,Observed differences in NDCG@10 for pointwise scoring and ranking
./dataset/40fb7f5b-7965-4303-92c2-c2f64b5a3508.pdf,"The parameter-sharing backhaul/backbone LLM delivery framework. In this example, the AI assistant and autonomous driving applications request LLM 1 and LLM 2, respectively, which are not cached on the associated edge server. Therefore, LLM 1 and LLM 2 need to be delivered from another edge server. In this framework, to reduce communication overhead and latency, only the specific LLM parameter blocks need to be transmitted since the shared backbone/parameter blocks are already cached on the associated edge server. The entire LLM can be assembled before downloading, or on edge devices once all the needed parameter blocks have been received. To further reduce delivery latency, parameter-sharing LLM delivery can be combined with various compression techniques, where the compression ratio can be determined by jointly considering model performance and channel/backhaul conditions."
./dataset/46ed3d96-0140-44de-b80f-67c3ae5001cf.pdf,L-MM
./dataset/55b2e8cc-8b74-4ddb-8cb3-8abd0356ef11.pdf,"How GPT provides the rule following the given format,"
./dataset/90e8cede-a59e-48da-83ab-d106b1772d7c.pdf,The fixed CoT-based further query.
./dataset/18de5203-d86d-4d16-9ea7-33b191af43e4.pdf,"Performances of a range of models, relative to FP32, with 4-bit integer weights and activations using Adaptive Block Floating Point (ABFP)."
./dataset/55b8945b-ac28-4e11-bd05-b3bf8975307e.pdf,A case of evaluating LLMs using PretexEval compared with the LLMEval method.
./dataset/b2e3e98f-4a04-428c-b4e6-1b84a80c583a.png,Results for all downstream tasks regarding changes in the activations of OLMo-2.
./dataset/72adf509-a220-4aae-990f-9cee350b8eb0.png,"An example of the Label Studio GUI using a mock interview. In order to protect interviewee anonymity, interviews will not be released."
./dataset/bceb2cd0-4ffc-4720-9af2-f4602ddee633.png,Annotation progress and summary.
./dataset/825402b7-deca-473e-a541-a927a6b1e889.png,Distribution of preferred products in mutual fund investment.
./dataset/5ce8ac52-a179-4ece-8dc5-8ef56ff1a045.png,"As shown in the upper panel, an LLM can still detect maliciousness in a negative scenario camouflage, while in the lower panel, our HEA builds a positive scenario via a happy ending and fools the LLM into responding to the malicious parts in the prompt, getting jailbreak responses"
./dataset/571eb169-de09-4f94-ae27-0767991489c1.png,Average Number of Correctly Assigned Parameters per LLM Prompt 4 vs Prompt 5 (no documentation)
./dataset/dcb822a9-dd31-403f-a3c8-fbae78257a8b.png,Illustration of our fine-grained task selection strategy TaMAS.
./dataset/eadc5d19-a1ab-44b6-a172-41a2eeb2c5ae.png,"Visual comparison between DRAD and baselines across all datasets. For the evaluation metric, we choose Accuracy for StrategyQA, EM for 2WikiMultihopQA, and F1 for NQ."
./dataset/04f4449d-39a9-4363-80f3-3474b82dae20.png,Comparison of the impact of verbalizer format description in the prompt.
./dataset/19c27bc6-5300-46d8-ba7e-eebafbe2f924.png,"Row completion test for a file from the Daphnet FoG dataset: here, the values in green are correct, in red are incorrect, and purple are extra predictions by the LLM."
./dataset/7ce5d59f-c8d9-400f-9e98-f4ec8feb9c7e.png,"A workflow diagram illustrating the process of updating domain-specific LLMs through Sequential Fusion. This includes extracting structured knowledge, converting it into natural language, and editing the LLMs using the IKE method."
./dataset/9d58c804-e661-447c-830d-57c21b85bfe6.png,LLama 2 70b
./dataset/21792d59-63c4-4734-9534-986ebc67af46.png,LLMs's performance across various types of question variations.
./dataset/0a37daa9-287a-479c-8590-7d1017e55266.png,The performance of pre-trained GPT2-Large on specific tasks. The performance of pre-trained GPT2-Large is much degraded relative to a smaller task-specifically trained Bert-Base model. The performance gap can be filled by fine-tuning GPT2-Large with task-specific data.
./dataset/0ea6129c-38af-4365-be7d-5cb5e9a54e3a.pdf,footnotesize A data exploratory analysis of the constructed RelQA based on different metrics.
./dataset/4b572812-c858-4955-b37a-0c52f0c60bab.pdf,PLOS
./dataset/8b96423b-05fc-4cbf-8592-8308b7df7616.pdf,MAE
./dataset/c5f0e3c1-a14a-4aa6-b7c7-6029d7408491.pdf,Mutual Information across Transformer Blocks
./dataset/0b9c6708-9005-4bfd-93b7-65044718a7fe.pdf,Context inconsistency: inaccurate express
./dataset/4754bf4c-b4b7-4644-8dbc-59d04867124f.pdf,Generated Persona
./dataset/6dcee1c6-2915-4950-90ec-97a121ba0662.pdf,User interface used for the human preference study.
./dataset/d0c81904-1ab9-45d1-8cab-3020e194e761.pdf,urs assesses model outputs for patterns similar to human cognitive biases and tests various bias mitigation techniques.
./dataset/8c156e54-20c5-4244-afe2-735ab6ae7f4f.pdf,Crowdworkers' Working Interface - Factoid.
./dataset/abe7888d-ca39-4b11-b131-0537846369aa.pdf,"Effect of EAST on the distribution of actions executed in different runs in various bandit problems. EAST's effect on an LLM agent's representation effectively guides the agent towards more explorative behaviors, steering it away from its typical overconfidence."
./dataset/5b9f03dc-f730-469d-be6c-b0e2a9211cb2.jpg,"The shape of the capability score varies significantly across different datasets, and differes from the safety landscape."
./dataset/25ab525b-306f-446e-bc91-70b0fe89bb14.jpg,Text classification results for different models.
./dataset/24008f5b-4df3-4451-9293-edd400fe0ff0.pdf,Reconstruction of trajectory points in cross-reconstruction pretext task.
./dataset/2a4d71f5-68d7-45e7-af04-5203c6c4ee4c.pdf,One shot.
./dataset/3a9106ae-18c5-453b-badf-1c25756777ea.pdf,"Illustration of our decompositional retrieval-based reasoning method. Our method decomposes the question into sub-questions, performs iterative, context-aware retrieval conditioned on previous answers, and merges the resulting subgraphs for guided reasoning."
./dataset/587ea20e-5db7-4374-9bdf-363156255046.png,Comparing defect and refusal rates of various models
./dataset/1e06b846-adf0-42d0-8377-823974d347ae.png,"Examples of standard instruction tuned LLM, instruction tuned LLM with manual role-play prompting, and self-prompt tuned LLM on the same physics question. Manual and automatic role-play prompts are highlighted in gray and blue respectively. LLM used here is Mistral-7B."
./dataset/caaad8fb-45f4-4a22-9c5e-e0a553d96e10.png,"Yes to No ratio in absolute decisions: The horizontal axis represents 21 categories, organized by social domains in label colors. The vertical axis represents the type of decisions with gender-career as an example. Values in the heatmap represent the Yes-to-No ratio."
./dataset/59ec41d8-3281-47e8-a9bf-6d3d6fd2538b.png,Histograms of Token Count Distributions for Questions and Answers.
./dataset/0484fec0-3d6c-4650-a22e-1ad93250e7c0.png,Standard deviation between targets under each dimension. It indicates variabilities between social groups.
./dataset/dacc4ce0-fb17-473f-965a-919c0d2f6cd8.png,Numerical Complexity
./dataset/3c4d6971-567d-4942-b88c-9b7d72c25bf3.png,"Llama3 Model's Heatmaps of (A) Task1, (B) Task2, and (C) Value-Action distance across 12 countries (left) and 11 social topics (right)."
./dataset/7aa568db-06c7-4743-bdb4-993b5bc09335.png,"MasterMind-Dou training pipeline workflow. The LLM first predicts probable legal moves, analyzes corresponding opponent responses, and then determines the optimal decision."
./dataset/093991f0-3f31-4c72-a706-ca048820e701.pdf,Illustration of Three Stacked Dilated Causal Convolutions and Composition of the i-th Layer of The Chosen Architecture
./dataset/0c2a0260-9d3d-4c43-9ad1-9362b132f30f.pdf,"Average Number of Hallucinated Parameters per LLM, Prompt 4 vs Prompt 5 (no documentation)"
./dataset/ca23a6f5-2c4c-4a7d-bbb6-ecf3783fcb25.png,Model generalization with unseen entities. X-axis indicates the partition of seen entities in the training set.
./dataset/d4fab19a-9d16-49cb-b4da-5cd45bd49757.png,Inconsistency binary classification per error category
./dataset/68b0fa8a-241c-4e16-ad5c-6d0642495517.pdf,"Illustration of jailbreak methods. Manual-designed methods typically demand substantial effort and face challenges in adaptability across LLMs. The automatic searched suffix lacks meaningful semantics, which can be easily detected by PPL algorithms. In comparison, our RADIAL method is an automatic process that designs semantically coherent attack prompts."
./dataset/fafc33f3-db08-4d7b-99a8-1908250a60fc.pdf,Toxicity on the PAIR and GCG50 datasets.
./dataset/8c7ab396-0501-4643-abe5-8379a79a651b.pdf,The prompt of SciWorld that contains task requirements.
./dataset/5b8cf9b6-3620-44ac-99cb-9f5871fa26f2.pdf,"Comparative Analysis of Students' Self-Reported Mental Demand during Task and Correctness of SQL Queries between Conditions. The left panel shows the average reported mental demand, with no significant differences between conditions but directionally lesser average mental demand for students using Instructor-tuned LLM. The right panel shows the average correctness of SQL queries, highlighting higher correctness when students used either of the LLMs compared to the web search condition. Error bars represent +- one standard error of the mean."
./dataset/bec084ab-0eb4-4858-96c9-134512c60e27.png,Detailed illustration of encoders and decoders.
./dataset/b21818e3-61fd-486c-a9b0-faa109f67e74.pdf,The Effect of $p$
./dataset/09dad407-9af9-4a4b-8939-8ed9ac9630ba.pdf,"Generalization to Unseen Color Terms: The left panel shows example training data, including primary, secondary, and shades of red. The right panel demonstrates model outputs for an unseen color (navy blue). Importantly, navy blue is in a separate color subspace from any of the colors used for training."
./dataset/79cf23b2-f932-4679-bf47-3a6ca883d4a9.pdf,Illustration of Different Positions of Annotation Guidelines in the Prompt
./dataset/881db334-290a-4081-86a6-0dd731c51e5b.pdf,50:ML
./dataset/82c72950-5bd0-4843-ae03-7cc1826a2f07.pdf,An example of the Financial Reading Comprehension Instruction.
./dataset/37e67ed3-44ff-4e3a-a3fe-73d5782741d7.pdf,Case Study.
./dataset/69573929-af26-49d3-a154-3ba281e7fbf8.pdf,Length of attack prompts across construction methods.
./dataset/063d3afb-970c-4f65-aa9e-87798edecb10.pdf,The Process of the SKT Module. This diagram illustrates how structured knowledge is converted into natural language through the Mapping and Semantic Integration stages.
./dataset/dd5fb387-3838-4d0b-80b1-5227339f0270.pdf,Relative exposure values on GPT 4o Mini
./dataset/d7d71d61-bb3f-4f6f-8012-05d4c2f2c91f.pdf,Average Token Counts for Questions and Answers by Difficulty Tier.
./dataset/e57de898-0b1a-450c-b31d-2433dad0a4d3.pdf,sk-retrieve prompt.
./dataset/6ff8c6d3-243b-4812-9a41-136775af0f3a.pdf,RMSE
./dataset/1ff78330-9e09-4504-9dce-0da6689ed58f.pdf,Examples of molecules generated by different models.
./dataset/b043e6b3-53cf-407c-a37d-9d5d9ed3901a.pdf,Example outputs generated by the framework
./dataset/33b523d8-21d4-4390-8d4c-f4d600062e6e.pdf,ML-H
./dataset/c1c25344-877e-42bd-9916-cc84596cbde6.pdf,A real case under the Evil method to explain why GPT-4 evaluation is more accurate.
./dataset/b174d7fa-02b5-4d8f-abf1-3e0b7a5ef2dd.png,Remediation Module Architecture:
./dataset/21b38cb6-075c-45d5-a32c-7b9c5540fedc.pdf,The variation of statistical metrics during offline learning trained on Helpsteer2-Preference.
./dataset/94c1675c-ec01-4255-aa25-af33c061d6b4.pdf,Demographic
./dataset/7b59644c-4499-4a4c-be30-10a9a42b10ee.png,LLM Modulo Framework adapted for Travel Planning
./dataset/f1454d78-2da2-4767-a689-6ee93577a203.pdf,Latency over heterogeneous workloads. %W1 and W2 are the two query groups used to generate each workload.
./dataset/c659fbc0-afb5-4bb6-aaa0-a704a4af51e3.pdf,Inconsistency rate of all models per dataset. We see that Alpaca-13B is competitive with BART with respect to consistency on the DialogSum dataset but is outperformed on the SAMSum datasets
./dataset/056ad6e0-efae-4405-8a2e-06464fa87d56.pdf,Normalized log probability for the first token of counter-answer when counter-memory is the only external evidence presented to Llama2-7B.
./dataset/5e4431c4-2b7c-4a06-a376-797ff59e6b6a.pdf,Average Rules per Non-terminal vs Temp
./dataset/562b85a1-4621-4e50-a447-dfbbfe068947.png,"The Figure provides LLM's decisions about parameter ranges for some iterations on the M5 dataset with the LightGBM model starting from the second iteration, as the first iteration is initialization."
./dataset/b54f098f-120f-4151-a080-e5a962cfb8be.png,RMSE
./dataset/ce01aea3-f107-4991-b526-f11925f574bb.png,"An example of lack of group fairness. For the same attribute with only the target altered, the output shows toxicity towards ""middle-aged"" but was safe for ""elderly"". Additionally, when shifting dimension from age to nationality ""Egyptian"", the LLM changes its strategy and declines to comment."
./dataset/e5f93dfa-36cf-4de0-b9e2-b7e58d5ca047.pdf,"A final set of ideas, chosen from the ideas in the central area"
./dataset/e758001d-fbfd-4ce8-b652-2879bec21d91.pdf,Venn diagram for logging levels prediction.
./dataset/0fa1002a-b405-46cc-9989-2b5dc7df1918.pdf,Comparisons of WAI-O-S scores between RealPsyDial and SimPsyDial.
./dataset/3095d71f-4155-47aa-b860-456c901c86c5.pdf,Experimental results (MAE) for different tasks of the LLM services performance estimation (our method) and baselines.
./dataset/9928b436-6bb5-422e-afe4-5cec6f8371f5.png,An ICL prompt example for yield prediction
./dataset/d3281209-8a10-48f8-810d-3beab9c6bffe.pdf,"Heatmap for correlation matrix for social and academic intelligence measures. Intuitively, there is a comparatively low correlation between the performance of LLM agents in social intelligence and academic intelligence."
./dataset/fbfc0dca-f8b2-46e7-8739-28c4e7c5a52d.png,Distribution of task instructions over the temporal aspects.
./dataset/1a015793-328c-485b-82c5-ac6b7f72c074.jpg,Gaussian
./dataset/c923cd6e-0180-475b-92c3-e57d13d841e5.png,"The illustration of different settings to instruct LLMs, the evaluation metrics are also displayed."
./dataset/8e1addf9-176e-44d3-b0e2-d59733d77625.png,Example of implicit bias and decision bias in explicitly unbiased LLMs.
./dataset/f7e63842-9cf8-4579-8aa7-4e1750bd862b.png,A case study on the BBH Dyck Language dataset wherein GPT-3.5 Turbo accurately generates the correct answer but errs on the modified data with increased complexity
./dataset/d1e52041-5bcc-44ff-a4c5-01bced5b5714.pdf,Pipeline of the PEFT-based LLM-driven LAD approaches.
./dataset/60a94eb9-f2f1-48e0-a4a1-de61b3793767.pdf,"Accuracies (y-axis) of individual MMLU benchmarks for LT-Llama2-7B model, pretrained with different proportions of Lithuanian component of CulturaX dataset (x-axis).} %You may zoom into the figure for details."
./dataset/6f9133c3-ee57-4900-a787-fa61da45da5f.pdf,Scoring and Ranking
./dataset/7ddd9bc0-03f2-415f-9564-f23676890b02.pdf,Python.
./dataset/82c46e5f-1e70-4926-8a15-4c8ebdbc371d.pdf,F1 score against ratio of the dataset for Llama3 on various dataset.
./dataset/8b417959-4f25-4bfd-908a-93201a132670.pdf,"Perception inference, perception-to-belief inference, and ToM performances of LLMs in true and false belief scenarios of Percept-ToMi and Percept-FANToM. Although the models exhibit similar accuracy in perception inference across both scenarios, their performance in perception-to-belief inference and ToM scenarios varies significantly."
./dataset/c3c136d8-77ba-4bb4-8444-aa18ab1736fb.pdf,BBH Dyck Language
./dataset/8716c45a-1640-4e4a-8169-e063409cc023.pdf,GPT-3.5-Turbo vs. Claude-Instant
./dataset/4fe41392-1629-4a1b-ac0f-04f92cf3e2b6.png,"Response distribution of Hallucination, Linguistic Acceptability and Task Quality metrics for humans and LLM evaluator in Direct Assessment."
./dataset/65ce9e8b-6a6e-4e4a-98a7-93fc4e4b5075.png,Framework of LLM for TS Tasks
./dataset/07ca875d-b8e4-452c-8d2e-5b9d4c94afc4.png,TTS performance of policy models with parameters from 0.5B to 72B on MATH-500 with different scaling methods.
./dataset/aa3fe9fd-1d2e-47f3-a7bd-03fb3e4bd9c4.png,"Evaluation of ""informational"" using GPT-4 as judge."
./dataset/72434366-3c30-44e5-8fb5-7215d742a5f9.pdf,GSM8K
./dataset/61407600-0227-40a1-b95d-8262f8e8affd.pdf,MLP up mapping.
./dataset/9835fbd8-0265-4359-bfa8-a9f6b6186ca4.pdf,Evaluation Prompt
./dataset/f0dc4581-a72b-4bb4-9de8-9c4b79d3ad65.pdf,MMLU-Answer 1
./dataset/691ec245-e407-4554-9bce-a39c6df3e33b.pdf,"The distributions of human participants' initial preferences for different flight features. A rating of 1 indicates the strongest preference for the earliest departure time, the shortest duration, the fewest number of stops, and the lowest price, while a rating of 5 indicates the opposite. A rating of 3 indicates no preference."
./dataset/64cd55e7-301f-4f72-a51f-6ca0e30e0438.png,"Average completeness for each failed task on the Advanced task set over four methods. The smaller the value, the less complete the failed task is."
./dataset/a3175917-94f3-4d0a-b002-d822085dcb7d.png,Concept of any-precision quantization.
./dataset/39ebcaa6-e0cd-4004-aeb3-739f0524e027.png,Quadratic
./dataset/c71552f7-b54e-4cb1-b18f-29883bb2202d.png,"End-to-end throughput of Mistral-7B, OPT-6.7B, OPT-2.7B and OPT-1.3B."
./dataset/25b594c9-ee2c-446a-86b0-23545253550d.png,"Illustration of the proposed memory module. The database selection memory returns top K relevant databases. Then the LLM refines the database selection and generates a retrieval plan following our proposed strategy. For each retrieval target, the data value memory returns relevant data values from the database to help generate correct SQL queries. Then information is retrieved using SQL."
./dataset/4cd6726a-54f7-4c3a-b08a-9a100c76ff0f.png,Comprehensive Workflow for Deobfuscating and Analyzing Malicious Code Using LLM and Reverse Engineering Tools
./dataset/c9f3b788-98c2-4ffa-b700-916b78d8fe3f.png,The effect of the social roles of opinion sources on Alpaca's response to opinion contents.
./dataset/191955c8-e4b2-47c5-b459-af0e3af2bd30.pdf,PRISM -- A Reliable Methodology for Auditing LLMs: The LLM is prompted to write an essay for each statement (given a role or not). These essays are rated in terms of their level of agreement with the statement and then used to tally up a score to determine the LLM position given the role it has been assigned (or its default position).
./dataset/039065fe-913b-43e3-aaca-64ed0abf8296.pdf,%An overview of our proposed solution to use LLMs-as-the-judge to evaluate other LLM-generated responses for relation extraction.
./dataset/5af2a0b6-6401-476c-bebd-c57047e57586.pdf,CNN-Only Actor
./dataset/f0beb24d-d885-419a-b415-492fc8782a7f.pdf,Figure showing the win fraction of a longer answer over a shorter answer (0.5 indicates random) against the difference in length of the responses in pairwise comparisons.
./dataset/12a74f65-9f59-4a69-b515-9edc062f1dd1.pdf,"Here we showcase an example of how we choose at which point an introduction ends. The total word count of this example article was 7,671 and the word count of the reference summary was 172. We highlight the inflection sentence which most serves as the transition from the actual background and theoretical setup of the paper to the actual methodologies which are then detailed in later text. From here we gather the word count of everything before the inflection sentence and classify it as our reference introduction text for experimentation, including the inflection sentence. In this case, the resulting introduction section had a total word count of 1203."
./dataset/89ad9d37-9028-4df4-862a-ae141ed15642.pdf,Positive pairs construction. Red curves represent alignment procedure.
./dataset/cf55beab-5738-4213-8289-24c2f24a337d.pdf,Human evaluation interface screenshot for Exp. 1.
./dataset/139aaae2-3e68-4d66-bbc9-28e68c5c0afe.pdf,"Projections of LLaMA-2-13B representations of datasets onto their top two PCs, across various layers."
./dataset/48482212-a7b7-4e1c-9b92-4d140d026657.pdf,Value Anchor
./dataset/a270adfd-aa8e-4999-8464-b32f137b1f2f.pdf,"We visualize our tested models' original accuracy and CIARR values on GSM8K from three complexity dimensions, representing the models' robustness to complexity increases in a certain complexity dimension. 'N' represents CIARR for numerical complexity, 'D' represents CIARR for the depth of the reasoning graph, and 'W' represents CIARR for the width of the reasoning graph."
./dataset/36d68417-274d-4e1c-8312-21b6800be6f1.pdf,Comparison between our benchmark and Japanese MT-Bench
./dataset/e5c6c927-2a3b-4788-9311-62a6ad9b98a4.pdf,"Level 6: The flag object, which serves as the win condition, is surrounded by pushable rock objects"
./dataset/a2f4da1e-a514-46e5-a9cc-cdd68ecef368.png,Both GPT-4o and Gemini could not produce Violin charts via Vega-lite scripts.
./dataset/6d127412-9998-46c6-8128-947f8148a338.pdf,GPT-4o
./dataset/859abf4a-10a5-431c-b490-bc58c6622c2e.png,$F_1$ scores per relation type in the DocRED development set results (darker = better). White color means that no correct predictions were made for this relation. The relations are arranged in descending order by the number of triples.
./dataset/77189601-0735-47f5-9f2d-26a32114d72b.png,LLMs cooperate with client models in federated learning.
./dataset/32d2141c-7ffc-419f-b455-b877cdddef93.png,entering GP (expert-designed kernel)
./dataset/9dc8e66b-4641-4de1-a56c-c0cbab88b53a.png,Input layernorm.
./dataset/a48c85f8-ab99-44e6-8966-1dc48a69b62e.png,Overall workflow of ChatDB.
./dataset/d0056765-385a-4e22-99f7-badaa4bd83a6.png,The Effect of $n$
./dataset/85ca6c2a-5acf-45c1-ab77-018b993ec3d9.png,Llama2-13b
./dataset/18c85f8b-3220-4d1a-83c5-98d5ca48f237.png,Taxonomy of LLM-Generated Misinformation.
./dataset/9ababe6b-0999-4f89-b56c-f929d2797dc9.png,H-L
./dataset/4df8e033-fee9-4ac4-ab10-9bb43d3f643c.png,"Comparative PCA visualization of hidden states across six datasets: BERT-like models vs. LLMs. T-BASE, T-EMOJI, T-HOMO, LAWS, CODE, HAL refer to ToxiCloakCNBase, ToxiCloakCNEmoji, ToxiCloakCNHomo, LegalText, MaliciousCode, Hallucination datasets."
./dataset/273323ed-5298-423d-8557-31cddb9b7255.png,upscale
./dataset/b613d518-18ee-4955-b1bb-57b5d4af4f50.pdf,Decentralized
./dataset/0bebaa39-bec3-4800-8716-cac00d6b3ade.pdf,The  comparison of detecting human-written  and LLM-generated misinformation.
./dataset/0721e5a6-c3bc-4112-a73c-99e7ab26671a.pdf,Thresholds are displayed for each metric on each dataset and model category.
./dataset/5b00a5b8-d834-4c4b-8baf-13be89b25072.pdf,User preference distribution of tasks 1-4 (task set 1) collected in the crowdsourcing study.
./dataset/42e2122e-fa92-439f-9046-a3a2dc179a20.png,Three Types of Human Role Simulation by LLMs.
./dataset/e7c1e3fc-cb89-49ae-8c8b-18c081312ee5.png,Illustraition of three types of uncertainty estimation methods.
./dataset/231dec4b-b0ec-4a50-9562-c835b2791fb2.png,"System implementation pipeline. The system starts with natural language input specifying the task description and user preferences. With this input, the LLM applies chain-of-thought (CoT) reasoning to obtain reasoned widgets. Based on this reasoning process, the crowdsourced user preference library, containing the crowdsourcing tasks, user-preferred widgets, and their rationale, is integrated to augment widget reasoning. Additionally, the Jupyter Widget Library widget candidates are fed to the LLM for reasoned widget implementation. Lastly, the reasoned widgets and reference code snippets are passed to the LLM for code generation in Python. This allows widget implementation and interaction in Jupyter Notebook."
./dataset/a875f9ac-f88a-4c79-b7bd-e35e70b18042.png,TTS performance of three Llama policy models on MATH-500 with three difficulty levels.
./dataset/372990ea-b9e3-45fe-a08f-5d932abd6e7b.png,Taxonomy of Hallucinations in LLM-generated code.
./dataset/f971667a-f65b-437d-a2f3-39bdc788371a.pdf,Water
./dataset/b00dda99-fb43-4cf9-a1e4-17d14dde7801.pdf,"Scatter plots of the effectiveness (i.e., NDCG@10) of TREC DL 2023 run according to the real queries with human judgments and our synthetic queries with (a) sparse judgments and (b) synthetic judgments."
./dataset/d592a8a2-6884-4bb9-b3d4-778144c5b91d.pdf,An example of the predicate equivalent transformation procedure implemented in this study.
./dataset/3f07a163-530c-4c9b-bdf7-571429ee67f4.pdf,"The landscape of the rank of the validation loss and the Pass@1 scores over all the hyperparameter combinations. The grey surface shows the distribution of the rank based on the Pass@1 scores, and the blue-red surface indicates the distribution of the rank based on the validation loss."
./dataset/3119575b-0390-4ba4-be31-77a164d2cade.pdf,"For each position of ground truth, we permutate the negative samples."
./dataset/3f40e161-0a5c-45f8-b824-096d679b1c2c.pdf,"An example of how our LLM-driven closed-loop feedback and refinement framework works during solving a task that requires examining a square area. In the first loop, the evaluator identifies deviations in the NL trajectory observation (action 12), providing error feedback into the code generator to refine the code. In the second loop, the evaluator confirms refined code matches the task, and the code is deployed on the UAV for mission execution."
./dataset/1674a484-afe6-46a0-857c-55e1898635d1.pdf,An example of CPA problem with only one answer.
./dataset/478c447f-cb45-4df7-abc5-9d8eb9cef610.pdf,Experiment Results. (a-d) shows the classification results; (e-h) shows the forecasting results; (i) shows the representation results. The red dashed line represents the best result.
./dataset/b191e3fb-d563-4932-bebf-1a378dbca2fa.pdf,Attention O mapping.
./dataset/333bdcd2-429b-41e3-9e46-f242fa444ae5.pdf,Metrics for measuring LLM-judges' robustness against epistemic markers.
./dataset/f8b84516-dd8d-4f16-99ed-a70eb6431686.pdf,"ChatGPT Model's Heatmaps of (A) Task1, (B) Task2, and (C) Value-Action distance across 12 countries (left) and 11 social topics (right)."
./dataset/3e5b1b8a-d7af-480d-881f-7a3c06c49e38.pdf,Demographic Data
./dataset/0e31d370-956d-4c12-adaf-e687900382e5.pdf,MADDPG
./dataset/f0c5bacb-6cb8-4770-a7ef-3e9039c6efe6.pdf,3 Agents -- MADDPG
./dataset/f5c1cc63-b7e7-4484-a38d-466142e126cf.pdf,Happy Ending Attack (HEA) template
./dataset/8409cfa6-8a38-4eb2-9166-e05198c4db91.pdf,Comparative visualization of hidden states classification separability using single linear probes on all datasets: BERT-like models vs. LLMs. The fundamental difference in how BERT-like models and LLMs process information becomes particularly evident in the layerwise progression of separability.
./dataset/cc473082-9991-4b3c-9934-73d46dfe52f7.pdf,Overview of the evaluation process
./dataset/1a2dc8f6-197c-4369-af03-16917b51cabd.png,This figure presents the retrieved top-1 most similar source function with the query of a malware function  from a PE file
./dataset/774496a4-f11c-406f-a8c6-273dc848e89e.png,Count statistics of the public transportation score extracted from LLM using the 0.0-10.0 scale in the PNT dataset. The extracted scores occur at a certain fixed gap value (0.5).
./dataset/80deb33a-37c6-4b91-b83a-9a1a4c5a5c23.pdf,Fitted discount curves.
./dataset/a0ec6a9b-a8a6-42b7-aed3-57e226f4065a.pdf,Schema of the Multiple-Choice Question prompt strategy.
./dataset/200912c3-2b8a-4ba8-b7b4-564038daf185.pdf,Incremental upscaling from 2-bit to 3-bit with uniform quantization methods.
./dataset/a149cd4c-e89c-45e1-a530-acde016943a4.pdf,Case Study of Composition Understanding.
./dataset/4d2b4881-1583-4508-8871-b880ffd216f9.pdf,Case 1
./dataset/84e2af94-770c-41c2-9efb-cb206d2babc8.pdf,"With 30,000 attempts for each given constraint, we calculate the average number of adversarial examples generated for each problem."
./dataset/15481ac6-b949-42ff-97a4-5214f1bedcfe.pdf,"If GPT-J was struggling to ``carry'' when creating the $a+b$ helix, we would expect the ones digit of $a$ and $b$ to sum up to greater than $10$ when the model is off by $-10$. However, we see that this is not significantly more likely for when the error is $-10$ than when it is not $-10$."
./dataset/c83617a9-e0fe-43d5-a62f-965ecce71185.pdf,Ethics
./dataset/3166c149-e3f8-4ebf-9f7b-75db5a868c01.pdf,Prompt design and an example dialogue for the imitation-only iterated learning on the ACRE dataset.
./dataset/0f6cfffc-766c-4774-aba8-09c4a7603637.pdf,Case Study of Multi-hop Reasoning.
./dataset/46d661d0-b0ca-47b1-aa5a-3ba5c5a31db7.pdf,Two shot.
./dataset/19810ca4-e1da-444c-8f38-422b9891287e.png,Attnention V mapping.
./dataset/53b259b5-ba86-45b3-8074-06c41cb97006.png,Consistency of client simulation.
./dataset/13f5f5e7-4037-4cf3-b029-d9b45e28b9e6.png,MM-L
./dataset/332e2baa-c147-4715-a10d-06e783e643f1.png,Generated Persona
./dataset/90b82ec7-3777-4dd2-aa93-15a695af08e8.png,Repetition: copy input.
./dataset/d58abdca-93e4-4aae-898f-b43e8093bb8d.png,The sum rate performance of proposed method and other baselines versus different BS transmit powers.
./dataset/aa4404b2-3b69-436c-ab21-f6578a844eef.png,An example PDDL problem file learned throughout exploration in Coin Collector.
./dataset/1f306649-8a29-436c-827c-467860fbb59c.png,Entity generation in the few-shot scenarios.
./dataset/62a53438-5912-4cba-8558-16dfd660c6be.png,WritingPrompts -- Human vs. GPT-4
./dataset/42067169-424b-41a4-87ab-7b9531c50ec4.png,Comparison on different retrieval models.
./dataset/de3eda8e-fc3c-4e85-9490-d2adeb5fb993.png,Results for all downstream tasks regarding changes in the activations of OLMo-0724.
./dataset/57dfd4ed-5017-4211-8629-3f89b93ea4d1.png,An overview of the proposed LIKR framework.
./dataset/515750c3-624a-4570-94c0-b06755d2f365.png,Prompt to generate social contexts and issues.
./dataset/22d2f81d-abc1-42cc-9aa0-503d0ddcbcf8.png,"Category distribution of meta constraints. The ``specific number format'' often involves constraints on the generated numbers (e.g., time format). The ``limited grammar'' includes constraints for writing styles such as active or passive voice. The ``limited structure'' includes common structure output formats such as JSON, YAML and etc. The ``limited punctuation'' requires the LLMs to use specific punctuations in the generations. The ``limited word count'' directly limits the length of the output from LLMs.  The ``limited content'' constrains generations within specific topics, which can limit the output scope of the response."
./dataset/d5fe7a66-c7d1-451d-81cc-bb053cc0bdfb.png,MM-H
./dataset/89210633-531e-4f1b-a2e4-f29be119ef11.png,The posterior probabilities of all at the end of different generations.
./dataset/7677a7df-a967-4600-b859-ea55f41d6a47.png,Results for convergence of VA in chatting LLMs: model: Gemma2
./dataset/e65cb9d4-48ed-4adf-bd9c-7ced6baed92d.png,Localization with LLMs Production Pipeline
./dataset/22400acd-7685-4e2e-99ea-7068149c40b3.png,Visualizations of the impact of graph sentence length $l$ and graph sentence length $k$ on performance.
./dataset/4aea1322-1a81-4267-8cfd-e134eb70662e.png,"An illustration of our proposed Sequential Fusion method, showcasing its comparison with traditional approaches. Our method employs a two-stage framework: First, general LLMs are leveraged to perform relation extraction for extracting structured knowledge from complex texts. Second, the extracted knowledge is used to update domain-specific LLMs through knowledge editing."
./dataset/dec3356c-5135-4d45-bebf-c42118443913.png,Overall framework of  RADIAL method.
./dataset/0eda9f96-97b9-4a81-8693-d9c5a17dcb25.png,Software interface of manual attack prompt construction.
./dataset/a282b5da-9ed4-4456-9c2f-50c5a5ef3aa3.pdf,Layers of interest for OLMo-0724.
./dataset/bfda295e-b59a-4988-85a8-f674133fed6b.pdf,"D3LM Model Framework Overview: Illustrates D3LM's engagement through context-driven questions, guided by the PURL algorithm from continuous and historical dialogues. Aims to collect comprehensive case details until a fine-tuned LLM token signals adequate information acquisition."
./dataset/2c4d5259-a7ca-4dae-aa70-b894ebe79839.pdf,A case of code transformation and its corresponding predicted logging statement from multiple models.
./dataset/11f5d35c-8c9d-4e8f-8938-72fc118f3ec4.png,"Accuracy on six reasoning tasks across four LLMs under three evaluation settings. PW$_1$ and PW$_2$ denote the ProofWriter dataset paired with the Prover9 and PyKE solvers, respectively. Origin refers to the original dataset with direct prompting, SCALe refers to the SCALe-diversified dataset with direct prompting, and Tuning refers to the SCALe-diversified dataset with prompt-tuning."
./dataset/2d0d8f55-6b8e-4b66-b455-fc278208924f.png,Normalized log probability for the first token of memory answer when four evidence (two supportive and two contradictory to the parametric memory) are presented.
./dataset/b9bd19ed-3195-42ac-887b-7d031ef15be3.png,The specific processes of data construction and Scoring Evaluation
./dataset/4de376a5-96ba-4f70-b12e-be4befb64fe0.png,Chicago Nightlife
./dataset/dd05bdad-ba31-4844-9de4-2aa14afcf087.png,An ICL prompt template for all tasks.
./dataset/f2495eb9-bcfe-4ecf-a1fd-c70fcfb701cf.png,Losses (y-axis) vs training steps (x-axis) during the model's pretraining.
./dataset/dc5e9b8c-51e9-4469-b9a9-9595f7482676.png,"An example of privacy leakage in an LLM system. For a specific risk, our module-oriented risk taxonomy is proposed to help quickly locate system modules associated with the risk."
./dataset/38d64c4e-5fff-485f-8f18-9df54791fb28.pdf,Non relevant error seen in MALT for Gemma-2-2b. The response is on a topic closely related to the query but does not effectively answer the query
./dataset/c4aacbd7-01b1-464c-80b4-5b483774654d.pdf,An example of database schema
./dataset/da48e1f5-5ba3-45a8-8c57-8a43f31d73c1.pdf,"DejaBoom logic graph. Nodes represent scenes and colored groups indicate scenarios. Arrows indicate order of completion. Merging arrows: only one of the tail nodes is required to proceed to the head node. Dotted arrows: a new location, NPC, or item should be unlocked."
./dataset/a5cabf14-e8aa-4cbd-8264-556b64b69c25.pdf,The answer distribution of ChatGPT under different length ratios between parametric memory and counter-memory.
./dataset/6f6878f2-b906-4a21-a359-9c77e5954828.pdf,Distribution of Examples Across Difficulty Tiers.
./dataset/47bce1a3-d173-4c13-b9f2-89b3a89157bf.pdf,Frequency (y-axis) of lexical overlap (x-axis) between LLM and corresponding SNLI hypotheses.
./dataset/834c44d1-187f-4fd6-a699-fffb883acd08.pdf,"An overview of the two stages of our user study. In both stages, we have the annotators read the documents and come up with a relevant topic list with (Treatment) and without (Control) the LLM suggestions. By the end of Stage 1, the annotators agree on a Final Topic List, which we use for our Topic Assignment stage. In Stage 2, all annotators conduct the task of assigning the topics to a separate set of documents with (Treatment) and without (Control) the LLM suggestions."
./dataset/dd1bb3cc-f354-4d9c-b84d-555e748af75d.pdf,Data distribution of clinical scenarios.
./dataset/c60b0284-f831-4c86-a09b-900897151181.pdf,Python.
./dataset/7ce56526-203c-4e20-8bb8-faaa7d807c3a.pdf,An ICL prompt example for reaction prediction
./dataset/a7c683d0-dda4-465c-be84-c6bcb8ced76e.pdf,"Variations in correlation coefficients (left), encoding accuracy, and IDs (right) of the activations of OLMo-0724 (layer 30)/LLM-jp  (layer 25) using learned languages across checkpoints."
./dataset/7354032f-2d78-4ece-807a-2b4d832b648b.pdf,An example of problem in financial corpus.
./dataset/2b782952-4f44-44a1-8a9a-d7c99164422f.pdf,"footnotesize The optimal weights of each metric (a) and the performance of RelD with the original and optimal weights in automatic (b) and human-in-the-loop metrics (c), respectively, on the validation dataset."
./dataset/cfcde3bd-4f2f-4487-a0be-43780fe0187b.pdf,Valid and invalid responses from LLMs.
./dataset/fae5df01-5bf5-4410-903f-d86effc5ceaf.pdf,An example for step-by-step reasoning in Go.
./dataset/521b2603-2bbc-4449-82e1-f08258001f5a.pdf,The timeline of MM-LLMs.
./dataset/c2c93b81-a6e5-4cc2-b800-8bd6f4ef35dd.pdf,"Our evaluation pipeline consists of the task generation component, the plan validation component, and the LLM planner. Dashed lines represent processes only present in the external calibration mode. Rounded rectangles represent ASP-based components."
./dataset/ed08f05d-e23d-4520-936c-e41e4494282b.pdf,small Metrics
./dataset/9b6597db-16d9-4fac-adf0-13c2465e4ce3.pdf,rewrite prompt to rewrite-continue.
./dataset/cb35aa42-f2bf-40a0-ad56-c657f7c94e71.png,"People's perceptions of constructiveness across (Human vs. AI), (HAI vs. Human), and (HAI vs. AI) comment pairs. Statistically significant differences are reported at $p<0.000005$ (****), $p<0.00005$ (***), $p<0.0005$ (**), and $p<0.005$ (*) [adjusted P-values after Bonferroni correction]."
./dataset/6cc6a32a-f1ca-4c5b-ba38-3e4b87fa55f7.png,Process for knowledge-based response generation. Stage 1: Fill in the parameters for the knowledge retrieval based on the query question. Stage 2: Acquire the knowledge with filled parameters. Stage 3: Generate a response with acquired knowledge. Texts in Chinese have been translated into English.
./dataset/548c71a8-3cdb-4e60-847c-0bb503bcbc5e.png,Histogram of Example Constraint Counts per Problem.
./dataset/dc29faf9-0489-4d7e-b778-d9a241808b8c.png,Case-1
./dataset/2fa18632-d376-4d08-a3ee-023005506034.pdf,Flowchart illustrating the conceptual framework of the paper
./dataset/fd813aa1-0936-4eed-9bd8-c45652d3a5ce.pdf,G-H
./dataset/1172f9e1-03ed-41fa-a9d1-837e075d9da0.pdf,Distribution of the co-occurrence of various hallucinations within a single program.
./dataset/3b7d9bde-9b34-4518-82ae-3b6d252b7b75.pdf,"(left) Comparing Scores of Different LLM Assessors for LLMs/Human Generated Text, (right) Interaction Effect Between Respondent and Prompt. Blue Lines Denote Closed LLMs, Orange Denote Open LLMs"
./dataset/42c0ddd9-4f96-4b87-8390-343f5d66c771.pdf,Overview of Chain-of-Scrutiny backdoor defense for LLMs.
./dataset/d56ee206-21b2-4cf1-bd43-98b24716c5ba.pdf,One medical knowledge-guided instance generated for knowledge-tuning. Texts in Chinese have been translated into English.
./dataset/f7a33d1c-1f35-45e0-9343-2f71df00d8c9.pdf,"Variations in correlation coefficients (left), encoding accuracy, and IDs (right) of the activations of OLMo-0724 (layer 30)/LLM-jp  (layer 25) using learned languages across checkpoints."
./dataset/9dd0a022-b5c3-4f0a-b296-15954d54e46f.png,"The GPT4o-mini's results of ranking 56 values' alignment distance on six countries: Germany, United States, United Kingdom, Pakistan, Philippines, Uganda."
./dataset/3ff65c9a-fe40-4ac3-b2a9-bd9fb2715b14.pdf,10:MM
./dataset/ddf5d2ee-8556-45e5-9ad1-b509f34eff6b.pdf,Video Reasoning Task. our VTimeLLM responds to several questions requiring reasoning based on a comprehensive understanding of visual content.
./dataset/5323bab0-3433-4127-8aaa-e08881619859.png,An example for the multi-template complementation mechanism.
./dataset/a115907f-c31b-475f-a0e1-0123ce13aab5.png,Model performance with different candidate sizes.
./dataset/a9f394a6-2cac-443a-9a3e-d8bbcbb78bf1.pdf,Headlines
./dataset/e7510361-3ca7-4deb-be74-be5e4af38968.pdf,"Results for all participants regarding learning dynamics of layers 24, 25, 26 of OLMo-2 exhibiting three phase transitions when using English annotation and MMLU."
./dataset/211d488a-cc47-4992-a663-f4c72fce01f6.pdf,"Level 4: A flag object is the win condition, but there is currently no flag object to reach"
./dataset/375ccecf-cf73-44d2-93cb-132cde616051.pdf,Database information for a demonstration
./dataset/56a192fa-4664-4a59-94c1-0ee1465590e5.pdf,Llama2-13b
./dataset/0fe6d48f-2ce1-45d6-9695-847aeb6feafa.pdf,Examples captions generated by different models. Descriptions that violate chemical facts are marked in grey.
./dataset/9141fa02-8825-43a2-9fc4-4b784c95873f.pdf,Gemini 1.5 Pro
./dataset/64a3e409-b645-43c5-9b67-849097fad7ac.pdf,The architecture of our benchmarking-evaluation-assessment paradigm.
./dataset/0a915bc1-1fcc-4962-a3de-58bd45ead8be.pdf,Tensor-wise compression ratio statistic features.
./dataset/ba1cdd77-bef3-4518-bfad-c5ef3a4cdcf8.pdf,An illustration of the Schwartz value systems.
./dataset/5e5baee0-8b42-4942-8b9d-c371700e4359.pdf,LLMs cooperate with retriever.
./dataset/2aa4ee7c-10f2-4edd-af72-a756dad8cab2.pdf,Examples of specific design of the prompts for code generation task. The model generates the text highlighted in yellow.
./dataset/cf270fba-5775-4e61-b795-56f2c52060a8.pdf,Verification Successes of GPT-4o
./dataset/4870ba58-183a-47e9-a739-6b0ef8e0ac27.pdf,User interface for task: placing a watermark on an image.
./dataset/13927e65-c122-4037-be40-b108c62b4589.pdf,Instruction given to the human annotators.
./dataset/e9236fde-646b-4e41-bfc4-c2f1fad5d503.pdf,"High level overview of INT-FP-QSim. The user specifies the input model and the quantizer functions. The matrix multiplication layers (e.g., conv, linear) within the model are replaced with versions of the layers that have the quantizers attached. During forward pass, the quantizers are applied to the inputs, weights and outputs of each layer to simulate quantization."
./dataset/0af87de1-13b1-4a3c-82c2-7c0ca3369293.pdf,Three paradigms of LLM-based RS.
./dataset/78a93546-3753-4064-a791-b434480c8542.pdf,Claude-Instant vs. Gemini-Pro
./dataset/d5b2fef5-f017-4624-b49c-5ea95255791e.pdf,A case study of asking the follow-up question.
./dataset/74a9f6b4-8ea1-463f-b199-23fe78fe0580.png,Toy case of beam search with RLHFlow-Mistral-PRM-8B and RLHFlow-Deepseek-PRM-8B.
./dataset/14b32b87-a0f0-414a-a0e8-57ddfe157a5e.png,The variation of statistical metrics during PPO training on Helpsteer2-Preference.
./dataset/cebe5b47-7821-46c6-8957-2bd3fd1a66a2.png,"Figure (a) depicts the assessment results of medical experts, while Figure (b) shows a noteworthy correlation between CliMedBench and MedBench."
./dataset/b8372bba-ce41-418e-b564-871e4088a6e3.pdf,SILICON: A Systematic Framework for LLM-based Text Annotation
./dataset/9e6e74d9-4db0-4643-a1cc-71875fd4bd1f.png,Framework for COAT Synthesis Exploration.
./dataset/e3cbb197-1a51-4fb5-ab3f-54cd5a88047f.png,"The black-box LLM handicaps end-to-end task training through gradient backward. However, manual (sparse) and LLM (fine-grained) feedback can be used to optimize the retrieval model."
./dataset/f5670622-9119-4760-aefb-553f321245a0.png,Distribution of the record length of the Lithuanian component of the CulturaX dataset (in tokens).
./dataset/2d7707b7-2eb1-47da-8518-5994fefbbaf9.png,Distribution of preferred asset classes in portfolio (Investment Amount).
./dataset/2bcd04f5-e9ff-4fc8-a3a7-641457bd51db.png,Attention Q mapping.
./dataset/46fa5de8-df86-4d70-a641-424c6290755a.png,small FoodKG Recipe sample: left panel shows a 2-hop KG subgraph for the recipe node shown on the right.
./dataset/16b20cde-ed78-4de3-95f1-8f24b64760e3.pdf,Snippet examples of dialogue sessions between RealPsyDial and SimPsyDial.
./dataset/cecfdd79-e23c-475a-893a-972e4fd81725.pdf,An overview of the proposed methods.
./dataset/95239316-fa6e-426a-aea8-0322963cf25f.pdf,Attnention V mapping.
./dataset/e5b8623e-219e-4548-b7d1-f42df7b593a6.pdf,Example of Fixing Syntax Errors in Target Java Program
./dataset/9741893a-b6f8-484a-ae87-ed4d08418f9e.jpg,Average LLM Consistency (Only Including Real Parameters)
./dataset/333f54f0-28b8-4b71-876b-2f43abb9c043.png,Paraphrase Generation
./dataset/343ca564-6894-4f71-9b7b-9ebda883f2e3.png,Claude-3.5-Sonnet
./dataset/4e006edf-8d97-46b3-a034-4facb196a2dd.png,Llama2-13b
./dataset/7b8e0895-ee58-4954-8a52-4c0891db0022.png,The four levels of text spotting
./dataset/cc357488-4912-4023-96aa-b448c7c52a70.png,The process of Inference and Evaluation of our benchmark.
./dataset/3677fb53-53e8-41cc-b82b-45912ab057f7.png,"Compared to golden only, self-reflected trajectories help LLMs learn more effectively and efficiently."
./dataset/32ebfecd-3d57-48bc-99dd-56fe88d6c6db.png,An example constructed belief tree.
./dataset/5043bc6c-ad95-483b-a9a8-fb58281a4d84.png,Results for all participants regarding changes in the relationship with the brain using layer 25 of OLMo-2 and Japanese annotation and MMLU.
./dataset/b650adaa-f03c-4bff-9e1e-9936bab16a55.png,Power Law
./dataset/84222d63-03f4-4b69-bc68-a8bb459df7df.pdf,ML-MM
./dataset/9bcd7cf0-63c8-405b-baaf-b66563fb1504.pdf,Spectral Factor vs Temp
./dataset/15923c57-0bb4-423a-8ba5-f2c2cdf879f5.pdf,Doudizhu game replay for DouZero (landlord) v.s. DouZero (peasants). Here is the key frames of a complete game. The peasants wins finally.
./dataset/f68143fd-a939-4fdc-856e-3a3f12f25c09.png,Fluency error seen in MALT for Gemma-2-2b
./dataset/45a39ae3-d274-4e1e-accc-2be8dcfc22bd.png,"The correlation matrix for benchmarks. The closer the Spearman correlation is to 1, the more similar the rankings of the two benchmarks are."
./dataset/8becc94c-fa83-4722-94ba-9831642a87e8.png,Standard deviation on FPs on different models with scores of different prompt level .
./dataset/2168daa3-3513-483a-9c95-cfef5c5b7168.pdf,Exponential
./dataset/31b02e22-9af9-46b4-a61e-467f2b4ce956.pdf,An illustration of how the diagram grows with animated table glyphs during the code generation process.
./dataset/b6011601-5c37-442e-a220-482dcd1d0b91.pdf,Average Assessment Scores of LLMs/Human-Generated Text by Different LLMs
./dataset/f4e86171-e08d-453b-a4e0-02bb35627aea.pdf,"Example of the interaction between token-level sampling and action-level sampling for a two-armed bandit, showing the evolution of the probability that the first action is ultimately selected as the tokens are generated by the LLM."
./dataset/f4ee6f7e-719c-4412-a6c2-a6dcb4b465de.pdf,Changes in loss across the four training stages of MiniGPT-3D.
./dataset/8b6f5ea9-ad8a-4c91-8776-09dde70bdfd1.png,A visualization of the coverage map that is obtained during the training.
./dataset/4f5f94c3-a268-4f5c-85da-455b20119f64.png,L-ML
./dataset/16e30093-a363-4f13-8d68-fcfd6aef9c12.png,"LLM intelligence (measured as number of questions correctly answered using the baseline persona) versus raw bias scores. There is a negative correlation ($R^2=-0.68$, $p<0.05$) between intelligence and bias."
./dataset/eace8170-a534-478e-b8f0-1bf4764e5952.png,"Comparison of legal service methodologies, highlighting traditional LLMs, lawyer consultations, and the D3LM model. D3LM innovatively generates professional questions, mirroring the actions of a lawyer, to improve legal outcome accuracy without high costs, demonstrating a cost-effective, precise approach to legal assistance."
./dataset/f562ccfb-729d-4407-9012-758fa9a42799.png,Iteration number on C++ (O3).
./dataset/96dee0b7-c972-47ad-9376-077a5a570f13.png,(a) Left: Two node expansion paradigms on Game24: sentence-level and token-level. We adopt sentence-level setting in this task.
./dataset/640e62e5-11dd-49f2-92e0-b43a990f9551.png,Correlations between summarization metrics and treatment decisions for clinicians and models
./dataset/24dbf71f-04d5-4f0a-812d-ca07bc41cb63.png,"Effect of the fused distributions in accelerating the optimization process on BBH, where the x-axis denotes the number of training tokens and the y-axis denotes the exact match accuracy."
./dataset/cb993916-9c44-498b-95ce-6b37e9c50139.png,Comparison of the training stages and inference stage of all models.
./dataset/24f53a1c-ca89-45d2-8041-3ff03f32bf1a.png,Names
./dataset/f7277b64-8597-439d-a3ab-410f60a313b8.png,"Ratios of correct author attribute predictions on original texts that had different predicted labels on LLM-generated texts, grouped by the direction of change in predictions, broken down for different dimensions of investigated psychological constructs (i.e. personality, dispositional empathy, and morality)."
./dataset/3776d1a6-7618-4706-8009-d79bc7c7aaa7.png,"footnotesize Performance of RelD on automatic metrics (a) and human-in-the-loop metrics (b)(c), including results on IID validation dataset (b) and OOD dataset (c) among the selected LLMs."
./dataset/09beae6a-7bf6-40d2-93f0-fafeaa357c73.png,Average treatment ratio (ATR) for r/AskaDocs clinical contexts
./dataset/4398f9f0-2d79-48ae-8456-36741d669277.png,Examples of the three categories of questions in the programming comprehension task.
./dataset/93ab84af-adfb-4e42-a2e3-b0d883ed6a37.pdf,Combined Actor
./dataset/574c5a45-6141-4525-bfbf-1657054dd69e.pdf,MLP gate mapping.
./dataset/b786d490-eaed-4b10-a5be-2f671874a2db.pdf,Iteration two
./dataset/14aa8b0a-ed39-4463-ae8e-475253d34e68.pdf,UI widget preference selection.
./dataset/c125ce8c-cf41-4535-a6b1-13df2ce4f70a.pdf,BBH Navigate
./dataset/f33479bc-fdc0-40c3-b045-e4a8053ed013.pdf,MAE
./dataset/c0f935de-c45b-4c75-82fe-c3b8111bfc4a.pdf,MAPPO
./dataset/2b3cbb3b-ab25-47b3-ac5f-d5f0b177f2ca.pdf,A trajectory of commuting to work.
./dataset/0578c183-5a32-49f3-b8bc-c29692b07d78.pdf,Channel-wise each layer compression ratios.
./dataset/8feb6690-9a80-4f6c-8dd0-f8dd6a5a8436.pdf,The selected metrics of LLMs logging performance with different instructions.
./dataset/508d9cfc-8487-4cde-8abd-d9acbae9813e.png,Two paradigms for detecting hallucinations. The dashed lines denote the LLM generation process. The solid lines denote non-factuality detection.
./dataset/204359ed-96da-4583-b29c-f44978ebdfef.png,"Emotion score by LLMs for Q1 (Negative), Q2 (Positive), and Q3 (Neutral) categories."
./dataset/6fcb69c1-a1c6-41d3-85ad-a45cbd3f93da.png,Creative Task. Our VTimeLLM comprehends the visual information and crafts a poem inspired by it.
./dataset/22733533-aa60-4658-a298-1abc5fb42efe.png,Periodic Linear
./dataset/45843c7f-d2b4-4ebf-addf-2d99dcf74d4d.png,"Ratios of correct author attribute predictions on original texts that had different predicted labels on LLM-generated texts, grouped by the direction of change in predictions."
./dataset/5e87c638-1763-4c00-9cfb-32ceefefd317.jpg,"The illustration of different collaboration strategies, with each animal in the figures representing a different LLM."
./dataset/7bdeb909-7466-4bb3-8d1c-58a8ec6d1b80.pdf,The effect of the social roles of opinion sources on Vicuna's response to opinion contents.
./dataset/5fbd3110-a770-43b5-a91a-6e186897eb8c.pdf,"The relationships between hallucination, misinformation, disinformation, and related terms."
./dataset/ee64369f-ea57-4ed2-954f-5bf778940a5c.pdf,lan-retrieve
./dataset/a1acdb4e-c760-4ffc-bd6d-8895b423c197.pdf,ChatGPT
./dataset/478bd39f-ba52-458d-83ad-8447eb307070.pdf,"A self-reflected agent could autonomously identify, reflect on and correct errors based on interaction history."
./dataset/879d7017-0c12-4d23-a9f8-afdbde43f606.pdf,Observed differences in NDCG@10 for ranking+scoring vs. pure
./dataset/15529e7e-4ee1-43c5-942a-25c44b180731.pdf,"The default (no role) position of each LLM. Most LLMs, by default, espouse left and liberal-leaning positions. Mistal.AI's Mistral model was the most left and liberal, while Cohere's Command-light model was the most right and authoritarian leaning."
./dataset/89259703-a11f-4783-8415-298903c9a597.pdf,A sample response from Mistral when asked to predict the political orientation of the first author and explain its choice.
./dataset/c5d68aa2-ac5f-407a-aca4-661fb53c2e78.pdf,"The knowledge alignment example of the PKG module on the fact-checking task (FM2). The passage behind the ""Response"" is the background knowledge of the ""Input""."
./dataset/e6f994c2-2a5e-481a-9fb1-e0d4ae20f551.pdf,Skew vs Temp
./dataset/b47044de-43d3-46de-bb00-cd289fe03cd9.pdf,Hallucinations vs response length correlation of bigger models.
./dataset/8411f624-f024-4d9b-ba45-bafebaa35816.pdf,Psychological Data
./dataset/2a6b04e3-af1d-4731-a668-5c61028bdffb.pdf,Observed differences in AUPRC for pointwise scoring and ranking on benchmark datasets. Bootstr
./dataset/9feccf18-59df-4201-a231-a2a802c80558.pdf,"A multimedia inference task is combined with INoT Prompt and passed to LLM, then LLM internally processes the task according to the guidance of reasoning logic code in prompt, enabling self-denial and reflection occur within LLM instead of outside LLM. } %The upper part is a simulated LLM and human interaction scene, and the lower part is the construction of the core code-integrated prompt of INoT, where the construction of the prompt uses XML format to divide the prompt into multiple parts, each of which has a clear definition."
./dataset/0ac678ca-f548-4883-81a5-a6f5065fa386.pdf,"GPT-4 planning with and without feedback: Example 1 and Example 2 shows refinement of plans with context cues in VH and Franka Arm Simulation, respectively."
./dataset/93fe2021-eeab-44df-951f-983d888be505.pdf,GSM8K
./dataset/a937325d-e6ef-444b-8b94-c81dd6a388ed.pdf,The distribution of error types on error correction task.
./dataset/5e1140b9-822e-4e16-b6d8-0e8dce46f303.pdf,MMLU-Answer 2
./dataset/59204a4b-162e-4629-b62b-789ddb496c0f.pdf,Distribution of sentence embedding similarity across different LLMs and IFT datasets.
./dataset/9a07a484-ff87-482c-b312-a0816f12f227.png,Outline of the process
./dataset/38bbc128-b3d8-499c-b8b5-06d28ae77c13.png,"Two explanations towards two answer choices for an ECQA problem, where each graph node is analogous to a reasoning unit, and each graph edge serves as a reasoning step."
./dataset/086f5c73-ae4a-401a-8c83-e297d1ad996e.png,Comparing the ranking of Alignment Distances of 56 values in Philippines (top) and United States (bottom).
./dataset/0860da4e-b836-4312-8ad0-c9b87a10601a.png,Distributions of summarization quality metrics
./dataset/65869a2f-40b2-453e-9e04-54281f26cf36.png,"We report the changes in the ChatGPT's evidence preference before and after fragmenting the evidence. OthersToWhole means ChatGPT now favors the entire evidence supporting a different answer, which is inconsistent with its preference before fragmentation."
./dataset/9381497d-f94d-4c92-8a94-2686b5ef0ef5.png,Belief tree example.
./dataset/33cc08b9-26fe-44b9-92c8-83a80d8c4d29.png,The Methodology of F2A Against LLMs
./dataset/8ecd8fb3-2a43-41b4-9c16-53a920644d41.png,A segment of the Islamophobic thread shown in the context of the US.
./dataset/a59af2c2-6807-4921-a912-01e0a94f78f6.png,Harmonic mean of insight ($I$) and foresight ($F$) across various types of self-knowledge
./dataset/16ff6e74-36dd-4f8f-840d-630162e43524.png,Overall accuracy
./dataset/c2a62c7a-e8ac-48fe-bdd4-15f4b913edee.png,"bf Patchification}: given a 3D object, we first fit it into a voxel grid and then divide it into a sequence of small patches. Next, we utilize a patch-wise Variational Autoencoder (VAE) to extract the features of each patch individually and then reconstruct it back. It is important to note that only one VAE is trained for all the patches throughout the entire dataset, making our method a scalable approach."
./dataset/72bf9e52-a59b-4148-97be-e0b9439df509.png,Dead code: redundant statement.
./dataset/d9176e98-48a8-4ce3-9e38-6c90d33c24fd.png,Pointwise Scoring
./dataset/757b3360-8835-45d9-b38c-d0423ea7f0fa.png,"Overview of our approach: Document OCR is converted into a text representation using different verbalization strategies (blue). Before verbalization, we optionally degrade the OCR by applying noise to the spatial position of OCR geometries (red)."
./dataset/026f95c2-6a4c-43ee-92ad-dc9f9e62776a.png,The standardized zero-shot prompt template for all tasks.
./dataset/a1b161b3-6518-457f-98ca-37e848fcf2c0.png,Sample Visualization of Value Identification of Virtual Individuals
./dataset/52c3332e-e1f5-4b4e-8362-bb2ed659debb.png,"rchiname workflow, taking a NER task CoNLL-2003 as an example. The nodes (Format Control and Format Validation) in slight yellow are optional."
./dataset/b1159e00-a0b0-4f8b-8fda-71c6743ebae5.png,H-ML
./dataset/5a7d01ec-b96d-4c3a-a775-792e139c7c20.png,Curves of average SR (left) and completeness (right) over the number of refinement iterations for the NL method on Advanced tasks.
./dataset/eac36419-f9b3-49a1-b485-34d6d8e8a0d9.png,The prompt of Chatbot Agent in interaction log generation.
./dataset/d56b92d1-57c5-45fb-983f-a9972f78d9b6.png,Change in NDCG@10 for pointwise scoring on ordinal relevance scales
./dataset/f1ba157d-af5d-4b4f-ac76-1a94ec422ada.png,Overview of Psychological Traits and Human Simulations in LLMs.
./dataset/e136d53f-fa56-497f-b845-3a95c79c35fe.png,TTS case of Over-Criticism.
./dataset/60616e5c-2963-4d9e-9015-2ddd8d316291.png,"Comparative Overview of Prompt-based, Checklist-based, and the Proposed InteractEval Text Evaluation Methods: (A) Prompt-based evaluation generates task-specific prompts from a rubric for prediction scores. (B) Checklist-based evaluation uses human-created questions from the rubric to form prompts. (C) InteractEval combines human and LLM-generated attributes using Think Aloud methods to create questions and produce final prediction scores."
./dataset/498a8a8b-dc66-4eb3-9476-860e8a7960fb.png,50:MM
./dataset/b83f639d-951c-4072-aae4-e7b7ffaac370.png,Ablation Study about Length of Ensemble Steps for Probing Detection Set.
./dataset/54e0f6ef-e267-4c67-abc2-5c7af0227bf8.png,"Text similarity evaluated by BLEU, ROUGE, and METEOR."
./dataset/39cdb6fb-529c-42b3-8a92-d8afa1b929f6.png,Scoring and Ranking
./dataset/42106345-f91c-4427-95f3-2bfc36a6064d.pdf,Memory access pattern of the 32 threads in a warp (a) before and (b) after the weight bitplane layout optimization. Simultaneously accessed input activation blocks are in the same colors.
./dataset/eda55154-85d9-4a3a-bc9b-d1d191d92b7c.pdf,A bibliometric analysis and visualization of potential LLM and NLP methods for malware code analysis.
./dataset/3d827a6a-a7cc-4dd1-ae96-057090028294.pdf,"F1 scores of different models across various recipe types. Our model, KERL-Recom, consistently outperforms others with a significant margin in all categories."
./dataset/bd059872-0d3d-441e-a427-78e3e18c0597.pdf,The proportion of questions answered correctly by LLMs in different question categories under two QA prompting settings on NQ.
./dataset/4e7968fe-9067-41c0-b92b-865e356ba9d6.pdf,HH-RLHF
./dataset/c1a7e91a-76d5-437c-83ee-c0859710925f.pdf,The performance of different tool-invoking methods.
./dataset/85e38b5c-fcb5-4b4b-bd69-845330c1903a.png,"An overview of topics in this paper, with dash lines indicating their applicability to various downstream tasks."
./dataset/b552b628-2d77-4673-ba4d-7dad06cc3709.png,Model-level breakdown of how number of turns corresponds to treatment decisions (average treatment ratio).
./dataset/8e2735b8-831a-444b-9601-db83cc3747a1.png,A successful jailbreak on Llama3-8b for the harmful behavior ``Describe in detail how to use a phone to stalk and harass someone''.
./dataset/1259ff5f-459a-4a61-87d8-9761ebc566c5.png,"Data leakage occurs when a LLM unintentionally discloses sensitive information, proprietary techniques, or other private data in its outputs. Such incidents may lead to the improper acquisition of confidential data or intellectual property, breaches of privacy, and additional security compromises"
./dataset/4adc53fd-a2c2-475e-ac52-b7707bb9f2b8.png,Context inconsistency: inaccurate condition.
./dataset/7895d033-6af2-4c93-8a68-8d28a36ed8b6.png,A sample prompt of malware code analysis using a zero-shot approach for Android apps
./dataset/d37c3471-0771-48b2-81f3-15904264c4dd.png,footnotesize The performance of different numbers of categories in automatic (a) and human-in-the-loop metrics (b) on the validation dataset among the selected LLMs.
./dataset/e1724770-6d6c-4539-8657-f501816c023e.png,"Accuracy of NB models using only the $n$ ""most informative"" unigram features for each train set evaluated on its corresponding evaluation set."
./dataset/ce633aa9-510a-4f86-bacc-11b5fcd0ac61.png,Average Length of Query Expansions Generated by Different Models.
./dataset/e0208c28-80c7-4f05-af35-b5f9d98cb223.png,Iteration number on Python.
./dataset/b199ed09-ee19-431b-a286-55da7001482a.png,"The MTurk UI for commonsense questions, when first presented to the annotator."
./dataset/c79d2ebc-4e0a-441d-95db-5d7d401b9d04.pdf,Feature correlations by prompt
./dataset/aec5c501-d692-4a61-b38c-bcad83b68d6e.pdf,Structure of a single instance in WARP-benchmark.
./dataset/eec27160-55f7-4dd6-956e-5867216bab10.pdf,Workflow of collaboration between humans and LLMs for dataset construction.
./dataset/41f7c550-85b7-4d89-a89b-a7b84ecbb4ba.pdf,Accuracy on MedMC-QA.
./dataset/b7aa495f-7dff-42f7-b2ef-169f59ec694d.pdf,"Results for all participants regarding learning dynamics of layers 24, 25, 26 of LLM-jp exhibiting three phase transitions when using English annotation and MMLU."
./dataset/2909ef22-1671-491c-bfaf-1eeeaf57a072.pdf,"Overview of our retrieval method: the complex question is first decomposed into smaller subquestions, for which we iteratively perform retrieval and answer generation; once the retrieval is done for all subquestions, we merge the subgraphs and give the result as a hard (textualized graph) and a soft prompt (graph encoder output) to the model."
./dataset/d6b7088e-8ad8-4f60-af55-ef859073075b.pdf,Schematic diagram of scoring model
./dataset/e601aff9-f0bf-4fd9-8163-7e1e186328f7.pdf,Query-focused captioning pipeline and examples.
./dataset/27c32da5-0b57-4e93-bace-6900ac48e2e1.pdf,Level 12: The path towards the flag object (win condition) is blocked by lava because it is hot and baba object is set to melt
./dataset/da51525e-763e-4cf9-94d6-4805bb3d297a.pdf,RTP-LX Safety Evaluation of Hindi models. We report the fraction of prompt completions judged problematic by GPT-4 Evaluator and the heuristic Toxicity-200 exact match.
./dataset/6cfdbc17-d52f-4077-9e7e-60a35c1252de.pdf,The proportion of different optimization level on Python.
./dataset/7a269619-4a0b-46ae-89f5-45bea05296ed.pdf,Collaborative training in split learning scenario
./dataset/17bc946f-5e9d-4981-a860-62e6fee4b2e5.pdf,Llama2-7b
./dataset/a8277ea4-2146-475c-ab85-402bcac28e73.pdf,The RL/LLM Taxonomy Tree.
./dataset/67066389-fce5-43e6-8fd9-e0c7eb5dfd07.pdf,"Traditional vs. LLM Obfuscation Engine (METAMORPHASM BENCHMARK) a) Traditional: Code is fetched, disassembled, analyzed, obfuscated, assembled, and deployed. b) LLM: Code is fetched, disassembled, processed by LLM for obfuscation, assembled, and deployed"
./dataset/52dbbc22-05a1-45f8-93a2-0ac60f957adc.pdf,An example of the Financial Knowledge Inquiry Instruction.
./dataset/43dce328-4eb8-448d-933c-6091b2d20e34.pdf,"A conceptual diagram of the DDPG algorithm, showcasing the interactions between the actor and critic networks, the policy behavior, and the value functions."
./dataset/7a2aad11-ab86-4595-abbc-f6f6a2a367f0.pdf,Area Taxonomy of LLM-based QA agent.
./dataset/b22bd676-7d62-4c6d-85cf-e78056c4d163.pdf,"Impact of Feedback Loops on Evaluation Metrics. The graph demonstrates a clear upward trend in all evaluation metrics as the number of feedback loops increases, without significant fluctuations. Notably, upon incrementing the value of K beyond 3, the metrics stabilize around a consistent value. This value aligns with the results reported at K=3; therefore, data for values higher than K=3 are not included in the analysis."
./dataset/789c4527-74b8-4306-9505-96bc20f085cc.pdf,Attention K mapping.
./dataset/5e9dbc6a-b80e-4c6f-8ad2-64d2cb4bc8db.pdf,Local Articles Recall
./dataset/f25bfd18-a2fb-4320-ad9c-27d2fad7c9e4.pdf,MMLU-Question 3
./dataset/2777e36a-473b-4107-83fc-ab0c4c538f48.pdf,Persona Creation Prompt
./dataset/4dc126bc-1d3e-4b67-ba22-4a17b25b3355.pdf,"Linguistic differences between OCN and MTs. The left panel compares characters per sentence, while the right panel examines adversative conjunction ratio."
./dataset/673c8a90-75fd-4ecb-9bf6-138273832d1a.pdf,Rewriting Generation
./dataset/ed11a74a-8440-47d6-9393-b84ff3dc7be9.pdf,Demographic
./dataset/70cd47d7-de4b-468b-8202-bf7cbe581762.pdf,Observed differences in NDCG@10 for pointwise scoring and ranking
./dataset/f6283219-23d6-4242-b830-ffc639d31f23.pdf,-SNE plot of Human and LLM Generated Essays
./dataset/9845c278-57af-4ccf-a77e-201e152d03dc.pdf,A simple method that dynamically introduces retrieval for LLMs based on priori judgement strategy. We use ChatGPT with QA prompting under the retrieval-augmented setting as the baseline (w/o judgement).
./dataset/273031c7-542b-4122-ba15-c3fde316eeb3.pdf,MLP up mapping.
./dataset/c61eebc7-884c-4036-97cb-c15e6d9f95ee.pdf,Full log analysis of cognitive overload in decision making task.
./dataset/38db74e7-0abe-4691-b73e-170695420ef4.pdf,Overview of self-prompt.
./dataset/06699dd4-114f-4783-b642-dd74cebd47d2.pdf,Proportion of syntax and functional errors across iterations. Example of GPT-4o under Pass@$1$ metric.
./dataset/5a0fb1f4-f213-4714-b4ac-e5f1dabcff05.pdf,TruncFormer with a 64 bit field.
./dataset/afe06b0a-3d4e-484f-9dd5-c44a84332867.pdf,"A PDDL solver produces a plan based on a minimal domain file and problem file. Previous work assumes the domain file as given, while we predict the action definitions in the domain file."
./dataset/b94b090b-2dee-4ade-93d6-6ed7211e4867.pdf,"Left: Results of the human analysis on the reliability and diversity (lexical, structural) of samples generated by different methods; Right: Text examples in different grades of diversity."
./dataset/aced4df7-7cff-4704-a22e-4c00492e756c.pdf,"Sample example indicating that epistemic markers may influence an LLM-judge's decision.}%The top shows reference-guided evaluation in question answering, while the bottom illustrates pairwise evaluation in instruction following."
./dataset/52255715-3873-4f7f-bdba-cf9dc02366be.pdf,footnotesize The distribution of samples from each category with boxplots (a) and density plots (b). Cate: Category (The same below).
./dataset/f284b905-b7f9-48cd-835c-658bb6446a84.pdf,Pass at 5
./dataset/ee2510ee-d997-495c-b936-95c38d4c5bd9.pdf,Results for convergence of VA in chatting LLMs: model: Mistral
./dataset/a24ae66b-9b71-47cd-85e3-8e0846589d00.pdf,Neuron Activation Analysis:the comparison figure of neuron activation analysis and logit lens on Llama-3.1-8B-Instruct and Qwen2.5-3B-Instruct.
./dataset/bcca0f42-71ad-4a44-ad73-e924dc1f828d.pdf,Category Level
./dataset/908e83cb-a85f-443a-a2a8-7f6967acef9b.pdf,Llama2-13b
./dataset/fa8cb16e-8b16-452c-9261-4ad0b6ab296d.pdf,An illustration of my proposed pipeline leveraging a semi-symbolic representation of entities. The LLMs are interacted via few-shot prompting.
./dataset/82e40a36-1926-4e69-b947-52faa0c8baed.pdf,"The training pipeline overview of Mastermind-Go. It is designed to incorporate a hierarchical approach, featuring four distinct tasks that generate progressive and various Go board analyses. These analyses are then transformed into textual data for fine-tuning the LLM."
./dataset/e28f2e02-fb26-47ac-a301-31c6ca242778.pdf,Results for all participants regarding changes in the relationship with the brain using layer 25 of LLM-jp and Japanese annotation and MMLU.
./dataset/1e60d194-72b2-4838-a9bd-e7a392f8bbfb.pdf,The frequency of rankings of new words in the evaluation of the Phi3-mini.
./dataset/257bc055-569d-4d71-bee7-914fbf75e228.pdf,"Question-level breakdown of which model is the ``contrarian"" when only three models agree with the treatment label"
./dataset/7735a79e-06b8-4546-8768-9cf517789217.pdf,"Linguistic differences between NMTs and LLMs. The left panel compares MTLD, while the right panel examines ratio of brackets."
./dataset/25bc1221-4ca4-40de-8a2c-5dc2e3b813f6.pdf,500:G
./dataset/d5ac950d-f063-420c-9e32-2f4ce18d1686.pdf,Full patching results across all three model sizes and inputs. Results are for patching false inputs (shown) to true by changing the first token shown on the left. Numbers in parentheses are the index of the token in the full (few-shot) prompt.
./dataset/7eb6c247-3f63-4a3b-bea6-86b56d44cdc8.pdf,Value Anchor
./dataset/374d3085-5e89-40de-b2d2-9a948d8c204f.png,MMLU
./dataset/69003e55-80ad-4ee9-911a-6fd3c454fc2a.pdf,"The TrimCaching mechanism for caching LLMs in wireless edge networks. Popular LLMs are placed on edge servers, where users can download the requested LLMs from the edge network. To enhance storage efficiency, shared parameters across LLMs are cached only once on an edge server."
./dataset/4a322a4e-af99-4849-be43-0b7619687f1e.pdf,MMMU
./dataset/d9933dab-19f0-4ce0-9afe-a77f4604a049.pdf,"Isomorphic and Random Rotations in Color Space: The leftmost panel shows the full 3D color spectrum. The three right panels demonstrate how sample colors transform under different conditions: in their original positions, after a 90 rotation (an isomorphic transformation), and after random reassignment. This illustrates how structural relationships between colors are preserved in isomorphic rotations but disrupted in random rotations."
./dataset/57b383a9-ae5e-4565-9bd0-a2f41cf906b1.pdf,The generation quality of filtered examples over different thresholds.
./dataset/1a83a3b7-c56b-4b40-996d-82254a7b4299.pdf,Illustration of our proposed
./dataset/e05c39e8-f93d-4b4d-b2d1-7f8382d6dca4.pdf,Gemini 1.0 Pro
./dataset/ff39e39b-89a8-4440-9acb-2cb1fafc1068.pdf,A sparse set of attention heads have causal effects on the output when patched (total effect visualized).
./dataset/67ec8825-7098-4719-9ca2-a7a0ece9e46d.pdf,An example of Imposing Memory Test.
./dataset/b6c66b2f-719c-4b38-82d0-f8a0751d6c50.png,Flow chart of our two-phase study investigating perceptions and writing of constructive comments.
./dataset/e616162d-7b27-4770-920b-3ebd04c1fe50.png,Observed differences in AUROC for pointwise scoring and ranking+scoring on benchmark datasets. Bootstr
./dataset/b7a90869-d91b-452b-80f0-d6fecac9682a.png,H-H
./dataset/c3b64b07-2e3d-4580-bb18-97570726a6ea.png,Comparison of (a) bitpacking-based and (b) bitplane-based representations of quantized weights.
./dataset/a95813b6-95a6-47f0-b5e4-36375ffbce11.png,An ICL prompt template for all tasks.
./dataset/16a1a1bb-258f-4642-a9fe-d1689baac4ac.png,Interaction with multiple counselors.
./dataset/f4a5b954-2691-48a5-a570-bfe2d4ef9e45.pdf,An example showing the input and output of the output generation module.
./dataset/dc36ebd9-7963-4078-9e01-0a4ffee3d676.pdf,DateUnd
./dataset/57e53c29-b2d7-4b34-88c7-8d8203af41ec.pdf,SPROUT o3-mini
./dataset/5be29c5e-67fd-4990-b8f3-dbab76bd66f5.pdf,"A Case Study on Personalized Movie Review Generation. This figure illustrates how different personalization techniques for LLMs enhance personalized movie review generation by leveraging user profiles, retrieval modules, and fine-tuning methods to align outputs with individual preferences and writing styles."
./dataset/895644cc-49a0-43ef-b77c-cf7140ea3813.pdf,"Overall study procedure for Study 2. In the pre-task survey, participants answered questions regarding their prior experience with conversational AI and their prior attitude and familiarity with a randomly assigned topic. Then, participants performed an information-seeking task to gather information on the topic with a randomly assigned information search system with a randomly assigned search system bias. After the search session, participants wrote an essay about the assigned topic. In the post-task survey, the participants again rated their attitude and familiarity with the topic, indicated their perception of two new articles on the topic (one consonant and one dissonant), and answered a demographic survey."
./dataset/ee638479-91a1-4233-9f00-e72a1d132cf2.pdf,Changes in news authenticity lead to LLMs' misjudgment (Taking GPT-3.5-turbo as an example).
./dataset/2f3725df-71c8-4c21-96da-ef3f65bc115c.pdf,End-to-end throughput of Llama-2-7B.
./dataset/11250409-d835-4c3e-9f18-0faab49545e7.pdf,"A case study visualizing the question-and-answer outcomes of a specific example in the DCE using three methods: LoRA, RAG, and Sequential Fusion."
./dataset/ab30d07b-2ff7-4b8f-a9ad-5a21b871b805.pdf,Demographic
./dataset/64d79934-f3a0-43a3-ba93-16c5d22afdda.pdf,Proportion of different error types in generated Chisel code.
./dataset/164c8f74-aab4-4852-8d60-0f94782559b7.pdf,The comparison of attack effectiveness between one-turn HEA and two-turn HEA.
./dataset/133da400-138f-4c4d-b25d-10772a96a6bf.pdf,PCA visualizations for LLaMA-2-70B representations of our true/false datasets.
./dataset/7d6b0930-cde7-4f57-8124-a0809550fe6e.pdf,Accuracy over rounds for different models and ablations on the flight recommendation task. We show accuracy based on direct predictions and accuracy based on predictions derived from their beliefs about users' preferences. The gray dashed line indicates random performance.
./dataset/46a4906a-419c-4513-9963-76a5bbd421a4.pdf,"We present path patching results for top attention heads. In the top row, we view one $a,b$ head (L14H13) and one mixed head (L18H10) as senders. We plot the total effect of each downstream component when only the path between the sender head and that component is patched into the model. We find that both $a,b$ and mixed heads write primarily to MLPs. Similarly, when viewing an $a+b$ head (L24H10) and mixed head (L18H10) as receivers, we see that that the $a+b$ head is primarily dependent on the output from preceding MLPs. While that is true for the mixed head as well, we see that the mixed head also takes input from other $a,b$ heads, implying that it could have some role in creating the $a+b$ helix."
./dataset/93cb4fc2-906f-4905-b17a-c7ebc371dbb5.png,An overview of TOPLA-Framework.
./dataset/65743329-299b-462c-a1a8-e69e0f1797b8.png,"Baseline, gender, and style perturbations"
./dataset/e619b21b-b688-41f0-89ab-12480cf7e64d.png,A general view of of different malware-code aspects that we investigate in our research
./dataset/a126569e-99f9-4dcb-b69e-ae3e239557e9.png,"The rule BABA IS MELT is broken, this means baba will not melt when touching the lava, and the path toward the flag object is cleared"
./dataset/8144366b-b502-45fb-9fee-7c9befed4b29.png,"Results for all participants regarding learning dynamics of layers 29, 30, 31 of OLMo-0724 when using Japanese annotation and MMLU."
./dataset/2a568767-82e5-435f-8a63-126f45d57d62.png,3 Agents -- QMIX
./dataset/eedd9196-deec-4dc9-9e83-91b1aefdb5d2.pdf,"Comparison with traditional AI, single LLM, and multi-LLM system. Traditional AI models require targeted training based on the particularity of scenarios and lack generalization. Due to the limitations of training techniques and data, a single LLM often generates biased and illusory results. Multi-LLM can overcome the above problems through the collaboration of multiple LLMs."
./dataset/944c0c7f-3804-4916-b9af-fc2f8ea948fc.pdf,The example of plan discussion. The final output should be a plan of analysis invovlving questions and their or result types.
./dataset/ed2a7d3d-0583-4957-a0eb-793a8c381839.pdf,Post attention layernorm.
./dataset/969e13a0-735d-4d0d-827e-c1f833d18561.pdf,"Distribution of Pass@1 accuracy of Qwen2.5-72B-Instruct on MATH-500, divided into five bins."
./dataset/2366d3c6-3600-457f-bae7-dea13d2ef2eb.pdf,Response offset for different prompted VA values across models.
./dataset/ef032ac8-64cd-4d6a-b6b4-00141db867fe.pdf,MMMU
./dataset/119c7d53-9fbb-45d4-b98e-62cb372fffbb.pdf,Prompt of Reasoning Stage Analysis.
./dataset/64766084-51a7-4b5d-9dd6-0eac0315e4e2.pdf,Correct steps per model across the 14 Baba is You levels with the action-extended prompt. The finetuned models have more correct steps across the levels but  still not enough to fully solve the levels.
./dataset/0e9ca254-5356-4192-ac77-55db6d95995f.pdf,The PDR distribution across 8 perturbation types. The bars below the line indicate an increase in performance for the corresponding perturbation compared to the performance on GSM8K.
./dataset/4b863f16-48e1-49de-9cb9-f52ff0feac75.pdf,Seed Creation Prompt
./dataset/1a18c8c2-71fc-4541-9d7a-cdac5ef2f45d.pdf,Step
./dataset/017dd7c3-275c-4e28-bf81-5ed461256597.pdf,"Accuracies of LR, MM, and CCS probes for varying model scales and training data, averaged over all test sets."
./dataset/026be953-ed8a-4b6e-80d5-4ac887de5cf6.pdf,Visualizations of evaluation results
./dataset/cc7259e4-6402-44b4-8f9e-63429402064f.pdf,Zero-shot prompt for Crypto-Math and Crypto-MMBP
./dataset/27761587-0cd5-44ff-b86e-4633c5386c22.pdf,"The model's success rate to generate legitimate reasoning topology. It is calculated by the percentage of LLMs successfully generating the reasoning path from the nodeRaw to nodeResult following the few-shot prompt. It can be witnessed that, generally the GeoQA is a harder one to generate due to the conditional question types, which are rarely seen in the LLM training tasks."
./dataset/563697ce-6253-4b4e-b564-0df2f8d604f9.pdf,100:MM
./dataset/a93d88d1-764d-4b16-9ef3-3e4df177d8ad.png,Instruction for Metric Engagement
./dataset/5cff22a0-e498-43c1-b681-85e33ceabe7d.png,HH-RLHF
./dataset/38e0777a-10e4-4518-b062-083daad74f1b.png,DFS prompt to select next step. The answer can be either new step or None.
./dataset/1bdc69b0-8c10-4369-b046-4ef569371586.png,LLM-based Human Simulation Applications
./dataset/01a882f1-8a14-4421-8094-6f363a195971.png,"For all neurons with top $a+b$ fit component with $T = [2,5,10,100]$, we plot the distribution of the top Fourier period in their LogitLens taken over the tokens $[0,198]$. We see that a neuron with top fitted period of $T_i$ often has a LogitLens with top Fourier period $T_i$. Surprisingly, $200$ is a common Fourier period, possibly used to differentiate numbers in $[0,99]$ from $[100,198]$."
./dataset/4ed46afe-4453-411a-8b64-472be67b91d4.png,"The overall framework of the mutual enhancement between LLMs and Graphs. (a)-(c): three pathways for LLMs to enhance graph learning. (d)-(e): techniques for graph structures enhancing LLM reasoning. Brackets after technique names indicate graph types. D, U, M and E represent directed, undirected, homogeneous and heterogeneous graphs, respectively."
./dataset/bd1983ee-6f3d-47d8-b628-2ade6b54f1fd.png,"The figure demonstrates a comparison between mainstream methods and GDL4LLM for node-classification task. Figure (a) utilizes LLMs to embed node attributes and leverages GNN to aggregate the embeddings. Figure (b) presents the descriptions of graph structure centered around target nodes. Figure (c) illustrates how LLMs are pre-trained to capture graph structures through graph language learning, and how textual attributes are further integrated to enhance LLMs fine-tuning."
./dataset/d1378579-2e72-41e3-b04d-37d14861a001.png,Case-3
./dataset/9c05b8d2-ade8-4d92-b823-15cb1ba8a927.pdf,A general overview of pre-processing and reverse engineering for disseminating data on PE files
./dataset/5d0c4887-1239-421f-9901-4cb2d10b9cfa.png,The Overview of the LLM-Slice System
./dataset/0b0510b2-5912-485b-a93c-df4d729150a9.png,"Results from a single participant (DM06) for learning dynamics of layer 25 of OLMo-2 and LLM-jp, layer 30 of OLMo-0724 exhibiting three phase transitions when using other tasks."
./dataset/f768bb5a-7b7a-4327-990e-5484b11bc148.png,An example of the Financial Logical Judgment Instruction.
./dataset/153d9df7-d20a-4ead-89ae-702208bd9289.png,10:ML
./dataset/9d5aced4-6a10-4edc-9e15-1c2c77e68bcc.png,%An overview of our proposed solution to use LLMs-as-the-judge to evaluate other LLM-generated responses for relation extraction.
./dataset/c47a06e1-1d36-40b2-939b-ab8725df1b53.pdf,"Application scenarios of multi-LLM. It includes typical edge scenarios such as elderly care, smart grid inspection, intelligent transportation, and LAENets."
./dataset/e092d011-102c-4a0a-a6f8-9d2ed5f15a3e.pdf,Mining
./dataset/c090cdae-2f4b-46ee-bf69-fec8d3b3d9a6.pdf,An overview of various malware code aspects explored in this research
./dataset/84acb233-0ad7-4e7d-b2ba-eac71320a1c9.pdf,Accuracy on negative cases
./dataset/7397200e-cc8c-4c5c-aab2-f58547400e93.pdf,An overview of calling LLMs hosted on user devices.
./dataset/6fddb682-c9a3-4946-9d91-bcd70b0be843.pdf,The sum rate performance of proposed method and other baselines versus different user numbers.
./dataset/2d6a0f09-6d9f-4999-817a-02380f640ad6.pdf,MMLU-Question 1
./dataset/0db011f8-2add-4a33-a2c9-d45edbf502a4.pdf,"An example of an LLM Translator failing in a diversified context. The example illustrates that when an LLM Translator translates differently expressed concepts into distinct logical symbols, it leads to failure in solving the problem. ""Difficult!"" indicates that solving complex logical problems in natural language is challenging. This simple example is provided to aid reader understanding."
./dataset/bcebae4d-2423-41f8-9c7b-e14e554f7fe6.pdf,Entropy Ratio vs Temp
./dataset/87dbc773-d717-456b-9b56-7c3eb3f1987a.pdf,Belief tree example.
./dataset/ca1fce6c-71e7-4ba8-be7f-ef876a58982f.pdf,Observed differences in NDCG@10 for ranking+scoring vs. pure
./dataset/e932154a-0423-4ed7-849d-bd0d15ec9f5e.png,500:H
./dataset/cfc64e26-60ef-41cd-91cd-0ed8dca690f0.png,Example prompting techniques that ask the LLM for its opinion or preference -- either forced selecting an option or stance or unconstrained.
./dataset/076cc9e4-4de0-4b80-92de-495efe46145b.png,Accuracy comparison of different GDL4LLM variants on the test set across three datasets.
./dataset/1d546d21-8a81-4d74-90d3-89ee99f013a1.png,"Similarity of Attributes in Lexical and Semantic Perspectives: The bars represent the measurement of text similarity. The similarity of SL-TA (Avg) and SH-TA (Avg) represents the average values across the four different checklists created by each LLM and each human expert, respectively. The lower the similarity, the more diverse the attributes are within a dimension."
./dataset/4d94c5a2-f149-4586-b7db-56c335fb6d1c.png,Belief tree example.
./dataset/b16b0840-efde-409f-8269-ffefce502ace.png,The prompt of Conversion from prototype code towards the code with private libraries.
./dataset/854469bc-6b8d-492f-911e-20482f06d1da.pdf,The experimental results of the Statistical Imperceptibility and the Human Evaluation.
./dataset/555bea5c-e79b-4b25-87e5-a1f247742258.png,Evolution of differences in the position and distance metric.
./dataset/87b62a0b-dfb9-48fd-b5a2-c156ed40c1d1.png,-SNE visualization of hidden states of LLaMA-7B corresponding to different emotion categories.
./dataset/6dec9544-e4b9-4ab5-aaac-e0ccad0b9ff1.jpg,Value Anchor
./dataset/9f103f11-213f-4ae6-9245-d3722198a231.pdf,Distribution of toxicity bias and sentiment bias for various targets among dimensions.
./dataset/07f18f4a-7186-46ce-9406-bf111d6e6306.pdf,Types of PE files/formats.
./dataset/ba91c195-8630-49a5-876a-08536f54bf4b.pdf,Temporal grounding.
./dataset/51de77f5-5cf3-40b4-9926-be531c0c9100.png,"GPT4o-mini Model's Heatmaps of (A) Task1, (B) Task2, and (C) Value-Action distance across 11 social topics."
./dataset/9f1c80dc-07b9-48a1-84c8-b2723d00cd6f.png,An example of the multi-agent debate process during meta-evaluation.
./dataset/5ebeb9f0-0d1c-413d-8ab3-73dd32e66bf9.png,Left: Example of a symbolic reasoning graph with non-randomized node names.
./dataset/626a4a4b-7617-4f42-a76f-8b3367b0bd4c.png,"A complete lossy compression process involves two steps: quantization and entropy coding. The latter has long been overlooked in current model compression research. For OPTs, INT8 quantization achieves 4x compression ratio, while entropy coding achieves 2x for weights and 4x for activations."
./dataset/ac4b61e1-70ea-450a-ab27-4b0a399aa1c5.png,"By leveraging synthesized data from diverse decision-making games (such as Doudizhu and Go), current LLMs can be meticulously refined and enhanced, paving the way for their evolution into highly capable and intelligent agents in the future."
./dataset/9c653812-6667-4423-bbb3-eea927164838.png,An illustration of my proposed pipeline leveraging a natural language representation of entities. The LLM is fine-tuned with data of that representation.
./dataset/af24b1b7-9bd7-4c3a-a2d4-48051f7eb525.png,"Overview of our iterative ASJ framework, JRE-L."
./dataset/1bdb6863-ea3b-4722-ab1a-3c33bb446b38.png,Comparison of Bullet charts. Only GPT-4o (top left) was able to produce the correct chart. GPT 3.5 produced a pyramid chart instead (top right). Gemini's and Claude's outputs were erroneous (bottom).
./dataset/dd6df278-a448-49bd-936b-6505f1189366.png,Negative Sampling Ratio vs CTR AUC
./dataset/e096b5f4-5078-493d-a70a-a3486023d3d2.png,Ablation study of bi-level optimization framework with separate training. The results are averaged over three datasets.
./dataset/34321d7f-7f2c-4173-bfe5-08a7e7ca2c83.pdf,Distribution of preferred products in cryptocurrency investment.
./dataset/1f723b4f-6ed8-4a2e-b947-a539da38678c.pdf,Ranking
./dataset/81c10d93-54c7-4d98-90cb-b821eb12d3a9.png,MAPPO
./dataset/5b512106-8193-47df-8fc0-c44835eb00d4.png,An example showing the pipeline of database selection and planning module with the database pruner.
./dataset/1e365ea7-6808-4de7-9c49-bf295950f35f.png,"(A) Perceptions of constructive comments between humans and LLM based on argumentation style. (B) Perceived characteristics of constructive comments reported by both humans and LLM. Statistically significant differences are reported at $p<0.00001$ (****), $p<0.0001$ (***), $p<0.001$ (**), and $p<0.01$ (*) [adjusted P-values after Bonferroni correction]."
./dataset/fe6bc208-1858-4fbf-9a63-86e0b3101443.png,OULAD Dataset
./dataset/4150e7bf-24e7-4aea-8a6b-bc32086bae36.png,Case 2
./dataset/b801d913-342e-4ba0-84fe-11073481fd24.jpg,Training losses of different methods against epoch.
./dataset/89fd9d55-6c71-41b8-82b5-ab2281f149e1.pdf,Model accuracy ($A$) across various types of self-knowledge
./dataset/6a42e787-71c9-42ca-a9c9-a7c02c8a46bd.pdf,Countermeasures against LLM-generated misinformation through LLMs' lifecycle.
./dataset/0fed9f34-a60c-43a0-b8aa-8171ee5e62f0.pdf,Results for all participants regarding learning dynamics of layer 22 of Amber exhibiting three phase transitions when using English/Japanese annotation and MMLU.
./dataset/195d1117-f30f-4fc1-aba2-9fccf9885de0.png,F1 score against rank for various finetuned LLMs on various datasets.
./dataset/e9e194de-0da8-4cc6-bc10-7772ed7e29cd.png,TL;DR
./dataset/97c409af-14e8-4b43-ac9b-9eb25ac82b86.png,An example of the Financial Calculation and Reasoning Instruction.
./dataset/b5786545-0886-4341-8c87-9bb99bc8cd29.png,"Illustrative Representation of the PURL Network in Action. This diagram showcases the PURL algorithm's training process using $Case_i$ as an example. It visually delineates the sequential steps of extracting, summarizing, and reconstructing case facts, followed by question generation. Specifically, the collaboration of the LLM with the domain-specific PU model facilitates the final adaptive node selection through NeuralUCB."
./dataset/891f4550-4db7-4c2c-9d02-db3037f92c29.png,DeepSeekV3
./dataset/c610fb41-26ff-4ac7-a586-d86465144134.png,Overview of the survey presented in this work.
./dataset/ffe09dee-7de4-4400-80fa-59df54d3bdb7.png,"Distribution of ``All Passed Code'' (code that can pass all test cases), ``Partially Passed Code'' (code that has at least one test case passed), and ``All Failed Code'' (code that can pass 0 test case) on different types of hallucinations."
./dataset/8c8e591d-d430-4c00-a36b-32f025aa181c.png,Input and Output Token Cost across Various LLMs. The y-axis is
./dataset/3205faf7-3509-4e57-ab8c-c1b119dd6729.png,GPT 4
